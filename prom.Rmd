---
title: 'Supplement: Prognostication for prelabor rupture of membranes and the 
        time of delivery in nationwide insured women: development, validation, 
        and deployment'
# author:
#   - name: Herdiantri Sufriyana
#     affiliation:
#     - &gibi Graduate Institute of Biomedical Informatics, College of Medical
#       Science and Technology, Taipei Medical University, Taipei, Taiwan
#     - Department of Medical Physiology, Faculty of Medicine, Universitas 
#       Nahdlatul Ulama Surabaya, Surabaya, Indonesia
#     email: herdiantrisufriyana@unusa.ac.id
#   - name: Yu-Wei Wu
#     affiliation:
#     - *gibi
#     - &tmuh Clinical Big Data Research Center, Taipei Medical University
#       Hospital, Taipei, Taiwan
#   - name: Emily Chia-Yu Su
#     affiliation:
#     - *gibi
#     - *tmuh
#     - Research Center for Artificial Intelligence in Medicine, Taipei Medical
#       University, Taipei, Taiwan
output: 
  # pdf_document
  word_document:
    reference_docx: styles_prom.docx
always_allow_html: yes
---

\tableofcontents

Please kindly see List of Figures and List of eTables in Appendix.

\newpage
# Introduction

In this Supplement, we describe details on this study following chronological 
order of our analysis pipeline on association test or predictive modelling for 
prelabor rupture of membranes (PROM). There are four sections corresponding to 
the same sections in the main text, which are respectively Introduction, 
Methods, Results, and Discussion. Along with this PDF document, we also provide 
R Markdown (.Rmd) containing the same texts with this document but including 
the programming codes for the data analysis in-between of these texts. An 
exception is subsection of "Comparison to previous studies". The codes for core 
steps in the analysis pipeline are also provided exclusively in 
an R Script (.R). The codes beyond the core steps were used for analytic 
decision or creating tables or figures. These are shown to provide details on 
how data are processed to construct all tables and figures in both the main 
text and this Supplement, including those in Appendix of this Supplement 
(eTables 1 to 5, and 13) and those shown only in the R Markdown due to the 
complexity (eTables 6 to 12 and 14 to 21). A 5-minute video was provided in the 
protocol [...] to briefly explain technical details on deep-insight visible 
neural network (DI-VNN) pipeline.

# Methods

## Research guidelines

To ensure that we conducted rigorous research, we carried out and reported this 
study by applying three sets of standard guidelines, specifically designed for 
a multivariable prediction model applying a machine learning algorithm that is 
suitable for healthcare (Luo W, et al, 2016; Scott I, et al, 2021; Moons KGM, 
et al, 2019). The checklists for all the guidelines are shown (eTables 1 to 3 
in Appendix). For a fair comparison to previous models, we also followed 
methods in the preferred reporting items for systematic reviews and 
meta-analyses (PRISMA) 2020 expanded checklist (Page MJ, et al, 2021). The 
checklist and the comparable models are also described (eTables 4 and 5 in 
Appendix). We also applied four approaches based on previous methods 
(Ritchie ME, et al, 2015; Maaten LVD, 2014; Ma J, et al 2018; Sharma A, et al, 
2019; Stalpers LJA, et al, 2018; Goodman LA, 1961; HernÃ¡n MA, Robins JM, 2020; 
Lee J, Spratling R, 2019) to develop the pipeline with several modifications to 
provide a framework for improving interpretability of a deep learning model 
[...]. All of the aforementioned procedures were comprehensively described in a 
human and machine learning pipelines we developed for responsible clinical 
prediction [...].

## Programming environment

We set up a programming environment for this study. Bioconductor was utilized 
as described in the main text. There were 198 R packages which are 9 base 
packages, 53 other packages, and 136 dependencies (eTable 6 in R Markdown).

```{r Set up reproducible environment, include=FALSE}
if(!require(renv)) install.packages('renv')
if(!file.exists('renv')) renv::init(restart=F)
```

```{r Set sample kind, include=FALSE}
# sample.kind=NULL # if using R 3.5 or earlier
sample.kind='Rounding' # if using R 3.6 or later
```

```{r Set to run or not run very heavy computations, include=FALSE}
# Many computations were very heavy;
# thus, we provided the RDS files as substitutes and load only ones
# that can be ran in most computers.
# Set to TRUE if you want to run the very heavy computations.
run_heavy_computation=F
```

```{r Recommended memory limit, include=FALSE}
# Minimum 1 GB free memory should be allocated
memory.limit(size=1000000)
```

```{r Files not included in GitHub for some reasons, eval=FALSE, include=FALSE}
# Explicitly including data source
# Send a data access request to the BPJS Kesehatan. Use their data and our 
# preprocessing codes to get these files.

# data/public2.rds
# data/target_population.rds
# data/pregnancy_status.rds
# data/outcome.rds
# data/target_visits.rds
# data/annotation.rds
# data/mh_nationwide.rds
# data/mh_provider.rds
# data/cf_nationwide.rds
# data/cf_provider.rds
# data/inferdata.rds
# data/predidata.rds
# data/infercause.rds
# data/predicause.rds
# data/int_nps.rds

# Too large for GitHub (>25 mb)
# If one needs these files below, send email to herdiantrisufriyana@unusa.ac.id.

# data/timing_pc_elnet.rds
# data/pc_elnet.rds
# data/calib_pc_elnet.rds
# data/timing_pc_rf.rds
# data/timing_pc_gbm.rds
# data/pc_gbm.rds
# data/pc_rf.rds
# data/calib_pc_gbm.rds
# data/calib_pc_rf.rds
# data/small_timing_pc_rf.rds
# data/timing_causal_ridge.rds
# data/eval_pc_elnet.rds
# data/eval_pc_gbm.rds
# data/calib_causal_ridge.rds
# data/pw_hlin.rds
# data/causal_ridge.rds
# data/nw_int_hlin.rds
# data/test_data_reg.rds
# data/eval_divnn.rds
# data/calib_divnn.rds
```

```{r Install and set specific version of Bioconductor, include=FALSE}
source('R/check_install_load-function.R')

# Install devtools to install specific version of BiocManager.
check_install_load('devtools',version='2.4.1',repo='cran',load=F)

# Install specific version of BiocManager and Bioconductor.
check_install_load('BiocManager',version='1.30.10',repo='cran',load=F)
install_steps=T
if(BiocManager::version()!='3.11'){
  BiocManager::install(version='3.11',update=TRUE,ask=FALSE)
  install_steps=c(F,T)
}
```

```{r Install and load packages with specific version, include=FALSE}
for(i in install_steps){
  if(i) renv::restore()
  
  check_install_load('tidyverse','1.3.1',repo='bioc',load=i)
  check_install_load('dslabs','0.7.4',repo='bioc',load=i)
  check_install_load('kableExtra','1.3.4',repo='bioc',load=i)
  check_install_load('parallel','4.0.2',repo='bioc',load=i)
  check_install_load('doParallel','1.0.16',repo='bioc',load=i)
  check_install_load('pbapply','1.4.3',repo='bioc',load=i)
  check_install_load('lubridate','1.7.10',repo='bioc',load=i)
  check_install_load('broom','0.7.9',repo='bioc',load=i)
  check_install_load('caret','6.0.88',repo='bioc',load=i)
  check_install_load('gmethods','0.1.0',repo='herdiantrisufriyana',load=i)
  check_install_load('igraph','1.2.6',repo='bioc',load=i)
  check_install_load('glmnet','4.2',repo='bioc',load=i)
  check_install_load('Rborist','0.2.3',repo='bioc',load=i)
  check_install_load('gbm','2.1.8',repo='bioc',load=i)
  check_install_load('gam','1.20',repo='bioc',load=i)
  check_install_load('preprocessCore','1.50.0',repo='bioc',load=i)
  check_install_load('limma','3.44.3',repo='bioc',load=i)
  check_install_load('AnnotationDbi','1.50.3',repo='bioc',load=i)
  check_install_load('Rcpp','1.0.7',repo='bioc',load=i)
  check_install_load('GO.db','3.11.4',repo='bioc',load=i)
  check_install_load('WGCNA','1.70.3',repo='bioc',load=i)
  check_install_load('matrixStats','0.60.1',repo='bioc',load=i)
  check_install_load('Rtsne','0.15',repo='bioc',load=i)
  check_install_load('reticulate','1.20',repo='bioc',load=i)
  check_install_load('Biobase','2.48.0',repo='bioc',load=i)
  check_install_load('medhist','0.1.0',repo='herdiantrisufriyana',load=i)
  check_install_load('survival','3.2.3',repo='bioc',load=i)
  check_install_load('imputeTS','3.2',repo='bioc',load=i)
  check_install_load('geepack','1.3.2',repo='bioc',load=i)
  check_install_load('rsdr','0.1.0',repo='herdiantrisufriyana',load=i)
  check_install_load('zeallot','0.1.0',repo='bioc',load=i)
  check_install_load('divnn','0.1.3',repo='herdiantrisufriyana',load=i)
  check_install_load('clixo','0.1.1',repo='herdiantrisufriyana',load=i)
  check_install_load('MLeval','0.3',repo='bioc',load=i)
  check_install_load('ggnetwork','0.5.10',repo='bioc',load=i)
  check_install_load('visNetwork','2.0.9',repo='bioc',load=i)
  check_install_load('ggpubr','0.4.0',repo='bioc',load=i)
  check_install_load('extrafont','0.17',repo='bioc',load=i)
  check_install_load('ggsci','2.9',repo='bioc',load=i)
  check_install_load('readxl','1.3.1',repo='bioc',load=i)
  check_install_load('tensorflow','2.0.0',repo='devtools',load=F)
  check_install_load('keras','2.3.0.0',repo='devtools',load=F)
  
  if(i){
    options(dplyr.summarise.inform=F)
    dslabs::ds_theme_set()
    select=dplyr::select
    rename=dplyr::rename
    slice=dplyr::slice
    use_condaenv('./renv/python/condaenvs/renv-python',required=T)
    renv::use_python(name='./renv/python/condaenvs/renv-python')
  }else{
    extrafont::font_import()
    reticulate::conda_create(
      envname='./renv/python/condaenvs/renv-python'
      ,packages='python=3.6.3'
    )
    reticulate::use_condaenv('./renv/python/condaenvs/renv-python',required=T)
    renv::use_python(name='./renv/python/condaenvs/renv-python')
    keras::install_keras(
      method='conda',
      version='2.3.0',
      tensorflow='2.0.0-gpu',
      envname='./renv/python/condaenvs/renv-python',
      conda_python_version='3.6.3',
      extra_packages=
        c('numpy','pandas','matplotlib==3.1.0','scikit-learn','h5py==2.10.0'),
      restart_session=F
    )
    renv::snapshot()
  }
}

rm(i)
```

```{r Load log for the expensive computation, include=FALSE}
# This is needed to print messages at the time we run the expensive computation.
source('data/log.R')
```

```{r Save package tables and the versions, eval=FALSE, include=FALSE}
rbind(
  
    # The base packages
    sessionInfo()$basePkgs %>%
      data.frame(
        package=.
        ,version=
          paste0(
            sessionInfo()$R.version$major
            ,'.'
            ,sessionInfo()$R.version$minor
          )
        ,base='Yes'
        ,loaded='Yes'
        ,attached='Yes'
      )
    
    # Additional packages specific to this study
    ,sessionInfo()$otherPkgs %>%
      lapply(function(x)data.frame(version=x$Version)) %>%
      do.call(rbind,.) %>%
      rownames_to_column(var='package') %>%
      mutate(base='No',loaded='Yes',attached='Yes')
    
    # The explicit dependencies
    ,sessionInfo()$loadedOnly %>%
      lapply(function(x)data.frame(version=x$Version)) %>%
      do.call(rbind,.) %>%
      rownames_to_column(var='package') %>%
      mutate(base='No',loaded='Yes',attached='No')
    
  ) %>%
  setNames(str_to_sentence(colnames(.))) %>%
  
  # Save for eTable 6.
  write_csv('data/supp_table_6.csv')
```

## Sampling procedures of the data source

The data source was a sample dataset of the whole health insurance database 
during 2015 and 2016 by cross-sectional design. Stratified random sampling was 
applied. The strata variable was constructed from 66,072 combinations of all 
the healthcare facilities (*n*=22,024) and category of family, which were: (1) 
a family of which members never visit the healthcare facilities; (2) a family 
of which members have visited only primary care; and (3) a family of which 
members have visited all levels of care. For each stratum, one to ten families 
were randomly included. This means only ten families were randomly included if 
more than that number, resulting 586,969 families with 1,697,452 subjects.

## The sampling procedures of the dataset in this study

We conducted non-essential data cleaning, e.g. revising the inconsistent name 
of states, estimating the healthcare identifiers, *et cetera*. These procedures 
were parts of our R package of medhist 0.1.0. No sampling was conducted.

```{r Read public data II tables from an R file, eval=FALSE, include=FALSE}
# This is the output of the raw data after being preprocessed by data.Rmd in:
# https://github.com/herdiantrisufriyana/medhist/preprocessing
public=readRDS('data/public2.rds')
```

```{r Combine tables, eval=FALSE, include=FALSE}
public$visits=
  
  # Access a list of visit datasets based on capitation, fee for service (FFS), 
  # and diagnosis-related group (DRG).
  public[c('visit_cap','visit_ffs','visit_drg')] %>%
  
  # For each element of the list, select 4 columns and bind the elements by row.
  lapply(select,visit_id,subject_id,healthcare_id,admission_date) %>%
  do.call(rbind,.) %>%
  
  # Join with diagnosis dataset by visit_id, follow the dataframe above.
  left_join(public$diagnosis,by='visit_id') %>%
  
  # Exclude diagnoses at admission since it's unlikely definitive.
  filter(!code_type%in%c('Admission diagnosis')) %>%
  select(-code_type) %>%
  
  # Find the earliest visit per subject and filter only the unique visits.
  group_by(subject_id) %>%
  mutate(db_start_date=min(admission_date)) %>%
  ungroup() %>%
  filter(!duplicated(.))
```

After the non-essential data cleaning, we applied retrospective cohort design, 
as described in the main text. For pregnant women, we use several codes for 
determining delivery or immediately after delivery care. The 220 codes are 
described (eTable 7 in R Markdown).

```{r Determine target population, eval=FALSE, include=FALSE}
public$target_population=
  
  ##### The health insurance holders of #####
  public$subject %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection
    data.frame(
        step='total'
        ,exc_visit=0
        ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
        ,exc_subject=0
        ,inc_subject=nrow(Y)
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]] %>%

  ##### 12-to-55-years-old (at visit between 2015 and 2016) #####
  filter(
    between(as.duration(as_date('2015-01-01')-birth_date)/dyears(1),12,55) |
    between(as.duration(as_date('2016-12-31')-birth_date)/dyears(1),12,55)
  ) %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    readRDS('data/selection.rds') %>%
      rbind(
        data.frame(
          step='age selection by 12 to 55 years old'
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]] %>%

  ##### females #####
  filter(sex=='female') %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    readRDS('data/selection.rds') %>%
      rbind(
        data.frame(
          step='sex selection by female'
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]] %>%

  ##### who visit healthcare providers #####
  filter(subject_id%in%public$visits$subject_id) %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    readRDS('data/selection.rds') %>%
      rbind(
        data.frame(
          step='visit healthcare providers'
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]]
```

```{r Determine pregnant women of target population, eval=FALSE, include=FALSE}
public$pregnancy_status=
  
  # Get pregnancy status of the target population.
  public$target_population %>%
  lapply(X=1,Y=.,Z=public$visits,K=public$annotation,function(X,Y,Z,K){
    
    # Use these regular expression for the target population
    # to find codes related to pregnancy.
    L=Z %>%
      filter(subject_id%in%Y$subject_id) %>%
      left_join(K,by='code') %>%
      filter(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
              'obstet'
              ,'pregnan'
              ,'labou?r\\s+'
              ,'deliver'
              ,'cesar'
              ,'natal'
              ,'z3[3467]'
              ,'o\\d+'
            ),collapse='|')
        )
      )
    
    # From the pregnancy-related visits, find the earliest and the latest.
    M=L %>%
      group_by(subject_id) %>%
      summarize(
        preg_e=min(admission_date)
        ,preg_l=max(admission_date)
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Find the related codes for the end of pregnancy.
    N=L %>%
      
      # This may include the false-positive codes
      filter(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'abort'
            ,'deliver'
            ,'cesar'
            ,'labou?r\\s+'
            ,'natal'
            ,'postpartum'
            ,'terminat'
            ,'o0[0123]'
            ,'o152'
            ,'o364'
            ,'o63'
            ,'o7[02]'
            ,'o8[579]'
            ,'o90'
            ,'z3[79]'
          ),collapse='|')
        )
      ) %>%
      
      # By manual inspection, exclude the false-positive codes.
      filter(!(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'aborter'
            ,'ante'
            ,'peri'
            ,'delayed'
            ,'false'
            ,'failed'
            ,'prenatal'
            ,'threatened'
            ,'o311'
            ,'z351'
            ,'o600'
            ,'o96'
          ),collapse='|')
        ) &
        !str_detect(
          str_to_lower(code)
          ,paste0(c(
            'o0[34568]'
            ,'o15[12]'
            ,'o7[02]'
            ,'o8[79]'
          ),collapse='|')
        )
      )) %>%
      
      # Record the codes for the end of pregnancy. 
      lapply(X=1,Y=.,function(X,Y){
        saveRDS(Y,'data/termination_codes.rds')
        Y
      }) %>%
      .[[1]] %>%
      
      # Find the earliest and latest dates of 
      # the related codes for  the end of pregnancy.
      group_by(subject_id) %>%
      summarize(
        termin_e=min(admission_date)
        ,termin_l=max(admission_date)
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Get visits after the end of the first pregnancy in the dataset period.
    O=L %>%
      left_join(N,by='subject_id') %>%
      filter(admission_date>termin_l)
    
    # Get the earliest date of the second-pregnancy visits.
    P=O %>%
      group_by(subject_id) %>%
      summarize(
        preg_e2=min(admission_date)
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Find the related codes for the end of the second pregnancy.
    Q=O %>%
      
      # This may include the false-positive codes
      filter(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'abort'
            ,'deliver'
            ,'cesar'
            ,'labou?r\\s+'
            ,'natal'
            ,'postpartum'
            ,'terminat'
            ,'o0[0123]'
            ,'o152'
            ,'o364'
            ,'o63'
            ,'o7[02]'
            ,'o8[579]'
            ,'o90'
            ,'z3[79]'
          ),collapse='|')
        )
      ) %>%
      
      # By manual inspection, exclude the false-positive codes.
      filter(!(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'aborter'
            ,'ante'
            ,'peri'
            ,'delayed'
            ,'false'
            ,'failed'
            ,'prenatal'
            ,'threatened'
            ,'o311'
            ,'z351'
            ,'o600'
            ,'o96'
          ),collapse='|')
        ) &
        !str_detect(
          str_to_lower(code)
          ,paste0(c(
            'o0[34568]'
            ,'o15[12]'
            ,'o7[02]'
            ,'o868'
            ,'o8[79]'
          ),collapse='|')
        )
      )) %>%
      
      # Find the earliest and latest dates of 
      # the related codes for  the end of the second pregnancy.
      group_by(subject_id) %>%
      summarize(
        termin_e2=suppressWarnings(min(admission_date))
        ,termin_l2=suppressWarnings(max(admission_date))
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Join offset dates to find the pregnancy periods.
    R=M %>%
      left_join(N,by='subject_id') %>%
      left_join(P,by='subject_id') %>%
      left_join(Q,by='subject_id') %>%
      select(
        subject_id
        ,preg_e
        ,termin_e
        ,termin_l
        ,preg_e2
        ,termin_e2
        ,termin_l2
        ,preg_l
        ,everything()
      )
    
    # Join the offset dates to the target population data
    # and compute the number of either gestation and termination.
    S=Y %>%
      left_join(R,by='subject_id') %>%
      mutate(
        gestation=
          as.integer(!is.na(preg_e))
          + as.integer(!is.na(preg_e2))
        ,termination=
          as.integer(!is.na(termin_l))
          + as.integer(!is.na(termin_l2))
      )
    
    # Filtered by gestation, select different offsets depending on
    # the pregnancy period, and combine the filtering results. 
    # This separates 2 pregnancy periods of a subject into 2 instances.
    rbind(
        filter(S,gestation==0) %>%
          select(
            subject_id,preg_e,termin_e,termin_l,preg_l,gestation,termination
          ) %>%
          mutate(gestation_n=0)
        ,filter(S,gestation==1) %>%
          select(
            subject_id,preg_e,termin_e,termin_l,preg_l,gestation,termination
          ) %>%
          mutate(gestation_n=1)
        ,filter(S,gestation==2) %>%
          select(
            subject_id,preg_e,termin_e,termin_l,preg_l,gestation,termination
          ) %>%
          mutate(gestation_n=1)
        ,filter(S,gestation==2) %>%
          select(
            subject_id,preg_e2,termin_e2,termin_l2,preg_l,gestation,termination
          ) %>%
          rename(preg_e=preg_e2,termin_e=termin_e2,termin_l=termin_l2) %>%
          mutate(gestation_n=2)
      ) %>%
      
      # If no termination for a pregnancy period of a subject,
      # then label the period as having censored outcome.
      mutate(censoring=is.na(termin_l)) %>%
      
      # Join to the target population data and arranged by subject_id
      left_join(
        S %>%
          select(
            -preg_e
            ,-termin_e
            ,-termin_l
            ,-preg_e2
            ,-termin_e2
            ,-termin_l2
            ,-preg_l
            ,-gestation
            ,-termination
          )
        ,by='subject_id'
      ) %>%
      arrange(
        factor(subject_id,S$subject_id %>% .[!duplicated(.)])
      )
    
  }) %>%
  .[[1]]
```

```{r Save termination codes as CSV, eval=FALSE, include=FALSE}
readRDS('data/termination_codes.rds') %>%
  
  # By manual inspection, check the termination codes.
  select(code,desc) %>%
  filter(!duplicated(.)) %>%
  arrange(code) %>%
  
  # Save for eTable 7.
  rename(description=desc) %>%
  setNames(str_to_title(colnames(.))) %>%
  write_csv('data/supp_table_7.csv')
```

```{r Determine outcome, eval=FALSE, include=FALSE}
public$outcome=
  
  # Filter visits from the target population only.
  public$visits %>%
  filter(subject_id %in% public$target_population$subject_id) %>%
  
  # Extract outcome, which is O42.
  extract_outcome(
    icd10_event='O42'
    ,latest_event=min
    ,day_to_event=0
    ,icd10_nonevent=''
    ,latest_nonevent=max
    ,day_to_nonevent=0
    ,verbose=0
  ) %>%
  
  # Join the pregnancy status data to the outcome data.
  left_join(public$pregnancy_status,by='subject_id') %>%
  
  # If non-event, the latest date should be 
  # the earliest date of the end of pregnancy.
  lapply(X=1,Y=.,function(X,Y){
    Z=Y %>%
      filter(!censoring & outcome=='nonevent') %>%
      mutate(latest_date=termin_e)
    K=Y %>%
      filter(!(!censoring & outcome=='nonevent'))
    rbind(Z,K) %>%
      arrange(factor(subject_id,unique(Y$subject_id)))
  }) %>%
  .[[1]] %>%
  
  # Get only column to filter visits before the outcome 
  # for each pregnancy episode.
  select(subject_id,latest_date,outcome,censoring,gestation_n)
```

```{r Determine target visits, eval=FALSE, include=FALSE}
public$target_visits=
  
  # Join the outcome data to the visit data.
  public$outcome %>%
  right_join(public$visits,by='subject_id') %>%
  select(visit_id, everything()) %>%
  
  # If the outcome is censored, take visits up to the outcome date.
  # Otherwise, take all visits.
  filter(!is.na(censoring)) %>%
  filter(censoring | (!censoring & admission_date<=latest_date)) %>%
  mutate(code=ifelse(is.na(code)|code=='','NA',code)) %>%
  
  # Take only codes that are available 
  # in all combinations of outcome and censoring status.
  lapply(X=1,Y=.,function(X,Y){
    Z=Y %>%
      select(outcome,censoring,code) %>%
      filter(!duplicated(.)) %>%
      mutate(count=1) %>%
      group_by(code) %>%
      summarize(count=sum(count)) %>%
      arrange(desc(count)) %>%
      filter(count==4) %>%
      pull(code)
    
    Y %>% filter(code%in%Z)
  }) %>%
  .[[1]] %>%
  
  # Take needed columns and make subject_id different 
  # between pregnancy episodes of the same subject.
  select(-censoring,-outcome,-latest_date) %>%
  unite(subject_id,subject_id,gestation_n,sep='.')
```

```{r Update related variables, eval=FALSE, include=FALSE}
# Take only subjects' outcome that are assigned to the target visits.
public$outcome2=
  public$outcome %>%
  unite(subject_id,subject_id,gestation_n,sep='.') %>%
  filter(subject_id%in%public$target_visits$subject_id)

# Take only subjects' pregnancy status that are assigned to the target visits.
public$pregnancy_status2=
  public$pregnancy_status %>%
  unite(subject_id,subject_id,gestation_n,sep='.') %>%
  filter(subject_id%in%public$target_visits$subject_id)

# Add more criterion to the target population.
public$target_population2=
  
  public$target_population %>%
  left_join(
    select(public$pregnancy_status,subject_id,gestation_n)
    ,by=c('subject_id')
  ) %>%
  unite(subject_id,subject_id,gestation_n,sep='.') %>%
  
  ##### before the latest date of event/non-event #####
  ##### and split if >1 pregnancies #####
  filter(subject_id%in%public$target_visits$subject_id) %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    step_desc='up to the latest date for uncensored and split if >1 pregnancies'
    readRDS('data/selection.rds') %>%
      filter(step!=step_desc) %>%
      rbind(
        data.frame(
          step=step_desc
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$target_visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$target_visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]]
```

```{r Describe selection results for sanity check, eval=FALSE, include=FALSE}
readRDS('data/selection.rds') %>%
  kable() %>%
  kable_classic()
```

```{r Describe pregnancy status, eval=FALSE, include=FALSE}
public$pregnancy_status %>%
  
  # Summarize the number and proportion of subjects with 0 to 2 gestation.
  select(subject_id,gestation) %>%
  filter(!duplicated(.)) %>%
  group_by(gestation) %>%
  summarize(n=n(),.groups='drop') %>%
  mutate(p=n/sum(n)) %>%
  
  # Join to compare with the subjects' pregnancy status 
  # that are assigned to the target visits.
  left_join(
    public$pregnancy_status2 %>%
      separate(subject_id,c('subject_id','gestation_n'),sep='\\.') %>%
      select(subject_id,gestation) %>%
      filter(!duplicated(.)) %>%
      group_by(gestation) %>%
      summarize(n2=n(),.groups='drop') %>%
      mutate(p2=n2/sum(n2))
    ,by='gestation'
  ) %>%
  kable() %>%
  kable_classic()
```

```{r Describe outcome per pregnancy status, eval=FALSE, include=FALSE}
public$pregnancy_status %>%
  
  # Join the ourcome data to the pregnancy status data.
  left_join(public$outcome,by=c('subject_id','gestation_n','censoring')) %>%
  
  # Summarize the number and proportion of subjects 
  # by combination of censoring status, outcome, and number of gestation.
  group_by(gestation_n,outcome,censoring) %>%
  summarize(n=n(),.groups='drop') %>%
  mutate(p=n/sum(n)) %>%
  
  # Join to compare with the subjects' pregnancy status 
  # that are assigned to the target visits.
  left_join(
    public$pregnancy_status2 %>%
      separate(subject_id,c('subject_id','gestation_n'),sep='\\.') %>%
      left_join(
        public$outcome2 %>%
          separate(subject_id,c('subject_id','gestation_n'),sep='\\.')
        ,by=c('subject_id','gestation_n','censoring')
      ) %>%
      mutate(gestation_n=as.numeric(gestation_n)) %>%
      group_by(gestation_n,outcome,censoring) %>%
      summarize(n2=n(),.groups='drop') %>%
      mutate(p2=n2/sum(n2))
    ,by=c('gestation_n','outcome','censoring')
  ) %>%
  kable() %>%
  kable_classic()
```

```{r Save selection results to RDS file, eval=FALSE, include=FALSE}
saveRDS(public$target_population2,'data/target_population.rds')
saveRDS(public$pregnancy_status2,'data/pregnancy_status.rds')
saveRDS(public$outcome2,'data/outcome.rds')
saveRDS(public$target_visits,'data/target_visits.rds')
saveRDS(public$annotation,'data/annotation.rds')
```

```{r Remove public data II after selection, eval=FALSE, include=FALSE}
rm(public)
```

## Data preprocessing

We conducted data pre-processing after defining the target population and 
sampling it retrospectively. Demographics were included as categorical 
variables for association tests. Then, we applied systematic human learning, as 
described in the main text, to determine what were latent candidate predictors 
that can be inferred from our dataset. We also computed a number of days for a 
code in the latest encounter before the time of prediction, including those by 
codes as a latent candidate predictor.

```{r Load an outcome table, include=FALSE}
outcome=
  readRDS('data/outcome.rds') %>%
  left_join(
    readRDS('data/pregnancy_status.rds')
    ,by=c('subject_id','censoring')
  ) %>%
  rename(reghc_id=healthcare_id)
```

```{r Visits with categorical identity as additional variables, include=FALSE}
visit_cip=
  
  # Transform age into a categorical variable based on domain knowledge.
  outcome %>%
  mutate(
    age=case_when(
      ((latest_date-birth_date)/dyears(1))<20~'Too Young'
      ,(((latest_date-birth_date)/dyears(1))>=20 &
        ((latest_date-birth_date)/dyears(1))<=35)
       ~'Low Risk'
      ,((latest_date-birth_date)/dyears(1))>35~'Too Old'
      ,TRUE~'NA'
    )
  ) %>%
  
  # Select subject_id and all categorical variables (not medical history).
  select(subject_id,age,marital_status,insurance_class,occupation_segment) %>%
  gather(variable,value,-subject_id) %>%
  unite(desc,variable,value,sep='_') %>%
  mutate(desc=str_replace_all(desc,'_|-|\\s',' ')) %>%
  separate(desc,c('first_word','desc'),sep=' ',extra='merge') %>%
  mutate_at('first_word',str_to_sentence) %>%
  unite(desc,first_word,desc,sep=' ') %>%
  
  # Create a code for each category.
  lapply(X=1,Y=.,function(X,Y){
    select(.,desc) %>%
      filter(!duplicated(.)) %>%
      
      # Create a code for each categorical variable.
      mutate(code=sapply(desc,function(x){
        str_split(x,'\\s')[[1]] %>%
          substr(1,1) %>%
          paste(collapse='') %>%
          str_to_upper()
      })) %>%
      
      # For each code, assign number to non-single code.
      group_by(code) %>%
      mutate(code_seq=seq(n()),max_code_seq=n()) %>%
      ungroup() %>%
      unite(code2,code,code_seq,sep='',remove=F) %>%
      mutate(code=ifelse(max_code_seq==1,code,code2)) %>%
      
      # Save all categorical variables and the codes.
      select(code,desc) %>%
      saveRDS('data/cat_identity.rds')
    Y
  }) %>%
  .[[1]] %>%
  
  # Join the codes of the categorical variables.
  left_join(readRDS('data/cat_identity.rds'),by='desc') %>%
  
  # Join the target visits.
  left_join(
    readRDS('data/target_visits.rds') %>%
      select(-code) %>%
      mutate(seq=seq(nrow(.))) %>%
      select(seq,everything())
    ,by='subject_id'
  ) %>%
  
  # Select columns according to the standard for medhist.
  select(
    seq
    ,visit_id
    ,subject_id
    ,healthcare_id
    ,admission_date
    ,code
    ,db_start_date
  ) %>%
  
  # Add the categorical variables as if these are diagnosis/procedure codes 
  # of which dates are the dates of any codes in a visit of a subject.
  rbind(
    readRDS('data/target_visits.rds') %>%
      mutate(seq=seq(nrow(.))) %>%
      select(seq,everything())
  )
```

```{r Convenient sampling from an authoritative, include=FALSE}
convenient_sampling=
  matrix(
    c('variable'
      ,'PROM'
      ,'Preterm birth'
      ,'Multiple pregnancy'
      ,'IAI'
      ,'Previous PROM'
      ,'Cervical shortening'
      ,'APH'
      ,'Low BMI'
      ,'Low SES'
      ,'Cigarette smoking'
      ,'Illicit drug use'
    )
    ,ncol=1,byrow=T
  ) %>%
  `colnames<-`(.[1,]) %>%
  .[-1,,drop=F] %>%
  as.data.frame()
```

```{r Show convenient sampling result, eval=FALSE, include=FALSE}
convenient_sampling %>%
  lapply(X=seq(nrow(.)-1),Y=.,function(X,Y){
    data.frame(
      variable1=Y$variable[X]
      ,variable2=Y$variable[(X+1):nrow(Y)]
    )
  }) %>%
  do.call(rbind,.) %>%
  kable() %>%
  kable_classic()
```

```{r Snowball sampling results, include=FALSE}
dag=list()

# Take notes while applying systematic human learning 
# by snowball sampling based on the convenient sampling.
dag$factor=
  matrix(
    c('dependent_factor','independent_factor','citation'
      ,'Preterm birth','PROM','ACOG (2016a)'
      ,'PROM','Previous preterm birth','ACOG (2016a)'
      ,'PROM','Family history of preterm birth','ACOG (2016a)'
      ,'PROM','Multiple pregnancy','ACOG (2016a)'
      ,'PROM','IAI','ACOG (2016a)'
      ,'PROM','Previous PROM','ACOG (2016a)'
      ,'PROM','Cervical shortening','ACOG (2016a)'
      ,'PROM','APH','ACOG (2016a)'
      ,'PROM','Low BMI','ACOG (2016a)'
      ,'PROM','Low SES','ACOG (2016a)'
      ,'PROM','Cigarette smoking','ACOG (2016a)'
      ,'PROM','Illicit drug use','ACOG (2016a)'
      ,'PROM','Maternal age','Song J, et al (2019)'
      ,'PROM','Race','Fiscella K (1996)'
      ,'PROM','Low education','Wang W, et al (2020)'
      ,'PROM','Stress','Wang W, et al (2020)'
      ,'PROM','GTI','Pandey D, et al (2019); Yan JJ, et al (2016)'
      ,'PROM','Periodontal disease','Figueiredo MGOP, et al (2019)'
      ,'PROM','Uterine anomaly','Hua M, et al (2011)'
      ,'PROM','Polyhydramnios','Odibo IN, et al (2016)'
      ,'PROM','Pneumonia','Getahun D, et al (2007)'
      ,'PROM','Tuberculosis','FernÃ¡ndez AA, et al (2017)'
      ,'PROM','Assisted reproduction','Lei LL, et al (2019)'
      ,'PROM','Chorioamnionitis','Fukami T, et al (2017)'
      ,'PROM','Conization','Zhuang H, et al (2019)'
      ,'PROM','Placenta on anterior wall','Torricelli M, et al (2015)'
      ,'PROM','Asthma','Baghlaf H, et al (2019)'
      ,'PROM','Influenza','Littauer EQ, et al (2017)'
      
      ,'Preterm birth','Multiple pregnancy'
          ,'ACOG (2016b); Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','IAI','ACOG (2016a)'
      ,'Preterm birth','Previous preterm birth','ACOG (2016a)'
      ,'Preterm birth','Previous PROM','ACOG (2016a)'
      ,'Preterm birth','Family history of preterm birth'
          ,'Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Cervical shortening','ACOG (2016a)'
      ,'Preterm birth','APH','ACOG (2016a)'
      ,'Preterm birth','Low BMI','ACOG (2016a)'
      ,'Preterm birth','Low SES','ACOG (2016a)'
      ,'Preterm birth','Cigarette smoking','ACOG (2016a)'
      ,'Preterm birth','Illicit drug use','ACOG (2016a)'
      ,'Preterm birth','Low education','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Maternal age','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Race','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Stress','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Depression','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','UTI','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','GTI','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Periodontal disease','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Uterine anomaly','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Previous stillbirth','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Induced abortion','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Polyhydramnios','Frey HA, and Klebanoff MA (2016)'
      ,'Preterm birth','Pneumonia','Stinson LF, et al (2019)'
      ,'Preterm birth','Typhoid','Stinson LF, et al (2019)'
      ,'Preterm birth','Malaria','Stinson LF, et al (2019)'
      ,'Preterm birth','Tuberculosis','Stinson LF, et al (2019)'
      ,'Preterm birth','Pyelonephritis','Stinson LF, et al (2019)'
      ,'Preterm birth','Assisted reproduction','Chen M, and Heilbronn LK (2017)'
      ,'Preterm birth','Chorioamnionitis','Kim CJ, et al (2015)'
      ,'Preterm birth','Conization','Jolley JA, and Wing DA (2008)'
      ,'Preterm birth','LEEP loop','Jolley JA, and Wing DA (2008)'
      ,'Preterm birth','Placenta in anterior wall','Sekiguchi A, et al (2013)'
      ,'Preterm birth','Parity','Shechter-Maor G, et al (2020)'
      ,'Preterm birth','Anorexia nervosa','Ante Z, et al (2020)'
      ,'Preterm birth','Type-2 diabetes mellitus','Ringholm L, et al (2019)'
      ,'Preterm birth','Anemia','Ardic C, et al (2019)'
      ,'Preterm birth','Asthma','Baghlaf H, et al (2019)'
      ,'Preterm birth','Chromosomal aberration','Toutain J, et al (2018)'
      ,'Preterm birth','Influenza','Meijer WJ, et al (2015)'
      ,'Preterm birth','Acute respiratory syndrome','Wong SF, et al (2004)'
      
      ,'Multiple pregnancy','Assisted reproduction'
          ,'Thilaganathan B, and Khalil A (2014)'
      ,'Multiple pregnancy','Maternal age'
          ,paste(
            'Thilaganathan B, and Khalil A (2014);'
            ,'Martin JA, and Osterman MJK (2019)'
          )
      ,'Multiple pregnancy','Race','Martin JA, and Osterman MJK (2019)'
      
      ,'Chorioamnionitis','IAI','Tantengco OAG, et al (2019)'
      ,'IAI','UTI','Tantengco OAG, et al (2019)'
      ,'IAI','GTI','Romero R, et al (2019); Tantengco OAG, et al (2019)'
      ,'IAI','Periodontal disease','Stinson LF, et al (2019)'
      ,'IAI','Pneumonia','Stinson LF, et al (2019)'
      ,'IAI','Multiple pregnancy','Lee SM, et al (2020)'
      ,'IAI','Cervical shortening','Kiefer DG, et al (2009)'
      ,'IAI','Race','Menon R, et al (2011)'
      ,'IAI','Parity','Park KH, et al (2012)'
      
      ,'Cervical shortening','Uterine anomaly','GuimarÃ£es FHA, et al (2013)'
      ,'Cervical shortening','Conization','GuimarÃ£es FHA, et al (2013)'
      ,'Cervical shortening','LEEP loop','GuimarÃ£es FHA, et al (2013)'
      ,'Cervical shortening'
          ,'Isthmo-cervical incompetence','GuimarÃ£es FHA, et al (2013)'
      ,'Cervical shortening','Maternal age','Tan P C, et al (2011)'
      ,'Cervical shortening','Race','Dijkstra K, et al (1999)'
      ,'Cervical shortening','Stress','Dijkstra K, et al (1999)'
      ,'Cervical shortening','GTI','Sendag F, et al (2010)'
      ,'Cervical shortening','Parity','Kang WS, et al (2010)'
      
      ,'APH','Maternal age','Fan D, et al (2017)'
      ,'APH','Gestational age','Fan D, et al (2017)'
      ,'APH','Placenta on anterior wall','Fan D, et al (2017)'
      ,'APH','Parity','Fan D, et al (2017)'
      ,'APH','Low SES','Bhandari S, et al (2014)'
      ,'APH','Cigarette smoking','Bhandari S, et al (2014)'
      ,'APH','Illicit drug use','Bhandari S, et al (2014)'
      ,'APH','Race','Shen JJ, et al (2005)'
      ,'APH','Assisted reproduction','Qin J, et al (2016)'
      
      ,'Low BMI','Anorexia nervosa','BjÃ¸rnholt SM, et al (2019)'
      ,'Low BMI','Diet restrictions','BjÃ¸rnholt SM, et al (2019)'
      ,'Low BMI','Stress','Han YS, et al (2011)'
      
      ,'Cigarette smoking','Pregnancy','Najman JM, et al (1998)'
      ,'Cigarette smoking','Low SES','Najman JM, et al (1998)'
      ,'Cigarette smoking','Maternal age','De Genna NM, et al (2017)'
      ,'Cigarette smoking','Race','De Genna NM, et al (2017)'
      ,'Cigarette smoking','Low education','De Genna NM, et al (2017)'
      
      ,'Illicit drug use','Low SES','Marzban M, et al (2019)'
      ,'Illicit drug use','Cigarette smoking'
          ,'Oga EA, et al (2018); De Genna NM, et al (2017)'
      ,'Illicit drug use','Maternal age','Homsup P, et al (2018)'
      ,'Illicit drug use','Race','Homsup P, et al (2018)'
      ,'Illicit drug use','Low education','Homsup P, et al (2018)'
      ,'Illicit drug use','Stress','Rocha PC, et al (2016)'
      
      ,'GTI','Type-2 diabetes mellitus','Nichols GA, et al (2017)'
      ,'GTI','Tuberculosis','Sharma JB, et al (2018)'
      
      ,'Periodontal disease','Anemia','Genco RJ, and Borgnakke WS (2013)'
      ,'Periodontal disease','Low education','Genco RJ, and Borgnakke WS (2013)'
      ,'Periodontal disease','Allergy','Genco RJ, and Borgnakke WS (2013)'
      ,'Periodontal disease','Maternal age','Genco RJ, and Borgnakke WS (2013)'
      ,'Periodontal disease','Type-2 diabetes mellitus'
          ,'Genco RJ, and Borgnakke WS (2013)'
      ,'Periodontal disease','Cigarette smoking'
          ,'Genco RJ, and Borgnakke WS (2013)'
      ,'Periodontal disease','Stress','Genco RJ, and Borgnakke WS (2013)'
      ,'Periodontal disease','Asthma','Moraschini V, et al (2018)'
      ,'Periodontal disease','Anorexia nervosa','Chiba FY, et al (2019)'
      
      ,'Polyhydramnios','Fetal anomalies','Moise KJ (1997)'
      ,'Polyhydramnios','Multiple pregnancy','Moise KJ (1997)'
      ,'Polyhydramnios','Type-2 diabetes mellitus','Moise KJ (1997)'
      ,'Polyhydramnios','Chromosomal aberration'
          ,'Sagi-Dain L, and Sagi S (2015)'
      ,'Polyhydramnios','Assisted reproduction','Lei LL, et al (2019)'
      ,'Polyhydramnios','Anemia','Mathew M, et al (2008)'
      
      ,'Pneumonia','Asthma','Goodnight WH, and Soper DE (2005)'
      ,'Pneumonia','Anemia','Goodnight WH, and Soper DE (2005)'
      ,'Pneumonia','Varicella','Goodnight WH, and Soper DE (2005)'
      ,'Pneumonia','Influenza','Goodnight WH, and Soper DE (2005)'
      ,'Pneumonia','Acute respiratory syndrome'
          ,'Goodnight WH, and Soper DE (2005)'
      
      ,'Tuberculosis','Race','MalhamÃ© I, et al (2016)'
      ,'Tuberculosis','Low SES','MalhamÃ© I, et al (2016)'
      ,'Tuberculosis','Asthma','Yii AC, et al (2019)'
      ,'Tuberculosis','Influenza','de Paus RA, et al (2013)'
      
      ,'Chorioamnionitis','Gestational age','Kim CJ, et al (2015)'
      ,'Chorioamnionitis','Cigarette smoking','Kim CJ, et al (2015)'
      ,'Chorioamnionitis','Parity','Kim CJ, et al (2015)'
      ,'Chorioamnionitis','Varicella','Kim CJ, et al (2015)'
      ,'Chorioamnionitis','Influenza','Kim CJ, et al (2015)'
      ,'Chorioamnionitis','Acute respiratory syndrome','Kim CJ, et al (2015)'
      ,'Chorioamnionitis','Asthma','Baghlaf H, et al (2019)'
      
      ,'Conization','Cervical neoplasia','Van Calsteren K, et al (2005)'
      
      ,'Asthma','Varicella','Murphy VE, et al (2017)'
      ,'Asthma','Influenza','Murphy VE, et al (2017)'
      )
    ,ncol=3,byrow=T
  ) %>%
  `colnames<-`(.[1,]) %>%
  .[-1,] %>%
  as.data.frame()

# Make a common-cause table.
dag$common_cause$df=
  dag$factor %>%
  lapply(X=seq(sum(!duplicated(.$dependent_factor))-1)
         ,Y=pull(.,dependent_factor) %>% .[!duplicated(.)]
         ,Z=.,function(X,Y,Z){
    
    data.frame(V1=Y[X],V2=Y[(X+1):length(Y)]) %>%
      mutate(
        
        # Common causes
        V1_V2=sapply(X=seq(nrow(.)),Y=V1,Z=V2,K=Z,function(X,Y,Z,K){
          intersect(
              filter(K,dependent_factor==Y[X])$independent_factor
              ,filter(K,dependent_factor==Z[X])$independent_factor
            ) %>%
            paste(collapse=',')
        })
        
        # Causes of the first variable only
        ,V1_only=sapply(X=seq(nrow(.)),Y=V1,Z=V2,K=Z,function(X,Y,Z,K){
          setdiff(
              filter(K,dependent_factor==Y[X])$independent_factor
              ,filter(K,dependent_factor==Z[X])$independent_factor
            ) %>%
            paste(collapse=',')
        })
        
        # Causes of the second variable only
        ,V2_only=sapply(X=seq(nrow(.)),Y=V1,Z=V2,K=Z,function(X,Y,Z,K){
          setdiff(
              filter(K,dependent_factor==Z[X])$independent_factor
              ,filter(K,dependent_factor==Y[X])$independent_factor
            ) %>%
            paste(collapse=',')
        })
      )
  }) %>%
  do.call(rbind,.)

# Identify the unique common causes.
dag$common_cause$vec=
  unlist(lapply(dag$common_cause$df$V1_V2,str_split,pattern=',')) %>%
  .[.!='' & !duplicated(.)]

# Bind tables for the edges.
dag$baseline_edges=
  rbind(
    filter(dag$factor,dependent_factor=='PROM')
    ,filter(dag$factor,!dependent_factor%in%c('PROM','Preterm birth')) %>%
      filter(independent_factor %in% dag$common_cause$vec)
  )

# Construct the nodes only.
dag$baseline_nodes=
  
  # Use either independent or dependent factor.
  c(dag$baseline_edges$dependent_factor
    ,dag$baseline_edges$independent_factor) %>%
  .[!duplicated(.)] %>%
  data.frame(name=.) %>%
  
  # Assign a code for each causal factor.
  `rownames<-`(str_pad(seq(nrow(.)),str_count(max(nrow(.))),'left','0')) %>%
  rownames_to_column(var='label') %>%
  
  # Apply different prefixes for outcome (Y), 
  # and the first (A) and second-level factors (L)
  mutate(
    label=
      paste0(
        case_when(
          name=='PROM'
          ~'Y'
          ,name %in%
           filter(dag$factor,dependent_factor=='PROM')$independent_factor
           ~'A'
          ,TRUE~'L'
        )
        ,label
      )
  ) %>%
  select(name,label)

# Construct the edges that connect the common causes to the effects.
dag$baseline_edges=
  dag$baseline_edges %>%
  left_join(
    setNames(dag$baseline_nodes,c('independent_factor','from'))
    ,by='independent_factor'
  ) %>%
  left_join(
    setNames(dag$baseline_nodes,c('dependent_factor','to'))
    ,by='dependent_factor'
  ) %>%
  select(from,to,citation)
```

```{r Show snowball sampling results, eval=FALSE, include=FALSE}
# Common cause table
dag$common_cause$df %>%
  kable() %>%
  kable_classic()

# Outcome with the first- and second-level factors
dag$baseline_nodes %>%
  kable() %>%
  kable_classic()

# All edges and the citations
dag$baseline_edges %>%
  left_join(
    dag$baseline_nodes %>%
      rename(from=label) %>%
      mutate(label=paste(from,name)) %>%
      select(from,label)
    ,by='from'
  ) %>%
  select(-from) %>%
  rename(from=label) %>%
  left_join(
    dag$baseline_nodes %>%
      rename(to=label) %>%
      mutate(label=paste(to,name)) %>%
      select(to,label)
    ,by='to'
  ) %>%
  select(-to) %>%
  rename(to=label) %>%
  select(from,to,everything()) %>%
  kable() %>%
  kable_classic()
```

```{r Determine measurement of causal factors, include=FALSE}
# Add measurement variables.
dag$measure_edges=
  dag$baseline_nodes %>%
  select(label) %>%
  rename(from=label) %>%
  mutate(to=paste0(from,'*'),citation=NA)

# Define components of each causal factor by ICD-10 coding.
dag$measure_nodes=
  dag$baseline_nodes %>%
  mutate(
    regex=case_when(
      name=='PROM'~'O42'
      ,name=='Multiple pregnancy'~'O3[01]'
      ,name=='Chorioamnionitis'~'O411'
      ,name=='IAI'~'O41[89]'
      ,name=='Cervical shortening'~'6751'
      ,name=='APH'~'O46'
      ,name=='Illicit drug use'~'F19'
      ,name=='GTI'~'914[139]|A6[03]|O23[59]|R87'
      ,name=='Periodontal disease'~'K05'
      ,name=='Polyhydramnios'~'O40'
      ,name=='Pneumonia'~'J1[23458]'
      ,name=='Tuberculosis'~'A1[569]'
      ,name=='Asthma'~'J4[56]'
      ,name=='Low SES'~'THIRD|UNEMPLOYED'
      ,name=='Maternal age'~'TOO YOUNG|TOO OLD'
      ,name=='Uterine anomaly'~'Q51'
      ,name=='Assisted reproduction'~'Z31|N98'
      ,name=='Influenza'~'J(00|09|10|11)'
      ,name=='UTI'~'N390|O23[01249]'
      ,name=='Parity'~'Z641'
      ,name=='Anorexia nervosa'~'F500|R63[0346]'
      ,name=='Type-2 diabetes mellitus'~'E1[124]'
      ,name=='Anemia'~'D5[^4]|D6[0-4]'
      ,name=='Chromosomal aberration'~'Q9'
      ,name=='Varicella'~'B01'
      ,name=='Acute respiratory syndrome'~'J80'
      ,TRUE~''
    )
  ) %>%
  filter(regex!='') %>%
  lapply(X=seq(nrow(.)),Y=.,function(X,Y){
    data.frame(name=Y$regex[X],label=paste0(Y$label[X],'*'))
  }) %>%
  do.call(rbind,.)

# For each measurement variable, add unmeasured variable 
# as a factor that affects the measurement variable.
dag$measure_edges=
  dag$measure_edges %>%
  filter(to %in% dag$measure_nodes$label) %>%
  rbind(mutate(.,from=str_replace_all(from,'[:alpha:]','U')))

# Assign labels of measurement variables with the ICD-10 codes.
dag$measure_nodes %>%
  lapply(X=seq(nrow(.))
         ,Y=.
         ,Z=
           readRDS('data/cat_identity.rds') %>%
           rbind(readRDS('data/annotation.rds'))
         ,function(X,Y,Z){
    
    if(Y$label[X]%in%c('A19*','A20*')){
      K=Z %>%
        filter(!str_detect(str_to_upper(code),'[:digit:]')) %>%
        filter(str_detect(str_to_upper(desc),Y$name[X]))
    }else{
      K=Z %>% filter(str_detect(str_to_upper(code),Y$name[X]))
    }
    
    K %>%
      mutate(label=Y$label[X]) %>%
      select(label,code,desc)
    
  }) %>%
  do.call(rbind,.) %>%
  kable() %>%
  kable_classic()
```

```{r Show all available data or measures, eval=FALSE, include=FALSE}
dag$measure_edges %>%
  kable() %>%
  kable_classic()
```

```{r Compute nationwide day interval of medical history, include=FALSE}
if(run_heavy_computation){
  mh_nationwide=
    readRDS('data/target_visits.rds') %>%
    mutate(healthcare_id='nationwide') %>%
    extract_medical_history(cl=detectCores()-1)
  saveRDS(mh_nationwide,'data/mh_nationwide.rds')
}else{
  cat(readRDS('data/log.rds')[['mh_nationwide']])
  mh_nationwide=readRDS('data/mh_nationwide.rds')
}
```

```{r Compute provider-wise day interval of medical history, include=FALSE}
if(run_heavy_computation){
  mh_provider=
    readRDS('data/target_visits.rds') %>%
    extract_medical_history(cl=detectCores()-1)
  saveRDS(mh_provider,'data/mh_provider.rds')
}else{
  cat(readRDS('data/log.rds')[['mh_provider']])
  mh_provider=readRDS('data/mh_provider.rds')
}
```

```{r Compute nationwide day interval of causal factor, include=FALSE}
if(run_heavy_computation){
  cf_nationwide=
    
    # Exclude outcome.
    dag$measure_nodes %>%
    filter(label!='Y01*') %>%
    
    # For each causal factor, filter rows with the codes related to the factor 
    # regardless the healthcare facilities.
    lapply(X=seq(nrow(.))
           ,Y=.
           ,Z=
             visit_cip %>%
             mutate(healthcare_id='nationwide') %>%
             left_join(
               readRDS('data/cat_identity.rds') %>%
                 rbind(annotation)
               ,by='code'
             )
           ,function(X,Y,Z){
      
      if(Y$label[X]%in%c('A19*','A20*')){
        K=Z %>%
          filter(!str_detect(str_to_upper(code),'[:digit:]')) %>%
          filter(str_detect(str_to_upper(desc),Y$name[X]))
      }else{
        K=Z %>% filter(str_detect(str_to_upper(code),Y$name[X]))
      }
      
      K %>%
        select(-desc) %>%
        mutate(
          code=
            Y$label[X] %>%
            str_remove_all('\\*')
        )
      
    }) %>%
    do.call(rbind,.) %>%
    filter(!duplicated(.)) %>%
    select(-seq) %>%
    
    # Extract the day intervals.
    extract_medical_history(cl=detectCores()-1) %>%
    filter(!duplicated(.)) %>%
    
    # Join to the target visits.
    right_join(
      readRDS('data/target_visits.rds') %>%
        select(-code) %>%
        mutate(healthcare_id='nationwide')
      ,by=c('visit_id'
            ,'subject_id'
            ,'healthcare_id'
            ,'admission_date'
            ,'db_start_date')
    ) %>%
    select(
      visit_id
      ,subject_id
      ,healthcare_id
      ,admission_date
      ,db_start_date
      ,everything()
    )
  
  saveRDS(cf_nationwide,'data/cf_nationwide.rds')
}else{
  cat(readRDS('data/log.rds')[['cf_nationwide']])
  cf_nationwide=readRDS('data/cf_nationwide.rds')
}
```

```{r Compute provider-wise day interval of causal factor, include=FALSE}
if(run_heavy_computation){
  cf_provider=
    
    # Exclude outcome.
    dag$measure_nodes %>%
    filter(label!='Y01*') %>%
    
    # For each causal factor, filter rows with the codes related to the factor 
    # in each of the healthcare facilities.
    lapply(X=seq(nrow(.))
           ,Y=.
           ,Z=
             visit_cip %>%
             left_join(
               readRDS('data/cat_identity.rds') %>%
                 rbind(annotation)
               ,by='code'
             )
           ,function(X,Y,Z){
      
      if(Y$label[X]%in%c('A19*','A20*')){
        K=Z %>%
          filter(!str_detect(str_to_upper(code),'[:digit:]')) %>%
          filter(str_detect(str_to_upper(desc),Y$name[X]))
      }else{
        K=Z %>% filter(str_detect(str_to_upper(code),Y$name[X]))
      }
      
      K %>%
        select(-desc) %>%
        mutate(
          code=
            Y$label[X] %>%
            str_remove_all('\\*')
        )
      
    }) %>%
    do.call(rbind,.) %>%
    filter(!duplicated(.)) %>%
    select(-seq) %>%
    
    # Extract the day intervals.
    extract_medical_history(cl=detectCores()-1) %>%
    filter(!duplicated(.)) %>%
    
    # Join to the target visits.
    right_join(
      readRDS('data/target_visits.rds') %>%
        select(-code)
      ,by=c('visit_id'
            ,'subject_id'
            ,'healthcare_id'
            ,'admission_date'
            ,'db_start_date')
    ) %>%
    select(
      visit_id
      ,subject_id
      ,healthcare_id
      ,admission_date
      ,db_start_date
      ,everything()
    )
  
  saveRDS(cf_provider,'data/cf_provider.rds')
}else{
  cat(readRDS('data/log.rds')[['cf_provider']])
  cf_provider=readRDS('data/cf_provider.rds')
}
```

```{r Sanity check for the number of visits and subjects, include=FALSE}
source('R/outcome_visit_subject-function.R')
```

```{r Show sanity check results, eval=FALSE, include=FALSE}
rbind(
    readRDS('data/target_visits.rds') %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('original')
    
    ,mh_nationwide %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('nationwide')
    
    ,mh_provider %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('provider')
    
    ,cf_nationwide %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('nationwide, causal')
    
    ,cf_provider %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('provider, causal')
  ) %>%
  kable() %>%
  kable_classic()
```

```{r Build a function to personalize PROM tidy set, include=FALSE}
source('R/prom_tidyset_personalization-function.R')
```

## Data partition

To ensure all inference or derivation using training set only, we need to 
conduct data partition before continuing the downstream analysis. We described 
data partition for model validation in the main text (see Methods).

```{r Build a function to split for external validation, include=FALSE}
source('R/extv-function.R')
```

```{r Compile tidy sets followed by data partitioning, include=FALSE}
if(run_heavy_computation){
  
  # Nationwide data for inference by medical histories with splitting 
  # information for external validation
  inferdata=
    mh_nationwide %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for causal inference'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded across
              healthcare providers nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=readRDS('data/annotation.rds')
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(inferdata,'data/inferdata.rds')
  
  # Provider-wise data for prediction by medical histories with splitting 
  # information for external validation
  predidata=
    mh_provider %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for prediction'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded
              individually within each healthcare provider nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=readRDS('data/annotation.rds')
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(predidata,'data/predidata.rds')
  
  # Nationwide data for inference by causal factors with splitting information 
  # for external validation
  infercause=
    cf_nationwide %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for causal inference'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The codes are re-assigned into causal factors (defined by regex).
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded across
              healthcare providers nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=
        dag$measure_nodes %>%
        mutate(label=str_remove_all(label,'\\*')) %>%
        rename(code=label,desc=name) %>%
        select(code,desc)
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(infercause,'data/infercause.rds')
  
  # Provider-wise data for prediction by causal factors with splitting 
  # information for external validation
  predicause=
    cf_provider %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for prediction'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The codes are re-assigned into causal factors (defined by regex).
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded
              individually within each healthcare provider nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=
        dag$measure_nodes %>%
        mutate(label=str_remove_all(label,'\\*')) %>%
        rename(code=label,desc=name) %>%
        select(code,desc)
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(predicause,'data/predicause.rds')
  
}else{
  inferdata=readRDS('data/inferdata.rds')
  predidata=readRDS('data/predidata.rds')
  infercause=readRDS('data/infercause.rds')
  predicause=readRDS('data/predicause.rds')
}
```

```{r Show data partition, eval=FALSE, include=FALSE}
inferdata %>%
  lapply(X=1,Y=.,function(X,Y){
    
    # Use phenotype and protocol data.
    cbind(
        pData(phenoData(Y))
        ,pData(protocolData(Y))
      ) %>%
      separate(subject_id,c('subject_id','gestation_n'),sep='\\.') %>%
      select(gestation_n,outcome,censoring,geo,tem,bgt,ran,int) %>%
      gather(partition,value,-gestation_n,-outcome,-censoring) %>%
      filter(value) %>%
      
      # Summarize number of instances 
      # by outcome, censoring status, and partition.
      group_by(gestation_n,outcome,censoring,partition) %>%
      summarize(n=n(),.groups='drop') %>%
      
      # Compute the statistics.
      group_by(partition) %>%
      mutate(subtotal=sum(n)) %>%
      ungroup() %>%
      mutate(total=ncol(Y)) %>%
      mutate(p=subtotal/total) %>%
      
      # Wrap up.
      select(partition,subtotal,total,p) %>%
      filter(!duplicated(.)) %>%
      arrange(factor(partition,c('int','ran','geo','tem','bgt')))
  }) %>%
  .[[1]] %>%
  kable() %>%
  kable_classic()
```

## Association tests

We conducted association tests as described in the main text. This will help us 
to include only the confirmed associated predictors as latent candidate 
predictors before conducting pre-selection of those candidates to fulfill 
quality control of predictors in the main text. We included latent candidate 
predictors of which the data were available in training set. Details on this 
information and International Classification of Disease version 10 (ICD-10) 
codes or demographical variables for each candidate of latent candidate 
predictors are shown in the next section. A protocol of systematic human 
learning was followed for these association tests [...].

```{r Unified causal diagram (caugram) plot function, include=FALSE}
source('R/plot_dag-function.R')
```

```{r Show caugram, fig.height=10.5, fig.width=10.5, eval=FALSE, include=FALSE}
dag$baseline_edges[,-3] %>%
  plot_dag(0,mode='in',circular=T,plot.title='Causal diagram')
```

```{r Create interactive causal diagram, include=FALSE}
pre_causal_diagram=
  dag$baseline_edges[,-3] %>%
  
  # Join the node table as 'from'.
  left_join(
    dag$baseline_nodes %>%
      rename(from=label) %>%
      mutate(label=paste(from,name)) %>%
      select(from,label)
    ,by='from'
  ) %>%
  select(-from) %>%
  rename(from=label) %>%
  
  # Join the node table as 'to'.
  left_join(
    dag$baseline_nodes %>%
      rename(to=label) %>%
      mutate(label=paste(to,name)) %>%
      select(to,label)
    ,by='to'
  ) %>%
  select(-to) %>%
  rename(to=label)
```

```{r Show the interactive causal diagram, eval=FALSE, include=FALSE}
pre_causal_diagram %>%
  gather(key,id) %>%
  select(id) %>%
  filter(!duplicated(.)) %>%
  mutate(label=id) %>%
  arrange(id) %>%
  mutate(
    color=
      unlist(mapply(
        RColorBrewer::brewer.pal
        ,RColorBrewer::brewer.pal.info %>%
          .[.$category=='qual',] %>%
          .$maxcolors
        ,RColorBrewer::brewer.pal.info %>%
          .[.$category=='qual',] %>%
          rownames()
      ))[seq(nrow(.))]
  ) %>%
  visNetwork(
    pre_causal_diagram %>%
      mutate(arrows='to')
    ,width='100%'
    ,height=700
  ) %>%
  visIgraphLayout(layout='layout_as_tree',root='Y01 PROM',mode='in',circular=T)
```

```{r Construct binary data of training set for causal inference, include=FALSE}
if(run_heavy_computation){
  cf_nw_int_bin=
    infercause %>%
    .[,pData(protocolData(.))$int] %>%
    trans_binary(verbose=F)
  
  saveRDS(cf_nw_int_bin,'data/cf_nw_int_bin.rds')
}else{
  cf_nw_int_bin=readRDS('data/cf_nw_int_bin.rds')
}
```

```{r Prepare formula for causal inference, include=FALSE}
dag$formula$A02=outcome~A02+A20
dag$backdoor$A02=c('A21','A25')

dag$formula$A03=outcome~A03+A15+A28+A04
dag$backdoor$A03=c('A08')

dag$formula$A04=outcome~A04+A10+A11+A13+A02
dag$backdoor$A04=c('A05','A21')

dag$formula$A06=outcome~A06+A19+A20
dag$backdoor$A06=c('A08','A09','A21','A25','A27')

dag$formula$A10=outcome~A10
dag$backdoor$A10=c('A14')

dag$formula$A11=outcome~A11+A15+A20
dag$backdoor$A11=c('A23','A22','A08')

dag$formula$A12=outcome~A12+A02
dag$backdoor$A12=c('A25')

dag$formula$A13=outcome~A13+A15+A28
dag$backdoor$A13=c()

dag$formula$A15=outcome~A15+A28
dag$backdoor$A15=c()

dag$formula$A19=outcome~A19
dag$backdoor$A19=c()

dag$formula$A20=outcome~A20
dag$backdoor$A20=c()

dag$formula$A28=outcome~A28
dag$backdoor$A28=c()
```

```{r Prepare unadjusted formula, include=FALSE}
dag$formula2=
  dag$formula %>%
  lapply(function(x){
    x=as.character(x)
    x[3]=str_split(x[3],' \\+ ')[[1]][1]
    paste0(x[2],x[1],x[3]) %>%
      as.formula()
  })
```

```{r Show an example of causal diagram for inference of a cause, include=FALSE}
source('R/backdoor_path-function.R')
```

```{r Causal DAG function and the resulting images, include=FALSE}
source('R/causal_dag-function.R')
caudag_img=
  dag$baseline_nodes %>%
  
  # Use only the first-level factors.
  filter(str_sub(label,1,1)=='A') %>%
  pull(label) %>%
  
  # For each factor, use it to make a causal diagram plot.
  lapply(causal_dag)
```

```{r The DAG, fig.height=3.46457, fig.width=3.46457, eval=FALSE, include=FALSE}
caudag_img
```

```{r Conduct logistic regression for causal inference, include=FALSE}
# Fit the logistic regression.
dag$glm=
  dag$formula %>%
  lapply(
    FUN=glm
    ,family=binomial(link='logit')
    ,data=
      cf_nw_int_bin %>%
      pData() %>%
      mutate(outcome=ifelse(censoring,NA,as.integer(outcome=='event'))) %>%
      select(outcome) %>%
      cbind(
        cf_nw_int_bin %>%
          exprs() %>%
          t() %>%
          as.data.frame()
      )
  )

dag$glm2=
  dag$formula2 %>%
  lapply(
    FUN=glm
    ,family=binomial(link='logit')
    ,data=
      cf_nw_int_bin %>%
      pData() %>%
      mutate(outcome=ifelse(censoring,NA,as.integer(outcome=='event'))) %>%
      select(outcome) %>%
      cbind(
        cf_nw_int_bin %>%
          exprs() %>%
          t() %>%
          as.data.frame()
      )
  )

# Compute the OR and the 95% CI.
dag$coef=
  dag$glm %>%
  lapply(tidy) %>%
  lapply(
    mutate
    ,OR=exp(estimate)
    ,OR_lb=exp(estimate-qnorm(0.975)*std.error)
    ,OR_ub=exp(estimate+qnorm(0.975)*std.error)
    ,Pr=OR/(1+OR)
    ,Pr_lb=OR_lb/(1+OR_lb)
    ,Pr_ub=OR_ub/(1+OR_ub)
  )

dag$coef2=
  dag$glm2 %>%
  lapply(tidy) %>%
  lapply(
    mutate
    ,OR=exp(estimate)
    ,OR_lb=exp(estimate-qnorm(0.975)*std.error)
    ,OR_ub=exp(estimate+qnorm(0.975)*std.error)
    ,Pr=OR/(1+OR)
    ,Pr_lb=OR_lb/(1+OR_lb)
    ,Pr_ub=OR_ub/(1+OR_ub)
  )
```

```{r Conduct IPW for causal inference based on the causal diagram, include=FALSE}
if(run_heavy_computation){
  cat('Conduct IPW for causal inference based on the causal diagram\n')
  cat('Started:',as.character(now()),'\n')
  
    dag$ipw=
      
      # Fit the IPW.
      dag$formula %>%
      lapply(
        FUN=ipw
        ,data=
          cf_nw_int_bin %>%
          pData() %>%
          mutate(outcome=ifelse(censoring,NA,as.integer(outcome=='event'))) %>%
          select(outcome) %>%
          cbind(
            cf_nw_int_bin %>%
              exprs() %>%
              t() %>%
              as.data.frame()
          )
        ,bootstrap=30
        ,state=33
        ,verbose=T
      ) %>%
      
      # Reduce the size of IPW object.
      lapply(function(x){
        x$data=NULL
        x$index=NULL
        x
      })
  
  cat('End:',as.character(now()))
  saveRDS(dag$ipw,'data/ipw.rds')
}else{
  cat(readRDS('data/log.rds')[['ipw']])
  dag$ipw=readRDS('data/ipw.rds')
}
```

```{r Conduct IPW using unadjusted formula, include=FALSE}
if(run_heavy_computation){
  cat('Conduct IPW using unadjusted formula\n')
  cat('Started:',as.character(now()),'\n')
  
    dag$ipw2=
      
      # Fit the IPW.
      dag$formula2 %>%
      lapply(
        FUN=ipw
        ,data=
          cf_nw_int_bin %>%
          pData() %>%
          mutate(outcome=ifelse(censoring,NA,as.integer(outcome=='event'))) %>%
          select(outcome) %>%
          cbind(
            cf_nw_int_bin %>%
              exprs() %>%
              t() %>%
              as.data.frame()
          )
        ,bootstrap=30
        ,state=33
        ,verbose=T
      ) %>%
      
      # Reduce the size of IPW object.
      lapply(function(x){
        x$data=NULL
        x$index=NULL
        x
      })
  
  cat('End:',as.character(now()))
  saveRDS(dag$ipw2,'data/ipw2.rds')
}else{
  cat(readRDS('data/log.rds')[['ipw2']])
  dag$ipw2=readRDS('data/ipw2.rds')
}
```

```{r Save significant causal covariates, include=FALSE}
dag$sig=
  lapply(X=1:2,Y=dag$coef,Z=dag$ipw,function(X,Y,Z){
    if(X==1){
      # Logistic regression
      sapply(Y,function(x){
        ifelse(
          between(1,round(x$OR_lb[2],4),round(x$OR_ub[2],4)) |
          is.na(x$OR[2])
          ,0,1
        )
      })
    }else{
      # IPW
      sapply(Z,function(x){
        ifelse(
          between(0,round(x$CI95_interval[1],4),round(x$CI95_interval[2],4)) |
          is.na(x$marginal_effect)
          ,0,1
        )
      })
    }
  }) %>%
  do.call(rbind,.) %>%
  `rownames<-`(c('glm','ipw'))

dag$sig2=
  lapply(X=1:2,Y=dag$coef2,Z=dag$ipw2,function(X,Y,Z){
    if(X==1){
      # Logistic regression
      sapply(Y,function(x){
        ifelse(
          between(1,round(x$OR_lb[2],4),round(x$OR_ub[2],4)) |
          is.na(x$OR[2])
          ,0,1
        )
      })
    }else{
      # IPW
      sapply(Z,function(x){
        ifelse(
          between(0,round(x$CI95_interval[1],4),round(x$CI95_interval[2],4)) |
          is.na(x$marginal_effect)
          ,0,1
        )
      })
    }
  }) %>%
  do.call(rbind,.) %>%
  `rownames<-`(c('glm2','ipw2'))
```

```{r Show significant causal covariates, eval=FALSE, include=FALSE}
dag$sig %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var='variable') %>%
  mutate_at(2:3,function(x)ifelse(x==1,'yes','no')) %>%
  left_join(rename(dag$baseline_nodes,variable=label),by='variable') %>%
  kable() %>%
  kable_classic()
```

```{r Show significant unadjusted covariates, eval=FALSE, include=FALSE}
dag$sig2 %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var='variable') %>%
  mutate_at(2:3,function(x)ifelse(x==1,'yes','no')) %>%
  left_join(rename(dag$baseline_nodes,variable=label),by='variable') %>%
  kable() %>%
  kable_classic()
```

```{r The causes, fig.height=10.5, fig.width=10.5, eval=FALSE, include=FALSE}
dag$baseline_edges[,-3] %>%
  filter(
    from %in% colnames(dag$sig)[dag$sig[2,]==1] &
    to %in% c('Y01',colnames(dag$sig)[dag$sig[2,]==1])
  ) %>%
  plot_dag(0,'Y01 PROM','all',T)
```

## Quality control of candidate predictors

For determining candidate predictors, we utilized nationwide medical histories, 
excluding those for validation. But, for the downstream analysis, medical 
histories were provider-wise by estimation. This means our prediction models 
only used medical histories recorded by a healthcare provider, which was 
blinded to those recorded by others. This reflects most real-world situations 
in which a healthcare provider does not have access to medical records of other 
providers.

All candidate predictors, including non-demographical associated factors, have 
non-zero variances (eTable 8 in R Markdown). There were 460 candidate 
predictors fulfilling this criterion. We also showed in the same eTable that 
there are 426 candidate predictors without perfect separation

```{r Combine selected causal factors and medical histories, include=FALSE}
# Nationwide data for inference by causal factors,
# confirmed by IPW, and by medical histories
inferboth=
  ExpressionSet(
    assayData=
      rbind(
        exprs(infercause) %>%
          .[colnames(dag$sig)[dag$sig['ipw',]==1],,drop=F] %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,exprs(inferdata)
      )
    ,phenoData=phenoData(inferdata)
    ,featureData=
      rbind(
        fData(infercause) %>%
          .[colnames(dag$sig)[dag$sig['ipw',]==1],,drop=F] %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,fData(inferdata)
      ) %>%
      AnnotatedDataFrame(varMetadata=fvarMetadata(inferdata))
    ,experimentData=
      MIAME(
        name='Herdiantri Sufriyana'
        ,lab='Emily Chia-Yu Su Lab'
        ,contact='herdiantrisufriyana@unusa.ac.id'
        ,title='Medical history dataset for causal inference'
        ,abstract=
          str_replace_all(
            'This dataset is a medical-history table from the BPJS Kesehatan.
            Selected causal factors, that is re-assigned by regex, are added.
            The target population is health insurance holders of 
            12-to-55-years-old females who visit healthcare providers between 
            2015 and 2016. The outcome of interest is prelabor rupture of 
            membrane (PROM). The medical history scenario is recorded
            individually within each healthcare provider nationwide.'
            ,'\n',' '
          ) %>%
          str_replace_all('\\s+',' ')
        ,url='https://github.com/herdiantrisufriyana/prom'
      )
    ,annotation=annotation(inferdata)
    ,protocolData=protocolData(inferdata)
  )

# Provider-wise data for prediction by causal factors,
# confirmed by IPW, and by medical histories
prediboth=
  ExpressionSet(
    assayData=
      rbind(
        exprs(predicause) %>%
          .[colnames(dag$sig)[dag$sig['ipw',]==1],,drop=F] %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,exprs(predidata)
      )
    ,phenoData=phenoData(predidata)
    ,featureData=
      rbind(
        fData(predicause) %>%
          .[colnames(dag$sig)[dag$sig['ipw',]==1],,drop=F] %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,fData(predidata)
      ) %>%
      AnnotatedDataFrame(varMetadata=fvarMetadata(predidata))
    ,experimentData=
      MIAME(
        name='Herdiantri Sufriyana'
        ,lab='Emily Chia-Yu Su Lab'
        ,contact='herdiantrisufriyana@unusa.ac.id'
        ,title='Medical history dataset for prediction'
        ,abstract=
          str_replace_all(
            'This dataset is a medical-history table from the BPJS Kesehatan.
            Selected causal factors, that is re-assigned by regex, are added.
            The target population is health insurance holders of 
            12-to-55-years-old females who visit healthcare providers between 
            2015 and 2016. The outcome of interest is prelabor rupture of 
            membrane (PROM). The medical history scenario is recorded
            individually within each healthcare provider nationwide.'
            ,'\n',' '
          ) %>%
          str_replace_all('\\s+',' ')
        ,url='https://github.com/herdiantrisufriyana/prom'
      )
    ,annotation=annotation(predidata)
    ,protocolData=protocolData(predidata)
  )
```

```{r Candidate predictors with non-zero variances, eval=FALSE, include=FALSE}
var_candidate_predictors=
  
  # Use only training set.
  prediboth %>%
  .[,pData(protocolData(.))$int] %>%
  
  # Get predictors and the outcome.
  lapply(X=1,Y=.,function(X,Y){
    exprs(Y) %>%
      t() %>%
      as.data.frame() %>%
      cbind(select(pData(Y),'outcome'))
  }) %>%
  .[[1]] %>%
  
  # Summarize a standard deviation for each predictor per outcome.
  select(outcome,everything()) %>%
  gather(key,value,-outcome) %>%
  group_by(key,outcome) %>%
  summarize(sd_value=sd(value,na.rm=T),.groups='drop') %>%
  arrange(factor(key,unique(key)),outcome) %>%
  spread(outcome,sd_value) %>%
  setNames(str_remove_all(names(.),'-')) %>% 
  arrange(factor(key,rownames(prediboth)[rownames(prediboth)%in%key]))
```

```{r Train-set-based NPS predictors excluding outcome leaker, include=FALSE}
# Select predictors without perfect separation problem.
if(run_heavy_computation){
  
  int_nps=
    prediboth %>%
    .[,pData(protocolData(.))$int] %>%
    extract_nps_mh() %>%
    arrange(factor(key,rownames(prediboth)[rownames(prediboth)%in%key]))
  
  saveRDS(int_nps,'data/int_nps.rds')
  
}else{
  int_nps=readRDS('data/int_nps.rds')
}

# Identify codes related to the end of pregnancy.
outcome_leaker=
  int_nps %>%
  left_join(rename(annotation,key=code),by='key') %>%
  filter(
    str_detect(
      str_to_lower(paste(key,desc))
      ,paste0(c(
        'abort'
        ,'deliver'
        ,'cesar'
        ,'labou?r\\s+'
        ,'natal'
        ,'postpartum'
        ,'terminat'
        ,'o0[0123]'
        ,'o152'
        ,'o364'
        ,'o63'
        ,'o7[02]'
        ,'o8[579]'
        ,'o90'
        ,'z3[79]'
      ),collapse='|')
    )
  ) %>%
  filter(!(
    str_detect(
      str_to_lower(paste(key,desc))
      ,paste0(c(
        'aborter'
        ,'ante'
        ,'peri'
        ,'delayed'
        ,'false'
        ,'failed'
        ,'prenatal'
        ,'threatened'
        ,'o311'
        ,'z351'
        ,'o600'
        ,'o96'
      ),collapse='|')
    ) &
    !str_detect(
      str_to_lower(key)
      ,paste0(c(
        'o0[34568]'
        ,'o15[12]'
        ,'o7[02]'
        ,'o8[79]'
      ),collapse='|')
    )
  )) %>%
  select(key,desc)

# Exclude the outcome-leaker codes.
int_nps_eol_p=
  int_nps %>%
  left_join(
    outcome_leaker %>%
      select(key) %>%
      mutate(excluded=1)
    ,by='key'
  ) %>%
  filter(is.na(excluded)) %>%
  select(-excluded)
```

```{r Non-zero variance and no perfect separation, eval=FALSE, include=FALSE}
var_candidate_predictors %>%
  
  # Exclude predictors that do not exist in training set.
  filter(!(nonevent==0 & event==0)) %>%
  
  # Join predictors without perfect separation problem.
  left_join(
    prediboth %>%
      .[,pData(protocolData(.))$int] %>%
      extract_nps_mh() %>%
      arrange(factor(key,rownames(prediboth)[rownames(prediboth)%in%key])) %>%
      mutate(perfect_separation='No')
    ,by=c('key','nonevent','event')
  ) %>%
  mutate(
    perfect_separation=
      ifelse(is.na(perfect_separation),'Yes',perfect_separation)
  ) %>%
  
  # Join combined annotation tables of medical histories and causal factors.
  left_join(
    rename(annotation,key=code) %>%
      rbind(
        dag$baseline_nodes %>%
          mutate(label=paste0('causal_',label)) %>%
          rename(desc=name,key=label) %>%
          select(key,desc)
      )
    ,by='key'
  ) %>%
  rename(candidate_predictor=key,description=desc) %>%
  setNames(str_to_title(colnames(.)) %>% str_replace_all('_',' ')) %>%
  
  # Save for eTable 8.
  write_csv('data/supp_table_8.csv')
```

To prevent outcome leakage, we removed the maternal or baby diagnosis/procedure 
codes that demonstrated the delivery or after delivery care (typically up to 6 
weeks following childbirth; eTable 9 in R Markdown). We only used the existing 
codes in the training set to determine outcome-leaker codes based on the 
previous codes for determining delivery or immediately after delivery care 
(eTable 7 in R Markdown). There were 54 codes that may leak the outcome.

```{r Save outcome leakers, eval=FALSE, include=FALSE}
outcome_leaker %>%
  arrange(key) %>%
  rename(code=key,description=desc) %>%
  setNames(str_to_title(colnames(.))) %>%
  
  # Save for eTable 9.
  write_csv('data/supp_table_9.csv')
```

To avoid redundancy, we computed pair-wise Pearson correlation coefficients 
(eTable 10 in R Markdown). None of the estimates showed a perfect correlation 
(*r*=1). High correlations (i.e., >~0.70) were reasonably identified between 
latent candidate predictors and the code components. We did not remove those 
pairs of predictors.

```{r Correlations of candidate predictors, eval=FALSE, include=FALSE}
cor_predictors=
  
  # Use only training set.
  prediboth %>%
  .[,pData(protocolData(.))$int] %>%
  
  # Get predictors and the outcome.
  lapply(X=1,Y=.,function(X,Y){
    exprs(Y) %>%
      t() %>%
      as.data.frame() %>%
      cbind(select(pData(Y),'outcome'))
  }) %>%
  .[[1]] %>%
  
  # Get only predictors without perfect separation problem and not outcome leakers. 
  select(outcome,everything()) %>%
  mutate_all(function(x)ifelse(is.na(x),0,x)) %>%
  .[,int_nps_eol_p$key] %>%
  
  # Compute inter-predictor Pearson correlation coefficients.
  cor()
```

```{r Save and show the inter-predictor correlations, eval=FALSE, include=FALSE}
# Save the correlation coefficients.
cor_predictors %>%
  as.data.frame() %>%
  rownames_to_column(var=' ') %>%
  
  # Save for eTable 10.
  write_csv('data/supp_table_10.csv')

# Show the correlation coefficients.
cor_predictors %>%
  
  # Remove auto-correlations.
  as.data.frame() %>%
  rownames_to_column(var='first') %>%
  gather(second,pearson_correlation_r,-first) %>%
  filter(first!=second) %>%
  
  # Remove redundant correlations.
  mutate(pair=seq(nrow(.))) %>%
  gather(order,predictor,-pair,-pearson_correlation_r) %>%
  group_by(pair) %>%
  arrange(pair,predictor) %>%
  mutate(order=c('first','second')) %>%
  ungroup() %>%
  spread(order,predictor) %>%
  select(-pair) %>%
  filter(!duplicated(.)) %>%
  
  # Filter the high-correlated pair of predictors.
  filter(pearson_correlation_r>0.7) %>%
  
  # Join regular expression of the coding components of causal factors 
  # to check if the high-correlated pairs are due to these components.
  left_join(
    dag$baseline_nodes %>%
      mutate(label=paste0('causal_',label)) %>%
      rename(first=label) %>%
      select(first,name)
    ,by='first'
  ) %>%
  left_join(
    dag$measure_nodes %>%
      mutate(label=paste0('causal_',str_remove_all(label,'\\*'))) %>%
      rename(regular_expression=name,first=label) %>%
      select(first,regular_expression)
    ,by='first'
  )
```

By systematic human learning and association tests using available data, we 
also determined latent candidate predictors (eTable 11 in R Markdown). There 
were 27 first- and 10 second-level factors that are considerably associated 
with PROM. Only data for 12 out of 27 factors were available in training set. 
Either the diagnosis/procedure codes, or demographical variables (not included 
as candidate predictors), for latent candidate predictors are also described 
(eTable 12 in R Markdown).

```{r Save results of systematic human learning, eval=FALSE, include=FALSE}
dag$baseline_nodes %>%
  
  # Get all the first-level factors.
  filter(name!='PROM') %>%
  
  # Pair with the effect.
  left_join(
    dag$factor %>%
      filter(dependent_factor!='Preterm birth') %>%
      select(dependent_factor,independent_factor,citation) %>%
      rename(effect_on=dependent_factor,name=independent_factor)
    ,by='name'
  ) %>%
  rename(factor=name) %>%
  
  # Identify the factor levels.
  mutate(level=ifelse(str_detect(label,'A'),'First','Second')) %>%
  select(level,effect_on,everything()) %>%
  
  # Add information about the data availability.
  left_join(
    data.frame(label=rownames(cf_nw_int_bin),data_availability='Yes')
    ,by='label'
  ) %>%
  mutate(
    data_availability=
      ifelse(
        !is.na(data_availability) & effect_on=='PROM'
        ,'Yes','No'
      )
  ) %>%
  
  # Add information about the model (formula).
  left_join(
    dag$formula %>%
      lapply(X=names(.),Y=.,function(X,Y){
        Z=as.character(Y[[X]])
        data.frame(label=X,mathematical_model=paste0(Z[2],' ~ ',Z[3]))
      }) %>%
      do.call(rbind,.) %>%
      mutate(effect_on='PROM')
    ,by=c('label','effect_on')
  ) %>%
  
  # Save for eTable 11.
  arrange(
    level
    ,factor(effect_on,dag$baseline_nodes$name)
    ,factor(factor,dag$baseline_nodes$name)
  ) %>%
  setNames(str_to_title(colnames(.)) %>% str_replace_all('_',' ')) %>%
  write_csv('data/supp_table_11.csv')
```

```{r Save codes and demographics for causal factors, eval=FALSE, include=FALSE}
dag$measure_nodes %>%
  
  # Get the regular expression to define causal factors.
  mutate(label=str_remove_all(label,'\\*')) %>%
  rename(regular_expression=name) %>%
  
  # Add the description.
  left_join(dag$baseline_nodes,by='label') %>%
  filter(label!='Y01') %>%
  mutate(label=paste0('causal_',label)) %>%
  rename(causal_factor=name,variable=label) %>%
  select(causal_factor,variable,regular_expression) %>%
  
  # Based on the regular expression, gather the ICD-10 codes that apply.
  lapply(X=seq(nrow(.)),Y=.,function(X,Y){
    
    Z=Y$regular_expression[X]
    
    if(str_detect(Z,'\\(|\\)')){
      
      K=str_split(Z,'\\(|\\)')[[1]]
      
      L=K[2] %>%
        str_split('\\|') %>%
        .[[1]] %>%
        sapply(X=seq(length(.)),Y=.,Z=K[1],function(X,Y,Z){
          paste0(Z,Y[X])
        })
      
    }else{
      
      K=str_split(Z,'\\|')[[1]]
      
      L=K %>%
        lapply(X=seq(length(.)),Y=.,function(X,Y){
          if(str_detect(Y[X],'\\[|\\]')){
            
            Z=str_split(Y[X],'\\[|\\]')[[1]]
            
            if(str_detect(Z[2],'\\^')){
              
              K=Z[2] %>%
                str_remove_all('\\^')
              
              paste0(0:9,collapse='') %>%
                str_remove_all(K) %>%
                sapply(X=seq(str_count(.)),Y=.,Z=Z[1],function(X,Y,Z){
                  paste0(Z,str_sub(Y,X,X))
                })
              
            }else if(str_detect(Z[2],'\\-')){
              
              K=Z[2] %>%
                str_split_fixed('\\-',2)
              
              paste0(K[1]:K[2],collapse='') %>%
                sapply(X=seq(str_count(.)),Y=.,Z=Z[1],function(X,Y,Z){
                  paste0(Z,str_sub(Y,X,X))
                })
              
            }else{
              
              Z[2] %>%
                sapply(X=seq(str_count(.)),Y=.,Z=Z[1],function(X,Y,Z){
                  paste0(Z,str_sub(Y,X,X))
                })
              
            }
            
          }else{
            
            Y[X]
            
          }
        }) %>%
        unlist()
      
    }
    
    data.frame(
      causal_factor=Y$causal_factor[X]
      ,variable=Y$variable[X]
      ,regular_expression=Y$regular_expression[X]
      ,code=L
    )
    
  }) %>%
  do.call(rbind,.) %>%
  select(-regular_expression) %>%
  
  # Add information related to demographics.
  lapply(
    X=seq(nrow(.))
    ,Y=.
    ,Z=
      annotation %>%
      rbind(
        readRDS('data/cat_identity.rds') %>%
          mutate(
            code=
              desc %>%
              str_to_lower() %>%
              sapply(
                str_remove_all
                ,paste0(
                  c('age'
                    ,'householder'
                    ,str_replace_all(colnames(outcome),'_',' '))
                  ,collapse='|'
                )
              ) %>%
              str_to_upper() %>%
              trimws()
          )
      )
    ,function(X,Y,Z){
    
      Z %>%
        filter(str_detect(code,Y$code[X])) %>%
        mutate(
          causal_factor=Y$causal_factor[X]
          ,variable=Y$variable[X]
        )
  }) %>%
  do.call(rbind,.) %>%
  
  # Save for eTable 12.
  select(causal_factor,variable,everything()) %>%
  rename(demographics_or_medical_history=code,description=desc) %>%
  setNames(str_to_title(colnames(.)) %>% str_replace_all('_',' ')) %>%
  write_csv('data/supp_table_12.csv')
```

## Feature extraction as historical rates

To deal with a problem in which a healthcare provider does not have access to 
medical records of other providers, medical histories were quantified as 
Kaplan-Meier (KM) estimators for each candidate predictors, so called 
historical rates.  We inferred these rates given the day number from a code 
encounter to current visit for each candidate predictor, derived only from the 
same nationwide medical histories, as previously described [...]. This used 
irredundant candidate predictors with non-zero variances and no perfect 
separation in training set only. The candidate predictors were transformed into 
the historical rates in all data partitions.

```{r Compute historical rate based on nationwide training set, include=FALSE}
if(run_heavy_computation){
  nw_int_hlin=
    inferboth %>%
    .[int_nps_eol_p$key
      ,pData(protocolData(.))$int] %>%
    trans_hist_rate(
      interpolation='linear'
      ,verbose=F
    )
  
  saveRDS(nw_int_hlin,'data/nw_int_hlin.rds')
}else{
  nw_int_hlin=readRDS('data/nw_int_hlin.rds')
}
```

```{r Use the nationwide rate to get it for whole prediction set, include=FALSE}
if(run_heavy_computation){
  pw_hlin=
    prediboth %>%
    .[int_nps_eol_p$key,] %>%
    trans_hist_rate(
      hist_rate=preproc(nw_int_hlin)$hist_rate
      ,interpolation='linear'
      ,verbose=F
    )
  
  saveRDS(pw_hlin,'data/pw_hlin.rds')
}else{
  pw_hlin=readRDS('data/pw_hlin.rds')
}
```

```{r Show the outcome and censoring in training set, eval=FALSE, include=FALSE}
pw_hlin %>%
  .[,pData(protocolData(.))$int] %>%
  pData() %>%
  select(outcome,censoring) %>%
  table()
```

## Feature representation as principal components by 10-fold cross validation

The historical rates of all candidate predictors were fitted to a principal 
component (PC) model. Only nationwide medical histories in training set were 
used for the model fitting. We applied 10-fold cross validation to estimate 
weights for all candidate predictors in each PC. A protocol for resampled dimensional reduction was followed for feature representation in this study 
[...].

```{r Fit PC models with resampling based on train set, include=FALSE}
if(run_heavy_computation){
  pw_int_rsdr=
    pw_hlin %>%
    .[,pData(protocolData(.))$int] %>%
    exprs() %>%
    t() %>%
    as.data.frame() %>%
    rsdr(
      rs_method='CV'
      ,rs_number=10
      ,dr_method='PCA'
      ,cl=detectCores()/2
    )
  saveRDS(pw_int_rsdr,'data/pw_int_rsdr.rds')
}else{
  cat(readRDS('data/log.rds')[['pw_int_rsdr']])
  pw_int_rsdr=readRDS('data/pw_int_rsdr.rds')
}
```

## Set up tuning-training-calibrating configuration and internal validation

Previous data partition had not held out instances for calibration yet. This 
took 80% of training set. We also gave different weights for event and 
non-event by including censored outcome, as described in the main text. For 
hyperparameter tuning, we applied 5-fold cross validation, instead of 10-fold 
as applied for PC modeling. Meanwhile, the final training and calibration for 
each model were conducted by bootstrapping for 30 times. The same resampling 
methods were applied for both classification and estimation tasks. Parallel 
computing by multiple central processing units (CPUs) were applied for training 
all models.

```{r Build a function to get set for training or testing, include=FALSE}
source('R/get_set-function.R')
```

```{r Prepare train set and parameters, include=FALSE}
# Create an empty list to save training parameters.
training_parameters=list()

# Hold out 20% of training set for calibration.
suppressWarnings(set.seed(66,sample.kind=sample.kind))
training_parameters$pre_calib_set=
  
  # Training set
  pw_hlin %>%
  .[,pData(protocolData(.))$int] %>%
  pData() %>%
  rownames_to_column(var='id') %>%
  
  # Uncensored outcome
  filter(!censoring) %>%
  
  # Get 80% for pre-calibrated set.
  lapply(X=1,Y=.,function(X,Y){
    Z=createDataPartition(Y$outcome,times=1,p=0.8)
    Y$id[Z$Resample1]
  }) %>%
  .[[1]]

# Compute outcome weights.
training_parameters$outcome_weights=
  
  # Training set
  pw_hlin %>%
  .[,pData(protocolData(.))$int] %>%
  pData() %>%
  select(outcome,censoring) %>%
  
  # Compute the probabilities, taking censoring into account.
  cbind(
    table(.) %>%
      as.numeric() %>%
      setNames(c('nonevent_uncensored','event_uncensored'
                 ,'nonevent_censored','event_censored')) %>%
      t() %>%
      as.data.frame() %>%
      mutate(
        total=
          nonevent_uncensored+
          nonevent_censored+
          event_uncensored+
          event_censored
        ,nonevent_prob=nonevent_uncensored/total
        ,event_prob=event_uncensored/total
      ) %>%
      select(nonevent_prob,event_prob)
  ) %>%
  rownames_to_column(var='id') %>%
  
  # Uncensored outcome
  filter(!censoring) %>%
  select(-censoring) %>%
  
  # Compute the weight from half od the inverse probability.
  mutate(
    weight=
      ifelse(
        outcome=='event'
        ,(1/event_prob)*0.5
        ,(1/nonevent_prob)*0.5
      )
  ) %>%
  column_to_rownames(var='id') %>%
  select(weight)

# Define classification tuning to apply 5-fold cross validation.
training_parameters$tuning_trControl=
  trainControl(
    method='cv'
    ,number=5
    ,summaryFunction=twoClassSummary
    ,classProbs=T
    ,savePredictions=T
    ,allowParallel=T
  )

# Define classification training to apply 30-time bootstrapping.
training_parameters$final_trControl=
  trainControl(
    method='boot'
    ,number=30
    ,summaryFunction=twoClassSummary
    ,classProbs=T
    ,savePredictions=T
    ,allowParallel=T
  )

# Define estimation tuning to apply 5-fold cross validation.
training_parameters$timereg_tuning_trControl=
  trainControl(
    method='cv'
    ,number=5
    ,savePredictions=F
    ,allowParallel=T
  )

# Define estimation training to apply 30-time bootstrapping.
training_parameters$timereg_final_trControl=
  trainControl(
    method='boot'
    ,number=30
    ,savePredictions=F
    ,allowParallel=T
  )
```

```{r Build a function for caret training and evaluation, include=FALSE}
source('R/caret_trainer_evaluator-function.R')
```

```{r Build a function for caret time-regression and evaluation, include=FALSE}
source('R/caret_timereg_evaluator-function.R')
```

## Hyperparameter tuning, final training, and calibrating

We applied the tuning grids and the training configurations for all models, 
except DI-VNN which required several modifications. This is already described 
in a protocol we followed for these procedures, as previously described [...]. 
More details for DI-VNN will be described in the next section.

```{r Empty lists to save models and the evaluation results, include=FALSE}
model=list()
calib_model=list()
eval_model=list()
timing_model=list()
eval_timing_model=list()
```

```{r Conduct causal ridge regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(model$causal_ridge
    ,calib_model$causal_ridge
    ,eval_model$causal_ridge) %<-% caret_trainer_evaluator(
      data=
        pw_hlin %>%
        .[rownames(.) %>% .[str_detect(.,'causal')],]
      ,method='glmnet'
      ,calib_method='glm'
      ,tuning=data.frame(alpha=0,lambda=10^seq(-9,0,len=10))
      ,training=training_parameters
      ,title='Conduct causal ridge regression by parallel computing'
      ,dir='data'
      ,file='causal_ridge'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['causal_ridge']])
  c(model$causal_ridge
    ,calib_model$causal_ridge
    ,eval_model$causal_ridge) %<-% list(
      readRDS('data/causal_ridge.rds')
      ,readRDS('data/calib_causal_ridge.rds')
      ,readRDS('data/eval_causal_ridge.rds')
    )
}
```

```{r Conduct causal ridge timing regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(timing_model$causal_ridge
    ,eval_timing_model$causal_ridge) %<-% caret_timereg_evaluator(
      data=
        pw_hlin %>%
        .[rownames(.) %>% .[str_detect(.,'causal')],]
      ,model=model$causal_ridge
      ,calib_model=calib_model$causal_ridge
      ,tuning=data.frame(alpha=0,lambda=10^seq(-9,0,len=10))
      ,training=training_parameters
      ,title='Conduct causal ridge timing regression by parallel computing'
      ,dir='data'
      ,file='causal_ridge'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['timing_causal_ridge']])
  c(timing_model$causal_ridge
    ,eval_timing_model$causal_ridge) %<-% list(
      readRDS('data/timing_causal_ridge.rds')
      ,readRDS('data/eval_timing_causal_ridge.rds')
    )
}
```

```{r Compute EPV using pre-calibrating train set and all PCs, eval=FALSE, include=FALSE}
pw_hlin %>%
  
  # Use pre-calibrated, training set.
  .[,pData(protocolData(.))$int] %>%
  .[,colnames(.)%in%training_parameters$pre_calib_set] %>%
  pData() %>%
  
  # Summarize the number of censored and uncensored outcomes.
  select(outcome,censoring) %>%
  table() %>%
  as.numeric() %>%
  
  # Get the minimum uncensored number between event and nonevent.
  .[1:2] %>%
  min() %>%
  
  # Floor the number and divide it with the number of candidate features.
  lapply(X=1,Y=.,function(X,Y){
    floor(Y/nrow(pw_hlin))
  }) %>%
  .[[1]] %>%
  c(nrow(pw_hlin)) %>%
  setNames(c('EPV','PC'))
```

```{r Prepare and combine PCs for all sets, include=FALSE}
if(run_heavy_computation){
  cat('Prepare and combine PCs for all sets\n')
  cat('Started:',as.character(now()),'\n')
  
  # Transform any features into the PCs.
  pw_pc=
    list('int','ran','geo','tem','bgt') %>%
    lapply(function(x){
      cat('For ',x,': ',sep='')
      pw_hlin %>%
        .[,pData(protocolData(.))[[x]]] %>%
        transformation(
          rsdr_object=pw_int_rsdr
          ,verbose=T
        )
    }) %>%
    setNames(c('int','ran','geo','tem','bgt'))
  
  # Provider-wise data for prediction by PCs
  pw_pc=
    ExpressionSet(
      assayData=
        pw_pc %>%
        lapply(exprs) %>%
        do.call(cbind,.) %>%
        .[,colnames(pw_hlin)]
      ,phenoData=
        pw_pc %>%
        lapply(phenoData) %>%
        lapply(pData) %>%
        do.call(rbind,.) %>%
        `rownames<-`(str_split_fixed(rownames(.),'\\.',2)[,2]) %>%
        .[colnames(pw_hlin),] %>%
        AnnotatedDataFrame() %>%
        `varMetadata<-`(varMetadata(phenoData(pw_pc$int)))
      ,featureData=featureData(pw_pc$int)
      ,experimentData=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history PC dataset for prediction'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              Selected causal factors, that is re-assigned by regex, are added.
              All are transformed by resampled dimensional reduction using 
              principal components analysis with 10-fold cross validation (CV).
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded
              individually within each healthcare provider nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=annotation(pw_pc$int)
      ,protocolData=
        pw_pc %>%
        lapply(protocolData) %>%
        lapply(pData) %>%
        do.call(rbind,.) %>%
        `rownames<-`(str_split_fixed(rownames(.),'\\.',2)[,2]) %>%
        .[colnames(pw_hlin),] %>%
        AnnotatedDataFrame() %>%
        `varMetadata<-`(varMetadata(protocolData(pw_pc$int)))
    )

  gc()
  cat('End:',as.character(now()))
}else{
  cat(readRDS('data/log.rds')[['pw_pc']])
}
```

```{r Conduct PC elastic net regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(model$pc_elnet
    ,calib_model$pc_elnet
    ,eval_model$pc_elnet) %<-% caret_trainer_evaluator(
      data=pw_pc
      ,method='glmnet'
      ,calib_method='gamLoess'
      ,tuning=expand.grid(alpha=seq(0,1,len=5),lambda=10^seq(-9,0,len=5))
      ,training=training_parameters
      ,title='Conduct PC elastic net regression by parallel computing'
      ,dir='data'
      ,file='pc_elnet'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['pc_elnet']])
  c(model$pc_elnet
    ,calib_model$pc_elnet
    ,eval_model$pc_elnet) %<-% list(
      readRDS('data/pc_elnet.rds')
      ,readRDS('data/calib_pc_elnet.rds')
      ,readRDS('data/eval_pc_elnet.rds')
    )
}
```

```{r Conduct PC elastic net timing regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(timing_model$pc_elnet
    ,eval_timing_model$pc_elnet) %<-% caret_timereg_evaluator(
      data=pw_pc
      ,model=model$pc_elnet
      ,calib_model=calib_model$pc_elnet
      ,tuning=expand.grid(alpha=seq(0,1,len=5),lambda=10^seq(-9,0,len=5))
      ,training=training_parameters
      ,title='Conduct PC elastic net timing regression by parallel computing'
      ,dir='data'
      ,file='pc_elnet'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['timing_pc_elnet']])
  c(timing_model$pc_elnet
    ,eval_timing_model$pc_elnet) %<-% list(
      readRDS('data/timing_pc_elnet.rds')
      ,readRDS('data/eval_timing_pc_elnet.rds')
    )
}
```

```{r Compute maximum number of PCs to get 200 EPV, eval=FALSE, include=FALSE}
pw_hlin %>%
  
  # Use pre-calibrated, training set.
  .[,pData(protocolData(.))$int] %>%
  .[,colnames(.)%in%training_parameters$pre_calib_set] %>%
  pData() %>%
  
  # Summarize the number of censored and uncensored outcomes.
  select(outcome,censoring) %>%
  table() %>%
  as.numeric() %>%
  
  # Get the minimum uncensored number between event and nonevent.
  .[1:2] %>%
  min() %>%
  
  # Floor the number and divide it with the number of candidate features.
  lapply(X=1,Y=.,function(X,Y){
    floor(Y/200)
  }) %>%
  .[[1]] %>%
  c(200) %>%
  setNames(c('PC','EPV'))
```

```{r Select the top-weight PCs for downstream analysis, include=FALSE}
selected_pc=
  
  # Get PC weights from the PC elastic net regression model.
  coef(model$pc_elnet$finalModel,model$pc_elnet$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var='term') %>%
  
  # Clean up.
  mutate(term=str_remove_all(term,'\\`|\\(|\\)')) %>%
  setNames(c('term','estimate')) %>%
  filter(estimate!=0) %>%
  arrange(desc(abs(estimate))) %>%
  filter(!term %in% c('Intercept')) %>%
  
  # Find the top-weight PCs up to the maximum number to get 200 EPV.
  slice(1:60) %>%
  mutate(idx=str_remove_all(term,'PC') %>% as.integer())
```

```{r Conduct PC random forest by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(model$pc_rf
    ,calib_model$pc_rf
    ,eval_model$pc_rf) %<-% caret_trainer_evaluator(
      data=pw_pc[selected_pc$term,]
      ,method='Rborist'
      ,calib_method='glm'
      ,tuning=expand.grid(predFixed=seq(5,45,len=5),minNode=seq(20,100,len=5))
      ,training=training_parameters
      ,title='Conduct PC random forest by parallel computing'
      ,dir='data'
      ,file='pc_rf'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['pc_rf']])
  c(model$pc_rf
    ,calib_model$pc_rf
    ,eval_model$pc_rf) %<-% list(
      readRDS('data/pc_rf.rds')
      ,readRDS('data/calib_pc_rf.rds')
      ,readRDS('data/eval_pc_rf.rds')
    )
}
```

```{r Conduct PC-RF timing regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(timing_model$pc_rf
    ,eval_timing_model$pc_rf) %<-% caret_timereg_evaluator(
      data=pw_pc[selected_pc$term,]
      ,model=model$pc_rf
      ,calib_model=calib_model$pc_rf
      ,tuning=expand.grid(predFixed=seq(5,45,len=5),minNode=seq(20,100,len=5))
      ,training=training_parameters
      ,title='Conduct PC-RF timing regression by parallel computing'
      ,dir='data'
      ,file='pc_rf'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['timing_pc_rf']])
  c(timing_model$pc_rf
    ,eval_timing_model$pc_rf) %<-% list(
      readRDS('data/timing_pc_rf.rds')
      ,readRDS('data/eval_timing_pc_rf.rds')
    )
}
```

```{r Conduct PC gradient boosting machine by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(model$pc_gbm
    ,calib_model$pc_gbm
    ,eval_model$pc_gbm) %<-% caret_trainer_evaluator(
      data=pw_pc[selected_pc$term,]
      ,method='gbm'
      ,calib_method='gamLoess'
      ,tuning=
        expand.grid(
          interaction.depth=1
          ,shrinkage=seq(0.0005,0.05,len=25)
          ,n.minobsinnode=20
        ) %>%
        mutate(n.trees=seq(0,2500,len=26) %>% .[-1] %>% rev()) %>%
        select(n.trees,everything())
      ,training=training_parameters
      ,title='Conduct PC gradient boosting machine by parallel computing'
      ,dir='data'
      ,file='pc_gbm'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['pc_gbm']])
  c(model$pc_gbm
    ,calib_model$pc_gbm
    ,eval_model$pc_gbm) %<-% list(
      readRDS('data/pc_gbm.rds')
      ,readRDS('data/calib_pc_gbm.rds')
      ,readRDS('data/eval_pc_gbm.rds')
    )
}
```

```{r Conduct PC-GBM timing regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  c(timing_model$pc_gbm
    ,eval_timing_model$pc_gbm) %<-% caret_timereg_evaluator(
      data=pw_pc[selected_pc$term,]
      ,model=model$pc_gbm
      ,calib_model=calib_model$pc_gbm
      ,tuning=
        expand.grid(
          interaction.depth=1
          ,shrinkage=seq(0.0005,0.05,len=25)
          ,n.minobsinnode=20
        ) %>%
        mutate(n.trees=seq(0,2500,len=26) %>% .[-1] %>% rev()) %>%
        select(n.trees,everything())
      ,training=training_parameters
      ,title='Conduct PC-GBM timing regression by parallel computing'
      ,dir='data'
      ,file='pc_gbm'
      ,cl=detectCores()/2
    )
}else{
  cat(readRDS('data/log.rds')[['timing_pc_gbm']])
  c(timing_model$pc_gbm
    ,eval_timing_model$pc_gbm) %<-% list(
      readRDS('data/timing_pc_gbm.rds')
      ,readRDS('data/eval_timing_pc_gbm.rds')
    )
}
```

## Deep-insight visible neural network (DI-VNN)

We applied different pre-processing pipelines for feature selection and 
representation in DI-VNN. Instead of PCs, we used the historical rates of the 
candidate predictors. A 5-minute video explaining DI-VNN pipeline is available 
in the protocol [...]. Only pre-calibration training set was used for the 
downstream pipeline. We followed a protocol for this model [...]. Briefly, we 
applied differential analysis for feature selection. Then, 1-bit stochastic 
gradient descent transformation was applied using the post-normalization, 
feature-wise average based on nationwide training set to determine if a value 
is lower, equal to, or higher than the average respectively as -1, 0, and 1. 
Using a feature-to-feature Pearson correlation matrix, we created a feature map 
*t*-stochastic neighbor embedding (*t*-SNE) with Barnes-Hut approximation. We 
also subset this map hierarchically by clique-extracted ontology (CliXO) 
algorithm based on the same correlation matrix. We constructed convolutional 
neural network (CNN) model using the hierarchy with input of feature map onto 
which transformed, candidate predictors were projected. We trained the CNN 
model to get the learning representation.

```{r DI-VNN feature selection and representation, include=FALSE}
if(run_heavy_computation){
  input=list()
  
  # Use provider-wise, pre-calibrated, training set for modeling.
  input$pw=
    pw_hlin %>%
    .[,!pData(phenoData(.))$censoring] %>%
    .[,pData(protocolData(.))$int] %>%
    .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F]
  
  # Use nationwide, pre-calibrated, training set for feature selection.
  input$nw=
    nw_int_hlin %>%
    .[,!pData(phenoData(.))$censoring] %>%
    .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F]
  
  output=list()
  
  # Get uncensored outcome.
  output$outcome=
    input$pw %>%
    phenoData() %>%
    pData() %>%
    select(outcome)
  
  # Get features corresponding to uncensored outcome.
  output$raw$pw=
    input$pw %>%
    exprs() %>%
    t() %>%
    .[rownames(output$outcome),]
  
  output$raw$nw=
    input$nw %>%
    exprs() %>%
    t() %>%
    .[rownames(output$outcome),]
  
  # Select features utilizing differential analysis
  # by moderated-t stats with Benjamini-Hochberg multiple testing correction.
  output$fit=
    t(output$raw$nw) %>%
    normalize.quantiles() %>%
    `dimnames<-`(dimnames(t(output$raw$nw))) %>%
    lmFit(model.matrix(~outcome,output$outcome)) %>%
    eBayes() %>%
    topTable(coef=2,nrow(.$coefficients),adjust.method='BH',sort.by='none') %>%
    filter(adj.P.Val<0.05)
  
  # Get the selected features as unnormalized ones.
  output$unnorm$pw=
    output$raw$pw %>%
    .[,rownames(output$fit)]
  
  output$unnorm$nw=
    output$raw$nw %>%
    .[,rownames(output$fit)]
  
  # Normalize features quantile-to-quantile
  # using differential average of features as target.
  output$norm=
    output$unnorm$pw %>%
    t() %>%
    normalize.quantiles.use.target(output$fit$AveExpr) %>%
    t() %>%
    `dimnames<-`(dimnames(output$unnorm$pw))
  
  # Transform features by 1-bit stochastic gradient descent (SGD)
  # using the differential average.
  cat('Transform features by 1-bit stochastic gradient descent (SGD)\n')
  output$predictor_v=
    output$norm %>%
    sweep(2,output$fit$AveExpr,'-') %>%
    pbsapply(function(x){ifelse(x==0,0,ifelse(x>0,1,-1))}) %>%
    matrix(
      ncol=ncol(output$unnorm$pw)
      ,byrow=F
      ,dimnames=dimnames(output$unnorm$pw)
    ) %>%
    as.data.frame()
  
  # Get feature-wise mean of unnormalized features.
  output$predictor_m=
    output$unnorm$nw %>%
    colMeans2()
  
  # Get feature-wise standard deviation (SD) of unnormalized features.
  output$predictor_s=
    output$unnorm$nw %>%
    colSds()
  
  # Scale unnormalized features by the mean and SD.
  output$scaled=
    output$unnorm$nw %>%
    sweep(2,output$predictor_m,'-') %>%
    sweep(2,output$predictor_s,'/')
  
  # Compute feature-feature Pearson correlation matrix
  # of unnormalized features.
  cat('Compute feature-feature Pearson correlation matrix\n')
  output$predictor_p=
    colnames(output$unnorm$nw) %>%
    lapply(X=seq(length(.)-1),Y=.,function(X,Y){
      data.frame(
        predictor1=Y[X]
        ,predictor2=Y[(X+1):length(.)]
      )
    }) %>%
    do.call(rbind,.) %>%
    mutate(
      pearson=
        pbsapply(X=seq(nrow(.)),Y=.,Z=output$scaled,function(X,Y,Z){
          cor(
            Z[,Y$predictor1[X]]
            ,Z[,Y$predictor2[X]]
          )
        })
    ) %>%
    rbind(
      setNames(select(.,predictor2,predictor1,everything()),colnames(.))
      ,data.frame(
        predictor1=colnames(output$unnorm$nw)
        ,predictor2=colnames(output$unnorm$nw)
        ,pearson=1
      )
    ) %>%
    spread(predictor2,pearson) %>%
    column_to_rownames(var='predictor1') %>%
    as.matrix()
  
  # Conduct Barnes-Hut t-moderated stochastic neighbor embedding (t-SNE).
  cat('Conduct Barnes-Hut t-moderated stochastic neighbor embedding (t-SNE)\n')
  suppressWarnings(set.seed(33,sample.kind=sample.kind))
  output$predictor_tsne=
    output$predictor_p %>%
    Rtsne(dims=3,verbose=T,is_distance=T)
  
  # Construct clique-extracted ontology (CliXO) (choose your own OS).
  output$predictor_c=
    output$predictor_p %>%
    clixo(os='windows')
  
  # Compile input.
  input=list()
  
  input$value=output$predictor_v
  
  input$outcome=
    output$outcome %>%
    mutate(outcome=as.integer(outcome=='event')) %>%
    pull(outcome) %>%
    setNames(rownames(input$value))
  
  input$similarity=output$predictor_p
  
  input$mapping=
    output$predictor_tsne$Y %>%
    `rownames<-`(colnames(input$value))
  
  input$ontology=output$predictor_c
  
  input$fit=output$fit
  
  # Compile into a TidySet.
  cat('Compile into a TidySet\n')
  output=
    TidySet.compile(
      value=input$value
      ,outcome=input$outcome
      ,similarity=input$similarity
      ,mapping=input$mapping
      ,ontology=input$ontology
      ,ranked=T
      ,dims=7
      ,decreasing=F
      ,seed_num=33
    )
  
  saveRDS(input,'data/input.rds')
  saveRDS(output,'data/output.rds')
}else{
  cat(readRDS('data/log.rds')[['output_predictor_v']])
  cat(readRDS('data/log.rds')[['output_predictor_p']])
  cat(readRDS('data/log.rds')[['output_predictor_tsne']])
  cat(readRDS('data/log.rds')[['output']])
  input=readRDS('data/input.rds')
  output=readRDS('data/output.rds')
}
```

```{r Change outcome for timing regression, include=FALSE}
output_reg=
  output %>%
  `phenoData<-`(
    output %>%
      phenoData() %>%
      `pData<-`(
        output %>%
          pData() %>%
          rownames_to_column(var='id') %>%
          mutate(
            outcome=
              pw_hlin %>%
              .[,!pData(phenoData(.))$censoring] %>%
              .[,pData(protocolData(.))$int] %>%
              .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F] %>%
              protocolData() %>%
              pData() %>%
              rownames_to_column(var='id') %>%
              mutate(
                timing=as.duration(latest_date-admission_date)/ddays(1)
              ) %>%
              select(id,timing) %>%
              pull(timing) %>%
              setNames(rownames(input$value))
          ) %>%
          column_to_rownames(var='id')
      )
  )
```

```{r Show ontonet, eval=FALSE, include=FALSE}
output %>%
  viz.ontonet(feature=F) %>%
  plot(
    node.size=5
    ,edge.arrow.size=0.25
    ,main='Untrained Ontonet'
  )
```

```{r Create a function to refresh keras backend session, include=FALSE}
source('R/refresh_session-function.R')
refresh_session()
```

```{r Create a function to create training function given lambda, include=FALSE}
source('R/trainer_generator-function.R')
```

```{r Create a function to create regressor function given lambda, include=FALSE}
source('R/regressor_generator-function.R')
```

```{r Conduct hyperparameter tuning for a DI-VNN, include=FALSE}
lambda=10^seq(-10,-1,len=10)
if(run_heavy_computation){
  cat('Conduct hyperparameter tuning for DI-VNN model\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model=
      trainer_generator(
        output
        ,class_weight=
          pw_hlin %>%
          .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F] %>%
          phenoData() %>%
          pData() %>%
          select(outcome) %>%
          mutate(outcome=as.integer(outcome=='event')) %>%
          cbind(
            training_parameters$outcome_weights %>%
            .[rownames(.)%in%training_parameters$pre_calib_set,,drop=F]
          ) %>%
          filter(!duplicated(.)) %>%
          arrange(outcome) %>%
          lapply(X=1,Y=.,function(X,Y){
            setNames(Y$weight,Y$outcome)
          }) %>%
          .[[1]]
        ,epochs=5
        ,patience=round(5/2)
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=T
        ,verbose=1
      )
    
    tuning_divnn=list()
    i=1
    for(j in seq(i,length(lambda))){
      suppressWarnings(set.seed(33,sample.kind=sample.kind))
      tuning_divnn[[j]]=surrogate_model(lambda[j])
    }
  
  rm(i,j)
  cat('End:',as.character(now()))
  saveRDS(tuning_divnn,'data/tuning_divnn.rds')
}else{
  cat(readRDS('data/log.rds')[['tuning_divnn']])
  tuning_divnn=readRDS('data/tuning_divnn.rds')
}
```

```{r Plot DI-VNN tuning results, eval=FALSE, include=FALSE}
tuning_divnn %>%
  sapply(function(x)min(x$Score)) %>%
  data.frame(mean_AUROC=.) %>%
  mutate(which_param=seq(nrow(.))) %>%
  mutate(best=which.max(mean_AUROC)) %>%
  qplot(which_param,mean_AUROC,color=which_param==best,data=.) +
  geom_smooth(method='loess',color='black',formula=y~x) +
  scale_x_continuous(breaks=seq(lambda))
```

```{r Conduct modeling for a DI-VNN, include=FALSE}
if(run_heavy_computation){
  cat('Conduct modeling for DI-VNN\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model=
      trainer_generator(
        output
        ,path='data/ontonet'
        ,class_weight=
          pw_hlin %>%
          .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F] %>%
          phenoData() %>%
          pData() %>%
          select(outcome) %>%
          mutate(outcome=as.integer(outcome=='event')) %>%
          cbind(
            training_parameters$outcome_weights %>%
            .[rownames(.)%in%training_parameters$pre_calib_set,,drop=F]
          ) %>%
          filter(!duplicated(.)) %>%
          arrange(outcome) %>%
          lapply(X=1,Y=.,function(X,Y){
            setNames(Y$weight,Y$outcome)
          }) %>%
          .[[1]]
        ,epochs=500
        ,patience=100
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=F
        ,checkpoint=T
        ,verbose=1
      )
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    modeling_divnn=surrogate_model(lambda[10])
    
  cat('End:',as.character(now()))
  save_model_weights_hdf5(modeling_divnn$ontonet,'data/ontonet.h5')
  saveRDS(modeling_divnn,'data/modeling_divnn.rds')
}else{
  cat(readRDS('data/log.rds')[['modeling_divnn']])
  modeling_divnn=readRDS('data/modeling_divnn.rds')
  refresh_session()
  modeling_divnn$ontonet=
    output %>%
    # readLines('data/ontonet.json') %>%
    # model_from_json() %>%
    generator.ontonet(l2_norm=lambda[10]) %>%
    load_model_weights_hdf5('data/ontonet.h5') %>%
    # load_model_hdf5('data/entire_ontonet.h5',compile=T)
    compile(
      optimizer=optimizer_sgd(
        lr=modeling_divnn$history$metrics$lr %>%
          .[which.max(modeling_divnn$history$metrics$val_root_roc)]
        ,momentum=0.9
      )
      ,loss='mean_squared_error'
      ,loss_weights=c(rep(
          0.3/(0.3*(length(.$outputs)-1)+1),length(.$outputs)-1)
          ,1/(0.3*(length(.$outputs)-1)+1)
        )
      ,metrics=c(
          tf$keras$metrics$AUC(name='roc')
          ,tf$keras$metrics$TruePositives(name='tp')
          ,tf$keras$metrics$FalseNegatives(name='fn')
          ,tf$keras$metrics$FalsePositives(name='fp')
          ,tf$keras$metrics$TrueNegatives(name='tn')
        )
    )
}
```

```{r Iteration plot, fig.height=7, fig.width=7, eval=FALSE, include=FALSE}
modeling_divnn$history$metrics %>%
  .[c('root_loss','val_root_loss','root_roc','val_root_roc','lr')] %>%
  do.call(cbind,.) %>%
  as.data.frame() %>%
  mutate(iteration=seq(nrow(.))) %>%
  gather(metric,value,-iteration) %>%
  mutate(set=str_remove_all(metric,'_loss|_roc')) %>%
  mutate(
    metric=case_when(
      metric=='lr'~'1. LR'
      ,str_detect(metric,'loss')~'2. Loss'
      ,str_detect(metric,'roc')~'3. AUROC'
      ,TRUE~''
    )
  ) %>%
  qplot(iteration,value,color=set,data=.,geom='line') +
  geom_smooth(method='loess',formula=y~x,se=F) +
  facet_wrap(~metric,scales='free_y',ncol=1) +
  theme_minimal()
```

```{r Conduct hyperparameter tuning for a DI-VNN timing regressor, include=FALSE}
lambda_reg=10^seq(-10,-1,len=10)
if(run_heavy_computation){
  cat('Conduct hyperparameter tuning for DI-VNN timing regressor model\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model_reg=
      regressor_generator(
        output_reg
        ,epochs=5
        ,patience=round(5/2)
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=T
        ,verbose=1
      )
    
    tuning_divnn_reg=list()
    i=1
    for(j in seq(i,length(lambda_reg))){
      suppressWarnings(set.seed(33,sample.kind=sample.kind))
      tuning_divnn_reg[[j]]=surrogate_model_reg(lambda_reg[j])
    }
  
  rm(i,j)
  cat('End:',as.character(now()))
  saveRDS(tuning_divnn_reg,'data/tuning_divnn_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['tuning_divnn_reg']])
  tuning_divnn_reg=readRDS('data/tuning_divnn_reg.rds')
}
```

```{r DI-VNN timing regression tuning results, eval=FALSE, include=FALSE}
tuning_divnn_reg %>%
  sapply(function(x)min(x$Score)) %>%
  data.frame(mean_RMSE=.) %>%
  mutate(which_param=seq(nrow(.))) %>%
  mutate(best=which.min(mean_RMSE)) %>%
  qplot(which_param,mean_RMSE,color=which_param==best,data=.) +
  geom_smooth(method='loess',color='black',formula=y~x) +
  scale_x_continuous(breaks=seq(lambda_reg))
```

```{r Conduct modeling for a DI-VNN timing regressor, include=FALSE}
if(run_heavy_computation){
  cat('Conduct modeling for DI-VNN timing regressor\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model_reg=
      regressor_generator(
        output_reg
        ,path='data/ontonet_reg'
        ,epochs=500
        ,patience=100
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=F
        ,checkpoint=T
        ,verbose=1
      )
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    modeling_divnn_reg=surrogate_model_reg(lambda_reg[3])
    
  cat('End:',as.character(now()))
  save_model_weights_hdf5(modeling_divnn_reg$ontonet,'data/ontonet_reg.h5')
  saveRDS(modeling_divnn_reg,'data/modeling_divnn_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['modeling_divnn_reg']])
  modeling_divnn_reg=readRDS('data/modeling_divnn_reg.rds')
  refresh_session()
  modeling_divnn_reg$ontonet=
    output_reg %>%
    generator.ontonet(
      l2_norm=lambda_reg[3]
      ,output_unit=1
      ,output_activation=NULL
    ) %>%
    # readLines('data/ontonet.json') %>%
    # model_from_json() %>%
    load_model_weights_hdf5('data/ontonet_reg.h5') %>%
    compile(
      optimizer=optimizer_sgd(
        lr=modeling_divnn_reg$history$metrics$lr %>%
          .[which.min(modeling_divnn_reg$history$metrics$val_root_rmse)]
        ,momentum=0.9
      )
      ,loss='mean_squared_error'
      ,loss_weights=c(rep(
          0.3/(0.3*(length(.$outputs)-1)+1),length(.$outputs)-1)
          ,1/(0.3*(length(.$outputs)-1)+1)
        )
      ,metrics=c(tf$keras$metrics$RootMeanSquaredError(name='rmse'))
    )
}
```

```{r Reg iteration plot, fig.height=7, fig.width=7, eval=FALSE, include=FALSE}
modeling_divnn_reg$history$metrics %>%
  .[c('root_loss','val_root_loss','root_rmse','val_root_rmse','lr')] %>%
  do.call(cbind,.) %>%
  as.data.frame() %>%
  mutate(iteration=seq(nrow(.))) %>%
  gather(metric,value,-iteration) %>%
  mutate(set=str_remove_all(metric,'_loss|_rmse')) %>%
  mutate(
    metric=case_when(
      metric=='lr'~'1. LR'
      ,str_detect(metric,'loss')~'2. Loss'
      ,str_detect(metric,'rmse')~'3. RMSE'
      ,TRUE~''
    )
  ) %>%
  qplot(iteration,value,color=set,data=.,geom='line') +
  geom_smooth(method='loess',formula=y~x,se=F) +
  facet_wrap(~metric,scales='free_y',ncol=1) +
  theme_minimal()
```

```{r Create a function to transform test data for DI-VNN input, include=FALSE}
source('R/test_transformer-function.R')
```

```{r Transform test data for DI-VNN input, include=FALSE}
if(run_heavy_computation){
  test_data=
    list('calib','ran','geo','tem','bgt') %>%
    lapply(function(x){
      cat('Compile data for set of:',x,'\n')
      
      # Select the subset.
      if(x=='int'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int]
      }else if(x=='calib'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int] %>%
          .[,!colnames(.)%in%training_parameters$pre_calib_set,drop=F]
      }else{
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))[[x]]]
      }
      
      # Transform the subset.
      test_transformer(
        test_data=data
        ,SGD1bit_fit=input$fit
        ,similarity=input$similarity
        ,mapping=input$mapping
        ,ontology=input$ontology
        ,ranked=T
        ,dims=7
        ,decreasing=F
        ,seed_num=33
      )
    }) %>%
    setNames(c('calib','ran','geo','tem','bgt'))
  saveRDS(test_data,'data/test_data.rds')
}else{
  cat(readRDS('data/log.rds')[['test_data']])
  test_data=readRDS('data/test_data.rds')
}
```

```{r Change outcome of test data for timing regression, include=FALSE}
if(run_heavy_computation){
  test_data_reg=
    list('int','calib','ran','geo','tem','bgt') %>%
    
    # Select the subset.
    lapply(function(x){
      cat('Compile data for set of:',x,'\n')
      if(x=='int'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int]
      }else if(x=='calib'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int] %>%
          .[,!colnames(.)%in%training_parameters$pre_calib_set,drop=F]
      }else{
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))[[x]]]
      }
      
      # Transform the subset.
      data2=
        data %>%
        test_transformer(
          SGD1bit_fit=input$fit
          ,similarity=input$similarity
          ,mapping=input$mapping
          ,ontology=input$ontology
          ,ranked=T
          ,dims=7
          ,decreasing=F
          ,seed_num=33
        )
      
      # Change outcome with the time of delivery.
      data2 %>%
        `phenoData<-`(
          data2 %>%
            phenoData() %>%
            `pData<-`(
              data2 %>%
                pData() %>%
                rownames_to_column(var='id') %>%
                mutate(
                  outcome=
                    data %>%
                    protocolData() %>%
                    pData() %>%
                    rownames_to_column(var='id') %>%
                    mutate(
                      timing=as.duration(latest_date-admission_date)/ddays(1)
                    ) %>%
                    select(id,timing) %>%
                    pull(timing) %>%
                    setNames(colnames(data))
                ) %>%
                column_to_rownames(var='id')
            )
        )
      
    }) %>%
    setNames(c('int','calib','ran','geo','tem','bgt'))
  saveRDS(test_data_reg,'data/test_data_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['test_data_reg']])
  test_data_reg=readRDS('data/test_data_reg.rds')
}
```

```{r Build a function for DI-VNN evaluation, include=FALSE}
source('R/eval_divnn-function.R')
```

```{r Build a function for DI-VNN calibration and evaluation, include=FALSE}
source('R/divnn_calibrator_evaluator-function.R')
```

```{r Build a function for DI-VNN timing regression evaluation, include=FALSE}
source('R/eval_divnn_reg-function.R')
```

```{r Build a function for DI-VNN time-regression and evaluation, include=FALSE}
source('R/divnn_timereg_evaluator-function.R')
```

```{r Calibrate and evaluate DI-VNN, include=FALSE}
if(run_heavy_computation){
  c(model$divnn
    ,calib_model$divnn
    ,eval_model$divnn) %<-% divnn_calibrator_evaluator(
      data=test_data
      ,modeling=modeling_divnn
      ,training=training_parameters
      ,batch_size=512
      ,title='Calibrate and evaluate DI-VNN'
      ,dir='data'
      ,file='divnn'
    )
}else{
  cat(readRDS('data/log.rds')[['divnn']])
  c(model$divnn
    ,calib_model$divnn
    ,eval_model$divnn) %<-% list(
      readRDS('data/divnn.rds')
      ,readRDS('data/calib_divnn.rds')
      ,readRDS('data/eval_divnn.rds')
    )
}
```

```{r Evaluate DI-VNN regressor, include=FALSE}
if(run_heavy_computation){
  c(timing_model$divnn
    ,eval_timing_model$divnn) %<-% divnn_timereg_evaluator(
      timing_data=test_data_reg
      ,data=test_data
      ,timing_model=modeling_divnn_reg
      ,model=modeling_divnn
      ,calib_model=calib_model$divnn
      ,batch_size=512
      ,title='Evaluate DI-VNN regressor'
      ,dir='data'
      ,file='divnn'
    )
}else{
  cat(readRDS('data/log.rds')[['timing_divnn']])
  c(timing_model$divnn
    ,eval_timing_model$divnn) %<-% list(
      readRDS('data/timing_divnn.rds')
      ,readRDS('data/eval_timing_divnn.rds')
    )
}
```

```{r DI-VNN training evaluation, eval=FALSE, include=FALSE}
modeling_divnn$history$metrics %>%
  sapply(X=seq(length(.))
         ,Y=.
         ,Z=modeling_divnn$history$metrics %>%
            .[str_detect(names(.),'val_root_roc')] %>%
            unlist() %>%
            which.max()
         ,function(X,Y,Z){
    setNames(Y[[X]][[Z]][[1]],names(Y)[X])
  }) %>%
  data.frame(value=.) %>%
  .[str_detect(rownames(.),'root'),,drop=F] %>%
  kable() %>%
  kable_classic()
```

```{r Plot-table plotting timing regression, include=FALSE}
eval_timing_model_plt=
  eval_timing_model %>%
  lapply(X=names(.),Y=.,function(X,Y){
    Y[[X]] %>%
      lapply(X=names(.),Y=.,Z=X,K=Y,function(X,Y,Z,K){
        
        # Combine classification and estimation data.
        cbind(
            Y[[X]] %>%
              select(timing,outcome,timing_pred)
            ,K$divnn[[X]] %>%
              select(pred,calib_pred)
          ) %>%
          select_at(colnames(Y[[X]])) %>%
          
          # Bin per week.
          filter(timing_pred>0) %>%
          mutate(timing_pred=round(timing_pred/7)) %>%
          
          # Define outcome and the prediction.
          mutate(
            outcome=
              ifelse(outcome==1|outcome=='event','event','nonevent') %>%
                  factor(c('nonevent','event'))
            ,pred=
              ifelse(pred==1|pred=='event','event','nonevent') %>%
                  factor(c('nonevent','event'))
          ) %>%
          mutate(any_metric=factor('event',c('nonevent','event')))%>%
          
          # Summarize each of the evaluation metrics.
          gather(metric_name,metric,-timing,-timing_pred) %>%
          group_by(timing_pred,metric_name,metric) %>%
          summarize(
            avg=mean(timing/7)
            ,lb=mean(timing/7)-qnorm(0.975)*sd(timing/7)/sqrt(n())
            ,ub=mean(timing/7)+qnorm(0.975)*sd(timing/7)/sqrt(n())
          ) %>%
          mutate(set=X,model=Z)
      }) %>%
      do.call(rbind,.)
  }) %>%
  do.call(rbind,.)
```

## Evaluating the best model for classification and estimation

Model evaluation is already clearly described in the main text. Practical costs 
of prediction errors were considered when evaluating the models. Under-
prognosis may cause pregnancy monitoring to be off-guard. A pregnant woman with 
a preterm delivery may not reside in an area with a readily available NICU, 
particularly in low-resource settings. Over-prognosis may lead to unnecessary 
enrollment of patients into a cohort study in an early intervention for 
preventing PROM or complications, e.g., an antibiotic administration. This may 
also lead to unnecessary tests for more-specific prognostications.

In addition, unlike other time-varying outcomes, e.g., cancer, we did not 
predict a survival rate for the estimation task because the time interval for a 
pregnancy period is definitely known. It is also more intuitive for clinicians 
if the given information is the estimated days from the current visit to the 
day a pregnant woman will deliver a baby, as such normal delivery estimations 
are based on the last menstrual period and ultrasound examination.

We started from evaluating the calibration measures of all models for 
classification task. Then, we chose the best model for classification by AUROC 
using only internal validation (calibration split) bootstrapped for 30 times. 
Using the same subset, the best estimation model was also chosen.

```{r Show calibration plot before calibration, eval=FALSE, include=FALSE}
model %>%
  # Compute true probability interval.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,obs=mean(obs)
        ,lb=mean(obs)-qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
        ,ub=mean(obs)+qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Create calibration plot.
  qplot(event,obs,data=.) +
  geom_linerange(aes(ymin=lb,ymax=ub)) +
  geom_abline(lty=2) +
  facet_wrap(~model) +
  coord_equal() +
  scale_x_continuous('Predicted probability',limits=0:1) +
  scale_y_continuous('True probability',limits=0:1) +
  ggtitle('A. Before calibration') +
  theme(axis.text.x=element_text(angle=90,vjust=0.5,hjust=1))
```

```{r Show probability distribution before calibration, eval=FALSE, include=FALSE}
model %>%
  # Compute number of sample per predicted probability.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,n=n()
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Create histogram of the predicted probability.
  qplot(event,n,data=.,geom='col',na.rm=T) +
  facet_wrap(~model,scales='free_y') +
  scale_x_continuous(limits=0:1) +
  theme(axis.text.x=element_text(angle=90,vjust=0.5,hjust=1))
```

```{r Show calibration measures before calibration, eval=FALSE, include=FALSE}
model %>%
  # Compute true probability.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,2)
        ,obs=as.integer(obs=='event')
      ) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,obs=mean(obs)
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Fitting a line to get the intercept and slope.
  group_by(model) %>%
  do(tidy(lm(obs~event,data=.))) %>%
  
  # Wrap up.
  mutate(term=str_remove_all(term,'\\(|\\)') %>% str_to_lower()) %>%
  pivot_wider(
    model,names_from=c('term','term'),values_from=c('estimate','std.error')
  ) %>%
  select(model,estimate_intercept,std.error_intercept,everything()) %>%
  mutate_at(colnames(.) %>% .[.!='model'],function(x)round(x,2)) %>%
  
  # Show results as a table.
  kable() %>%
  kable_classic()
```

```{r Compare ROC among models before calibration, eval=FALSE, include=FALSE}
model %>%
  # Compute interval estimates.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$results
        ,ROC_lb=ROC-qnorm(0.975)*ROCSD
        ,ROC_ub=ROC+qnorm(0.975)*ROCSD
      ) %>%
      mutate(model=X) %>%
      select(model,ROC,ROC_lb,ROC_ub)
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(model=reorder(model,ROC)) %>%
  
  # Plot the comparison of the AUROCs.
  qplot(model,ROC,data=.) +
  geom_errorbar(aes(ymin=ROC_lb,ymax=ROC_ub),width=0.35) +
  geom_hline(yintercept=0.5,lty=2) +
  coord_flip()
```

```{r Compare AUC of ROC among models before calibration, eval=FALSE, include=FALSE}
model %>%
  # Compute the interval estimates.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$results
        ,ROC_lb=ROC-qnorm(0.975)*ROCSD
        ,ROC_ub=ROC+qnorm(0.975)*ROCSD
      ) %>%
      mutate(model=X) %>%
      select(model,ROC,ROC_lb,ROC_ub)
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Show results as a table.
  kable() %>%
  kable_classic()
```

```{r Show calibration plot after calibration, eval=FALSE, include=FALSE}
calib_model %>%
  # Compute true probability interval.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,obs=mean(obs)
        ,lb=mean(obs)-qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
        ,ub=mean(obs)+qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Create calibration plot.
  qplot(event,obs,data=.) +
  geom_linerange(aes(ymin=lb,ymax=ub)) +
  geom_abline(lty=2) +
  facet_wrap(~model) +
  coord_equal() +
  scale_x_continuous('Predicted probability',limits=0:1) +
  scale_y_continuous('True probability',limits=0:1) +
  ggtitle('B. After calibration') +
  theme(axis.text.x=element_text(angle=90,vjust=0.5,hjust=1))
```

```{r Show probability distribution after calibration, eval=FALSE, include=FALSE}
calib_model %>%
  # Compute number of sample per predicted probability.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,n=n()
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Create histogram of the predicted probability.
  qplot(event,n,data=.,geom='col',na.rm=T) +
  facet_wrap(~model,scales='free_y') +
  theme(axis.text.x=element_text(angle=90,vjust=0.5,hjust=1))
```

```{r Show calibration measures after calibration, eval=FALSE, include=FALSE}
calib_model %>%
  # Compute true probability.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,obs=mean(obs)
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Fitting a line to get the intercept and slope.
  group_by(model) %>%
  do(tidy(lm(obs~event,data=.))) %>%
  
  # Wrap up.
  mutate(term=str_remove_all(term,'\\(|\\)') %>% str_to_lower()) %>%
  pivot_wider(
    model,names_from=c('term','term'),values_from=c('estimate','std.error')
  ) %>%
  select(model,estimate_intercept,std.error_intercept,everything()) %>%
  mutate_at(colnames(.) %>% .[.!='model'],function(x)round(x,2)) %>%
  
  # Show results as a table.
  kable() %>%
  kable_classic()
```

```{r Compare ROC among models after calibration, eval=FALSE, include=FALSE}
calib_model %>%
  # Compute the interval estimates.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$results
        ,ROC_lb=ROC-qnorm(0.975)*ROCSD
        ,ROC_ub=ROC+qnorm(0.975)*ROCSD
      ) %>%
      mutate(model=X) %>%
      select(model,ROC,ROC_lb,ROC_ub)
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(model=reorder(model,ROC)) %>%
  
  # Plot the comparison of the AUROCs.
  qplot(model,ROC,data=.) +
  geom_errorbar(aes(ymin=ROC_lb,ymax=ROC_ub),width=0.35) +
  geom_hline(yintercept=0.5,lty=2) +
  coord_flip()
```

```{r Compare AUC of ROC among models after calibration, eval=FALSE, include=FALSE}
calib_model %>%
  # Compute interval estimates.
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$results
        ,ROC_lb=ROC-qnorm(0.975)*ROCSD
        ,ROC_ub=ROC+qnorm(0.975)*ROCSD
      ) %>%
      mutate(model=X) %>%
      select(model,ROC,ROC_lb,ROC_ub)
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  
  # Show results as a table.
  kable() %>%
  kable_classic()
```

```{r Internal validation of timing regression, eval=FALSE, include=FALSE}
eval_timing_model_plt %>%
  filter(set=='timing_calib') %>%
  mutate(
    model=factor(model,c(names(eval_timing_model),'C_divnn_R_pc_rf'))
  ) %>%
  qplot(
    timing_pred,avg,color=metric
    ,data=.
    ,geom='smooth',method='loess',formula=y~x
  ) +
  geom_abline(slope=1,intercept=0,lty=2) +
  facet_grid(metric_name~model,scales='free')
```

## External validation

We also confirmed the robustness of the best models using external validation 
sets. By stratified random splitting, we provided three non-overlapping splits 
comprising ~20% for external validation: (1) geographical split; (2) temporal 
split; and (3) geotemporal split. We set these subsets challenging to predict 
by our models, in which the geotemporal split was the most difficult. This way 
we can estimate robustness of our model generalizability. In addition, 
uncertainty intervals were also computed by bootstrapping for 30 times. But, 
these do not reflect common situations nationwide; thus, the ~20% of the 
remaining was held out by simple random splitting for external validation, 
which is called external random split. For association tests and predictive 
modeling, including calibration, we only used the remaining ~64% of all the 
selected visits. A clear description on model validation is given in a protocol 
we followed for these procedures, as previously described [...].

```{r ROC scores of any models for each partition set, eval=FALSE, include=FALSE}
eval_model %>%
  
  # Get AUROC results of testing sets.
  lapply(X=names(.),Y=.,function(X,Y){
    Z=Y[[X]] %>%
      lapply(function(x)x$optres$Group1['AUC-ROC',]) %>%
      do.call(rbind,.) %>%
      rownames_to_column(var='set') %>%
      mutate(model=X)
    suppressWarnings(separate(Z,CI,c('lb','ub'),sep='-'))
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up things.
  mutate(
    lb=ifelse(lb=='NA',Score,lb)
    ,ub=ifelse(ub=='NA',Score,ub)
  ) %>%
  mutate_at(
    colnames(.) %>% .[!.%in%c('model','set')]
    ,function(x)as.numeric(x)
  )%>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'1. Causal LR'
    ,model=='pc_elnet'~'2. PC-LR'
    ,model=='pc_rf'~'3. PC-RF'
    ,model=='pc_gbm'~'4. PC-GBM'
    ,model=='divnn'~'5. DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(model=factor(model,sort(unique(model)))) %>%
  mutate(
    calibrated=ifelse(str_detect(set,'nocalib'),'Non-Calibrated','Calibrated')
  ) %>%
  mutate(set=case_when(
    set=='nocalib_ran'~'1. Random test'
    ,set=='calib_ran'~'1. Random test'
    ,set=='nocalib_geo'~'2. Geographical test'
    ,set=='calib_geo'~'2. Geographical test'
    ,set=='nocalib_tem'~'3. Temporal test'
    ,set=='calib_tem'~'3. Temporal test'
    ,set=='nocalib_bgt'~'4. Geotemporal test'
    ,set=='calib_bgt'~'4. Geotemporal test'
    ,TRUE~''
  )) %>%
  mutate(set=factor(set,rev(sort(unique(set))))) %>%
  
  # Compute the mean of score per subset.
  group_by(set) %>%
  mutate(set_score=mean(Score)) %>%
  ungroup() %>%
  
  # Add the calibrated models of training set.
  rbind(
    calib_model %>%
      # Compute the interval estimates.
      lapply(X=names(.),Y=.,function(X,Y){
        mutate(
            Y[[X]]$results
            ,ROC_lb=ROC-qnorm(0.975)*ROCSD
            ,ROC_ub=ROC+qnorm(0.975)*ROCSD
          ) %>%
          mutate(model=X) %>%
          select(model,ROC,ROC_lb,ROC_ub) %>%
          setNames(c('model','Score','lb','ub'))
      }) %>%
      do.call(rbind,.) %>%
      
      # Clean up naming.
      mutate(model=case_when(
        model=='causal_ridge'~'1. Causal LR'
        ,model=='pc_elnet'~'2. PC-LR'
        ,model=='pc_rf'~'3. PC-RF'
        ,model=='pc_gbm'~'4. PC-GBM'
        ,model=='divnn'~'5. DI-VNN'
        ,TRUE~''
      )) %>%
      mutate(model=reorder(model,Score)) %>%
      mutate(
        set='0. Internal validation'
        ,calibrated='Calibrated'
        ,set_score=mean(Score)
      ) %>%
      select(set,Score,lb,ub,model,everything())
  ) %>%
  
  # Add the non-calibrated models of training set.
  rbind(
    model %>%
      # Compute the interval estimates.
      lapply(X=names(.),Y=.,function(X,Y){
        mutate(
            Y[[X]]$results
            ,ROC_lb=ROC-qnorm(0.975)*ROCSD
            ,ROC_ub=ROC+qnorm(0.975)*ROCSD
          ) %>%
          mutate(model=X) %>%
          select(model,ROC,ROC_lb,ROC_ub) %>%
          setNames(c('model','Score','lb','ub'))
      }) %>%
      do.call(rbind,.) %>%
      
      # Clean up naming.
      mutate(model=case_when(
        model=='causal_ridge'~'1. Causal LR'
        ,model=='pc_elnet'~'2. PC-LR'
        ,model=='pc_rf'~'3. PC-RF'
        ,model=='pc_gbm'~'4. PC-GBM'
        ,model=='divnn'~'5. DI-VNN'
        ,TRUE~''
      )) %>%
      mutate(model=reorder(model,Score)) %>%
      mutate(
        set='0. Internal validation'
        ,calibrated='Non-Calibrated'
        ,set_score=mean(Score)
      ) %>%
      select(set,Score,lb,ub,model,everything())
  ) %>%
  
  # Show results as a table and a plot.
  lapply(X=1:2,Y=.,function(X,Y){
    if(X==1){
      Y  %>%
        arrange(set,desc(Score)) %>%
        kable() %>%
        kable_classic()
    }else{
      Y %>%
        qplot(model,Score,data=.) +
        geom_errorbar(aes(ymin=lb,ymax=ub),width=0.5) +
        geom_hline(aes(yintercept=set_score),lty=2) +
        geom_hline(yintercept=0.5,lty=2) +
        facet_grid(set~calibrated) +
        coord_flip() +
        scale_x_discrete('Data Partition') +
        scale_y_continuous('AUROC (95% CI)') +
        theme(strip.text.y=element_text(angle=0))
    }
  })
```

```{r Any validation of timing regression, eval=FALSE, include=FALSE}
eval_timing_model_plt %>%
  lapply(X=.$set %>% .[!duplicated(.)],Y=.,function(X,Y){
    filter(Y,set==X) %>%
      mutate(
        model=factor(model,c(names(eval_timing_model),'C_divnn_R_pc_rf'))
      ) %>%
      qplot(
        timing_pred,avg,color=metric
        ,data=.
        ,geom='smooth',method='loess',formula=y~x,na.rm=T
      ) +
      geom_abline(slope=1,intercept=0,lty=2) +
      facet_grid(metric_name~model,scales='free') +
      ggtitle(X)
  })
```

## Exploring the best model

Model exploration method is also described in the main text (see Model 
evaluation in Methods). In this Supplement (see R Markdown and R Script), we 
directly explored the best model for classification task. None of the remaining 
models were explored.

```{r Visualization tables, include=FALSE}
if(run_heavy_computation){
  visualization=list()
  
  cat('Get ontonet visualisation table.\n')
  visualization$ontonet=
    output %>%
    viz.ontonet(
      feature=F
      ,eval.results=modeling_divnn$evaluation
      ,eval.metric='roc'
      ,eval.pal=c('#E64B35FF','#00A087FF')
    )
  
  cat('Get ontoarray visualisation table.\n')
  visualization$ontoarray=
    output %>%
    viz.ontoarray(modeling_divnn$ontonet,batch_size=512,verbose=T)
  
  saveRDS(visualization,'data/visualization.rds')
}else{
  cat(readRDS('data/log.rds')[['visualization']])
  visualization=readRDS('data/visualization.rds')
}
```

```{r Build a function to plot ontonet, include=FALSE}
source('R/plot.viz.ontonet-function.R')
```

```{r Ontonet, fig.height=12, fig.width=18, eval=FALSE, include=FALSE}
visualization$ontonet %>%
  plot.viz.ontonet(
    node.size=8
    ,node.shape='square'
    ,label=T
    ,label.family='serif'
    ,label.cex=0.85
    ,label.color='black'
    ,asp=0
    ,ylim=c(-1,1)
    ,xlim=c(-1,1)
  )
```

```{r Build a function to plot ontoarray, include=FALSE}
source('R/plot.viz.ontoarray-function.R')
```

```{r Ontoarray, fig.height=42, fig.width=12, eval=FALSE, include=FALSE}
visualization$ontoarray %>%
  plot.viz.ontoarray(
    pal=c('#E64B35FF','#00A087FF')
    ,label=T
    ,grid_col=12
  )
```

```{r Filter non-zero ontotype, echo=FALSE}
visualized_codes=
  
  # Get feature data from the ontoarray.
  visualization$ontoarray %>%
  lapply(function(x){
    x$ontotype %>%
      lapply(X=seq(nrow(.)),Y=.,Z=x$output,function(X,Y,Z){
        K=Z[Y$x[X],Y$y[X],Y$z[X]]
        if(K==0){
          NULL
        }else{
          K
        }
      }) %>%
      setNames(x$ontotype$feature)
  }) %>%
  setNames(names(visualization$ontoarray)) %>%
  unlist() %>%
  data.frame(output=.) %>%
  rownames_to_column(var='ontotype') %>%
  separate(ontotype,c('ontotype','code'),sep='\\.') %>%
  
  # Add tendency to event or nonevent.
  mutate(outcome=ifelse(output>=0,'event','nonevent')) %>%
  
  # Add the description.
  left_join(
    pw_hlin %>%
      fData() %>%
      rownames_to_column(var='code')
    ,by='code'
  ) %>%
  select(outcome,ontotype,code,desc) %>%
  arrange(ontotype,code,outcome) %>%
  
  # Simplify outcome.
  group_by(code,ontotype,desc) %>%
  summarize(outcome=paste0(outcome,collapse=' & '),.groups='drop') %>%
  mutate(outcome=ifelse(outcome=='event & nonevent','both',outcome)) %>%
  arrange(code,ontotype,factor(outcome,c('event','nonevent','both')))
```

```{r Show non-zero ontotype, eval=FALSE, include=FALSE}
visualized_codes %>%
  kable() %>%
  kable_classic()
```

## Preparing web application

For web application, we prepared an example dataset, user interface, and 
processing script at the side of server computer. No line of codes for the web 
is included in the R Markdown or R Script. Description for this web application 
is already clearly described in the main text.

```{r Save variables for app, eval=FALSE, include=FALSE}
# Save the feature and the reg expression.
int_nps_eol_p %>%
  select(key) %>%
  mutate_at('key',function(x){
    ifelse(str_detect(x,'causal_'),paste0(str_remove_all(x,'causal_'),'*'),x)
  }) %>%
  left_join(rename(dag$measure_nodes,key=label),by='key') %>%
  mutate(name=ifelse(is.na(name),key,name)) %>%
  mutate_at('key',function(x){
    ifelse(str_detect(x,'\\*'),paste0('causal_',str_remove_all(x,'\\*')),x)
  }) %>%
  setNames(c('features','regex')) %>%
  saveRDS('data/features.rds')

# Save nationwide historical rate.
preproc(nw_int_hlin)$hist_rate %>%
  filter(mh%in%readRDS('data/features.rds')$features) %>%
  saveRDS('data/hist_rates.rds')

# Save PC names.
selected_pc$term %>%
  saveRDS('data/pcs.rds')

# Reduce caret object like random forest.
source('R/reduce_rf-function.R')
readRDS('data/timing_pc_rf.rds') %>%
  reduce_rf() %>%
  saveRDS('data/small_timing_pc_rf.rds')
readRDS('data/calib_divnn.rds') %>%
  reduce_rf() %>%
  saveRDS('data/small_calib_divnn.rds')

# Save the ontonotation.
model_weight$divnn %>%
  select(Ontology,Predictor,Description) %>%
  filter(!duplicated(.)) %>%
  arrange(desc(Ontology),Predictor,Description) %>%
  saveRDS('data/ontonotation.rds')
```

```{r Prepare ROC and prediction tables, eval=FALSE, include=FALSE}
# ROC table
calib_model$divnn$pred %>%
  
  # Take needed columns.
  select(Resample,rowIndex,event,obs) %>%
  setNames(c('subset','index','score','outcome')) %>%
  mutate(outcome=as.double(outcome=='event')) %>%
  
  # Create multiple thresholds.
  left_join(
    select(.,index) %>%
      filter(!duplicated(.)) %>%
      arrange(index) %>%
      cbind(
        data.frame(
            key=paste0('th_',str_pad(0:100,3,'left','0'))
            ,value=seq(0,1,len=101)
          ) %>%
          spread(key,value)
      )
    ,by='index'
  ) %>%
  
  # Compute confusion matrix.
  gather(key,th,-subset,-index,-score,-outcome) %>%
  select(-key) %>%
  mutate(pred=as.integer(score>th)) %>%
  mutate(
    tp=as.integer(outcome==1 & pred==1)
    ,fn=as.integer(outcome==1 & pred==0)
    ,fp=as.integer(outcome==0 & pred==1)
    ,tn=as.integer(outcome==0 & pred==0)
  ) %>%
  select(-index,-score,-outcome,-pred) %>%
  group_by(subset,th) %>%
  summarize_all(sum) %>%
  ungroup() %>%
  
  # Compute evaluation metrics.
  mutate(
    tpr=tp/(tp+fn+1e-17)
    ,tnr=tn/(tn+fp+1e-17)
    ,ppv=tp/(tp+fp+1e-17)
    ,npv=tn/(tn+fn+1e-17)
  ) %>%
  select(-subset,-tp,-fn,-fp,-tn) %>%
  
  # Compute ROC components.
  group_by(th) %>%
  summarize_all(function(x){
    y=mean(x)+(-1:1)*qnorm(0.975)*sd(x)/sqrt(length(x))
    paste0(y,collapse='|')
  }) %>%
  gather(metric,interval,-th) %>%
  separate(interval,c('lb','avg','ub'),sep='\\|') %>%
  mutate_at(c('lb','avg','ub'),as.numeric) %>%
  mutate_at(c('lb','avg','ub'),function(x){
    round(ifelse(x<0,0,ifelse(x>1,1,x)),4)
  }) %>%
  
  # Save the ROC table.
  saveRDS('data/classification.rds')

# Prediction table
modeling_divnn$ontonet %>%
  
  # Use calibration split for classification.
  predict_generator(
    generator=
      generator.ontoarray(
        test_data$calib
        ,seq(ncol(test_data$calib))
        ,batch_size=512
      )
    ,steps=ceiling(ncol(test_data$calib)/512)
    ,verbose=1
  ) %>%
  setNames(
    modeling_divnn$history$metrics %>%
      names() %>%
      .[str_detect(.,'val_')
        &str_detect(.,'_loss')
        &str_detect(.,'ONT|root')] %>%
      str_remove_all('val_|_loss')
  ) %>%
  lapply(function(y){
    y[seq(ncol(test_data$calib)),,drop=F]
  }) %>%
  c(list(outcome=test_data$calib$outcome)) %>%
  as.data.frame() %>%
  select(root,outcome) %>%
  rename(event=root) %>%
  mutate(nonevent=event) %>%
  select(nonevent,everything()) %>%
  `rownames<-`(colnames(test_data$calib)) %>%
  
  # Get the calibrated, predicted probability.
  cbind(
    suppressWarnings(predict(calib_model$divnn,newdata=.,type='prob')) %>%
      rename_all(function(x)paste0(x,'2'))
  ) %>%
  select(-nonevent,-event) %>%
  rename_all(function(x)str_remove_all(x,'2')) %>%
  select(event) %>%
  
  # Save the prediction table.
  saveRDS('data/prediction.rds')
```

```{r Show training samples with the best predictions, eval=FALSE, include=FALSE}
pw_hlin %>%
  
  # Select true classification in estimation data of calibration split.
  .[,eval_timing_model$divnn$timing_calib %>%
      select(-timing,-timing_pred) %>%
      cbind(
        eval_timing_model$pc_rf$timing_calib %>%
          select(timing,timing_pred)
      ) %>%
      rownames_to_column(var='id') %>%
      filter(as.integer(calib_pred=='event')==outcome) %>%
      arrange(timing_pred) %>%
      pull(id)
  ] %>%
  
  # Get the phenotype and protocol data.
  lapply(X=1,Y=.,function(X,Y){
    Z=phenoData(Y) %>%
      pData()
    K=protocolData(Y) %>%
      pData()
    Z %>%
      rownames_to_column(var='id') %>%
      left_join(
        K %>%
          rownames_to_column(var='id')
        ,by='id'
      )
  }) %>%
  .[[1]] %>%
  select(-outcome) %>%
  
  # Join the estimation data of calibration split with true classification 
  # to the previous data.
  left_join(
    eval_timing_model$divnn$timing_calib %>%
      select(-timing,-timing_pred) %>%
      cbind(
        eval_timing_model$pc_rf$timing_calib %>%
          select(timing,timing_pred)
      ) %>%
      rownames_to_column(var='id') %>%
      filter(as.integer(calib_pred=='event')==outcome) %>%
      arrange(timing_pred)
    ,by='id'
  ) %>%
  
  # Convert the estimation results as week.
  mutate(timing_pred=round(timing_pred/7),timing=round(timing/7)) %>%
  
  # Join the estimation results based on the classification.
  left_join(
    eval_timing_model_plt  %>%
      filter(
        model=='pc_rf' & set=='timing_calib' & metric_name=='calib_pred'
      ) %>%
      rename(calib_pred=metric) %>%
      ungroup() %>%
      select(calib_pred,timing_pred,lb,ub) %>%
      mutate(
        fall_into_interval=
          timing_pred>=lb & timing_pred<=ub & abs(ub-lb)<=timing_pred
          & lb<=42 & ub<=42
        ,fall_into_interval=
          ifelse(is.na(fall_into_interval),FALSE,fall_into_interval)
      ) %>%
      select(calib_pred,timing_pred,fall_into_interval)
    ,by=c('calib_pred','timing_pred')
  ) %>%
  
  # Identify the estimation results that fall into the true interval.
  filter(fall_into_interval) %>%
  
  # By manual inspection, define the optimum estimation period 
  # for each predicted outcome.
  filter(
    (calib_pred=='nonevent' & timing_pred>=9 & timing_pred<=34)
    |
    (calib_pred=='event' & timing_pred>=6 & timing_pred<=26)
  ) %>%
  
  # Get the estimation results that have error less than 6 weeks 
  # for predicted time of delivery between 6 and 26 weeks' gestation.
  filter(abs(timing_pred-timing)<=6 & timing_pred>6 & timing_pred<26) %>%
  select(
    subject_id
    ,healthcare_id
    ,admission_date
    ,outcome
    ,pred
    ,calib_pred
    ,timing
    ,timing_pred
    ,everything()
  ) %>%
  
  # Save these training examples with the best predictions.
  saveRDS('data/subject_examples.rds')
```

```{r Create individual ontoarray extractor function, include=FALSE}
source('R/ontoarray1-function.R')
```

```{r Show input example for the app, eval=FALSE, include=FALSE}
readRDS('data/subject_examples.rds') %>%
  filter(subject_id=='51160204.1' & healthcare_id=='051103')
```

```{r Create alternative example 2, eval=FALSE, include=FALSE}
app$subject_id='63230897.1'
app$healthcare_id='001924'


app$raw_visit=
  readRDS('data/target_visits.rds') %>%
  filter(subject_id==app$subject_id & healthcare_id==app$healthcare_id) %>%
  select(admission_date,code)

app$prediction_date=as.Date('2016-09-01')

app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  write_csv('data/example2.csv')
```

```{r Create alternative example 3, eval=FALSE, include=FALSE}
app$subject_id='12843776.1'
app$healthcare_id='195276'


app$raw_visit=
  readRDS('data/target_visits.rds') %>%
  filter(subject_id==app$subject_id & healthcare_id==app$healthcare_id) %>%
  select(admission_date,code)

app$prediction_date=as.Date('2015-08-29')

app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  write_csv('data/example3.csv')
```

```{r Create alternative example 4, eval=FALSE, include=FALSE}
app$subject_id='32331648.1'
app$healthcare_id='192469'


app$raw_visit=
  readRDS('data/target_visits.rds') %>%
  filter(subject_id==app$subject_id & healthcare_id==app$healthcare_id) %>%
  select(admission_date,code)

app$prediction_date=as.Date('2016-09-23')

app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  write_csv('data/example4.csv')
```

```{r Create alternative example 5, eval=FALSE, include=FALSE}
app$subject_id='53312440.1'
app$healthcare_id='212823'


app$raw_visit=
  readRDS('data/target_visits.rds') %>%
  filter(subject_id==app$subject_id & healthcare_id==app$healthcare_id) %>%
  select(admission_date,code)

app$prediction_date=as.Date('2016-06-03')

app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  write_csv('data/example5.csv')
```

```{r Create alternative example 6, eval=FALSE, include=FALSE}
app$subject_id='102366639.1'
app$healthcare_id='141404'


app$raw_visit=
  readRDS('data/target_visits.rds') %>%
  filter(subject_id==app$subject_id & healthcare_id==app$healthcare_id) %>%
  select(admission_date,code)

app$prediction_date=as.Date('2016-08-24')

app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  write_csv('data/example6.csv')
```

```{r Input example for the app, include=FALSE}
app=list()

app$subject_id='51160204.1'
app$healthcare_id='051103'

app$raw_visit=
  readRDS('data/target_visits.rds') %>%
  filter(subject_id==app$subject_id & healthcare_id==app$healthcare_id) %>%
  select(admission_date,code)

app$prediction_date=as.Date('2016-07-30')
```

```{r Save an input example as csv, eval=FALSE, include=FALSE}
app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  write_csv('data/example.csv')
```

```{r Show input table, eval=FALSE, include=FALSE}
app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  kable() %>%
  kable_classic()
```

```{r Show the true outcome and the timing, eval=FALSE, include=FALSE}
c(outcome=
    readRDS('data/outcome.rds') %>%
    filter(subject_id==app$subject_id) %>%
    mutate(outcome=ifelse(is.na(outcome),0,as.integer(outcome=='event'))) %>%
    pull(outcome)
  ,timing=
    readRDS('data/outcome.rds') %>%
    filter(subject_id==app$subject_id) %>%
    mutate(timing=as.duration(latest_date-app$prediction_date)/ddays(1)) %>%
    mutate(timing=round(timing/7)) %>%
    pull(timing)
  )
```

```{r Processing codes at the server side, include=FALSE}
if(run_heavy_computation){
  cat('Start',as.character(now()),'\n')
  
  # Create null input if no positive features are found.
  app$input0=
    readRDS('data/features.rds') %>%
    mutate(I1=NA) %>%
    select(-regex) %>%
    column_to_rownames(var='features')
  
  if(nrow(app$raw_visit)==0){
    # Use the null input if no visit.
    app$input=app$input0
  }else{
    # Filter visit up to the current date (the prediction date).
    app$visit_up_to_current=
      app$raw_visit %>%
      filter(admission_date<=app$prediction_date)
    
    if(nrow(app$visit_up_to_current)==0){
      # Use the null input if no visit up to the current date.
      app$input=app$input0
    }else{
      # Only use the latest encounter for each code.
      app$visit_latest_enc=
        app$visit_up_to_current %>%
        group_by(code) %>%
        filter(admission_date==max(admission_date)) %>%
        ungroup()
      
      # Only use the codes for the features.
      app$visit_with_features=
        app$visit_latest_enc %>%
        filter(
          code
          %in%(
            readRDS('data/features.rds')$features %>%
              .[!str_detect(.,'causal_')]
          )
          |
          str_detect(
            code
            ,paste0(
              readRDS('data/features.rds')$regex %>%
                .[str_detect(readRDS('data/features.rds')$features,'causal_')]
              ,collapse='|'
            )
          )
        )
      
      if(nrow(app$visit_with_features)==0){
        # Use the null input if no positive features are found.
        app$input=app$input0
      }else{
        # Create the input dataframe with the positive features.
        app$input=
          readRDS('data/features.rds') %>%
          sapply(X=seq(nrow(.)),Y=.,Z=app$visit_with_features,function(X,Y,Z){
            if(str_detect(Y$features[X],'causal_')){
              K=Z %>%
                filter(str_detect(str_to_upper(code),Y$regex[X]))
            }else{
              K=Z %>%
                filter(code==Y$regex[X])
            }
            if(nrow(K)==0){
              NA
            }else{
              as.duration(app$prediction_date-max(K$admission_date))/ddays(1)
            }
          }) %>%
          setNames(readRDS('data/features.rds')$features) %>%
          data.frame(I1=.)
      }
    }
  }
  
  # Transform into historical rates.
  app$hlin=
    ExpressionSet(
      assayData=
        app$input %>%
        as.matrix()
      ,phenoData=
        data.frame(outcome=NA,row.names='I1') %>%
        AnnotatedDataFrame()
      ,featureData=
        readRDS('data/features.rds') %>%
        column_to_rownames(var='features') %>%
        AnnotatedDataFrame()
    ) %>%
    trans_hist_rate(
      hist_rate=readRDS('data/hist_rates.rds')
      ,interpolation='linear'
      ,verbose=T
    )
  
  # Transform into PCs.
  app$pc=
    app$hlin %>%
    transformation(
      rsdr_object=readRDS('data/pw_int_rsdr.rds')
      ,dimensions=as.integer(str_remove_all(readRDS('data/pcs.rds'),'[:alpha:]'))
      ,output_dim=readRDS('data/pcs.rds')
      ,verbose=T
    )
  
  # Estimate the time of delivery.
  app$estimation=
    app$pc %>%
    exprs() %>%
    t() %>%
    as.data.frame() %>%
    predict(readRDS('data/small_timing_pc_rf.rds'),newdata=.)
  
  # Compile a tidy set.
  app$tidyset=
    test_transformer(
      test_data=app$hlin
      ,SGD1bit_fit=readRDS('data/input.rds')$fit
      ,similarity=readRDS('data/input.rds')$similarity
      ,mapping=readRDS('data/input.rds')$mapping
      ,ontology=readRDS('data/input.rds')$ontology
      ,ranked=T
      ,dims=7
      ,decreasing=F
      ,seed_num=33
    ) %>%
    `colnames<-`('I1')
  
  # Save a single tidy set 
  # for reducing time of the compiling process.
  saveRDS(app$tidyset,'data/single_tidyset.rds')
  
  # Calling DI-VNN.
  cat('Calling DI-VNN\n')
  refresh_session()
  app$ontonet=
    readRDS('data/output.rds') %>%
    generator.ontonet(l2_norm=0.1) %>%
    load_model_weights_hdf5('data/ontonet.h5') %>%
    compile(
      optimizer=optimizer_sgd(
        lr=readRDS('data/modeling_divnn.rds')$history$metrics$lr %>%
          .[which.max(
            readRDS('data/modeling_divnn.rds')$history$metrics$val_root_roc
          )]
        ,momentum=0.9
      )
      ,loss='mean_squared_error'
      ,loss_weights=c(rep(
          0.3/(0.3*(length(.$outputs)-1)+1),length(.$outputs)-1)
          ,1/(0.3*(length(.$outputs)-1)+1)
        )
      ,metrics=c(
          tf$keras$metrics$AUC(name='roc')
          ,tf$keras$metrics$TruePositives(name='tp')
          ,tf$keras$metrics$FalseNegatives(name='fn')
          ,tf$keras$metrics$FalsePositives(name='fp')
          ,tf$keras$metrics$TrueNegatives(name='tn')
        )
    )
  
  # Save the ontonet architecture, weights, and optimizer.
  save_model_hdf5(app$ontonet,'data/entire_ontonet.h5')
  
  # Compute prediction.
  cat('Compute prediction\n')
  app$outputs=
    app$ontonet %>%
    predict_on_batch(
      generator.ontoarray(
        app$tidyset
        ,seq(ncol(app$tidyset))
        ,batch_size=1
      )()[[1]]
    ) %>%
    lapply(as.array) %>%
    setNames(
      readRDS('data/modeling_divnn.rds')$history$metrics %>%
        names() %>%
        .[str_detect(.,'val_')
          &str_detect(.,'_loss')
          &str_detect(.,'ONT|root')] %>%
        str_remove_all('val_|_loss')
    ) %>%
    as.data.frame()
  
  # Calibrate prediction.
  cat('Calibrate prediction\n')
  app$prediction=
    app$outputs %>%
    select(root) %>%
    rename(event=root) %>%
    mutate(nonevent=event) %>%
    select(nonevent,everything()) %>%
    `rownames<-`(colnames(app$tidyset)) %>%
    cbind(
      suppressWarnings(
          predict(readRDS('data/small_calib_divnn.rds'),newdata=.,type='prob')
        ) %>%
        rename_all(function(x)paste0(x,'2'))
    ) %>%
    select(-nonevent,-event) %>%
    rename_all(function(x)str_remove_all(x,'2')) %>%
    select(event) %>%
    `rownames<-`(colnames(app$tidyset)) %>%
    pull(event)
  
  # Retrieve internal properties.
  cat('Retrieve internal properties\n')
  app$ontoarray=
    app$tidyset %>%
    ontoarray1(app$ontonet,1)
  
  cat('End',as.character(now()),'\n')
  
  saveRDS(app,'data/app.rds')
}else{
  cat(readRDS('data/log.rds')[['app']])
  app=readRDS('data/app.rds')
  
  # Calling DI-VNN.
  cat('Calling DI-VNN\n')
  refresh_session()
  app$ontonet=
    readRDS('data/output.rds') %>%
    generator.ontonet(l2_norm=0.1) %>%
    load_model_weights_hdf5('data/ontonet.h5') %>%
    compile(
      optimizer=optimizer_sgd(
        lr=readRDS('data/modeling_divnn.rds')$history$metrics$lr %>%
          .[which.max(
            readRDS('data/modeling_divnn.rds')$history$metrics$val_root_roc
          )]
        ,momentum=0.9
      )
      ,loss='mean_squared_error'
      ,loss_weights=c(rep(
          0.3/(0.3*(length(.$outputs)-1)+1),length(.$outputs)-1)
          ,1/(0.3*(length(.$outputs)-1)+1)
        )
      ,metrics=c(
          tf$keras$metrics$AUC(name='roc')
          ,tf$keras$metrics$TruePositives(name='tp')
          ,tf$keras$metrics$FalseNegatives(name='fn')
          ,tf$keras$metrics$FalsePositives(name='fp')
          ,tf$keras$metrics$TrueNegatives(name='tn')
        )
    )
}
```

```{r Interval estimates of inference times, eval=FALSE, include=FALSE}
# Record inference times.
c('start','end'
  ,'2021-03-27 08:34:49','2021-03-27 08:39:57'
  ,'2021-03-27 08:43:37','2021-03-27 08:48:47'
  ,'2021-03-27 08:49:19','2021-03-27 08:54:26'
  ,'2021-03-27 09:01:53','2021-03-27 09:07:06'
  ,'2021-03-27 09:07:29','2021-03-27 09:12:32'
  ,'2021-03-27 09:13:02','2021-03-27 09:18:13'
  ,'2021-03-27 09:26:32','2021-03-27 09:31:40'
  ,'2021-03-27 09:34:17','2021-03-27 09:39:22'
  ,'2021-03-27 09:39:50','2021-03-27 09:45:02'
  ,'2021-03-27 09:45:37','2021-03-27 09:50:46') %>%
  
  # Create the dataframe.
  matrix(ncol=2,byrow=T) %>%
  `colnames<-`(.[1,]) %>%
  .[-1,] %>%
  as.data.frame() %>%
  
  # Create the report.
  mutate(
    subject_id=app$subject_id
    ,healthcare_id=app$healthcare_id
  ) %>%
  mutate_at(c('start','end'),as_datetime) %>%
  mutate(duration=as.duration(end-start)/dminutes(1)) %>%
  summarize(
    avg=mean(duration)
    ,lb=mean(duration)-qnorm(0.975)*sd(duration)/sqrt(n())
    ,ub=mean(duration)+qnorm(0.975)*sd(duration)/sqrt(n())
  ) %>%
  mutate_all(round,2)
```

```{r Post-processing codes at the server side, include=FALSE}
# Create an empty list to save the results.
results=list()

# Since the prediction is positive, 
# use this precise threshold (the best positive predictive value).
app$threshold=0.67

# Construct estimation table based on the prediction.
estimation=
  cbind(
    readRDS('data/eval_timing_pc_rf.rds')$timing_calib %>%
      select(timing,outcome,timing_pred)
    ,readRDS('data/eval_timing_divnn.rds')$timing_calib %>%
      select(pred)
    ,readRDS('data/prediction.rds') %>%
      mutate(event=as.integer(event>app$threshold)) %>%
      mutate(
        event=
          ifelse(event==1,'event','nonevent') %>%
          factor(c('nonevent','event'))
      ) %>%
      `rownames<-`(colnames(readRDS('data/test_data.rds')$calib)) %>%
      rename(calib_pred=event)
  ) %>%
  select_at(colnames(readRDS('data/eval_timing_pc_rf.rds')$timing_calib)) %>%
  select(calib_pred,timing_pred,timing) %>%
  filter(timing_pred>0) %>%
  mutate(timing_pred=round(timing_pred/7)) %>%
  group_by(calib_pred,timing_pred) %>%
  summarize(
    avg=mean(timing/7)
    ,lb=mean(timing/7)-qnorm(0.975)*sd(timing/7)/sqrt(n())
    ,ub=mean(timing/7)+qnorm(0.975)*sd(timing/7)/sqrt(n())
  )

# Get population-level estimate using calibration split 
# in which the subjects have the same prediction and estimated week.
app$est_timing=
  estimation %>%
  filter(
    calib_pred==ifelse(app$prediction>app$threshold,'event','nonevent')
    & timing_pred==round(app$estimation/7)
  )

# Manually inspect the prediction output.
readRDS('data/prediction.rds') %>%
  mutate(event=as.integer(event>app$threshold))

# Create a dataframe for the result conclusion.
results$main=
  c(prediction=ifelse(app$prediction>app$threshold,'event','nonevent')
  ,estimation=round(app$estimation/7)) %>%
  data.frame(result=.)
```

```{r Show prediction results of the input example, eval=FALSE, include=FALSE}
results$main %>%
  kable() %>%
  kable_classic()
```

```{r Draw the timeline of the input example, include=FALSE}
# Construct the timeline table
results$timeline_data=
  app$input %>%
  filter(!is.na(I1)) %>%
  rownames_to_column(var='feature') %>%
  rename(days_to_current=I1) %>%
  mutate(admission_date=app$prediction_date-days_to_current) %>%
  inner_join(
    app$ontoarray$representation %>%
      app$ontoarray$ontotable()
    ,by='feature'
  ) %>%
  arrange(fill) %>%
  mutate(level=seq(nrow(.))) %>%
  select(admission_date,fill,level,ontology,feature)

# Plot the timeline.
results$timeline=
  
  # Plot by the encounter date and the rank of values.
  results$timeline_data %>%
  ggplot(aes(admission_date,level),data=.) +
  
  # Give larger space for plotting.
  geom_blank(
    data=
      data.frame(
        admission_date=
          c(app$input %>%
              filter(!is.na(I1)) %>%
              rownames_to_column(var='feature') %>%
              rename(days_to_current=I1) %>%
              mutate(admission_date=app$prediction_date-days_to_current-56) %>%
              inner_join(
                app$ontoarray$representation %>%
                  app$ontoarray$ontotable()
                ,by='feature'
              ) %>%
              pull(admission_date) %>%
              min()
            ,app$prediction_date+round(app$est_timing$ub*7)+3)
        ,level=0
      )
  ) +
  
  # Shade period of the population-level, estimated time of delivery.
  geom_rect(
    data=
      app$est_timing %>%
      mutate(
        admission_date=app$prediction_date
        ,level=
          app$input %>%
          filter(!is.na(I1)) %>%
          rownames_to_column(var='feature') %>%
          rename(days_to_current=I1) %>%
          mutate(admission_date=app$prediction_date-days_to_current) %>%
          inner_join(
            app$ontoarray$representation %>%
              app$ontoarray$ontotable()
            ,by='feature'
          ) %>%
          arrange(abs(fill)) %>%
          mutate(level=seq(nrow(.))) %>%
          pull(level) %>%
          abs() %>%
          max()
      )
    ,aes(
      xmin=admission_date+round(lb*7)
      ,xmax=admission_date+round(ub*7)
      ,ymin=0
      ,ymax=level
    )
    ,fill=ifelse(app$prediction>app$threshold,'#79AF97FF','#B24745FF')
    ,alpha=0.5,lty=2,na.rm=T
  ) +
  
  # Plot the medical histories as label geometry.
  geom_label(
    aes(
      label=paste0(ontology,':',feature)
      ,fill=ifelse(fill>=0,'Positive','Negative')
    )
    ,family='sans'
    ,label.padding=unit(0.1,'lines')
    ,hjust=1.2
    ,vjust=0.5
    ,size=3
    ,alpha=0.5
    ,show.legend=F
  ) +
  geom_text(
    label='\u25BA'
    ,aes(color=ifelse(fill>=0,'Positive','Negative'))
    ,size=3,family='sans'
  ) +
  
  # Draw a line for the prediction date.
  geom_vline(xintercept=app$prediction_date,lty=2) +
  
  # Annotate the prediction date.
  annotate(
    geom='text'
    ,x=app$prediction_date,y=0
    ,label=
      paste0(
        'Time of prediction: '
        ,app$prediction_date
      )
    ,hjust=0,vjust=1.1,angle=90,size=3,family='sans'
  ) +
  
  # Draw a line for the estimated date of delivery.
  geom_vline(xintercept=app$prediction_date+round(app$estimation),lty=2) +
  
  # Annotate the predicted time of delivery and the prediction.
  annotate(
    geom='text'
    ,x=app$prediction_date+round(app$estimation),y=0
    ,label=
      paste0(
        'Predicted time of delivery: '
        ,app$prediction_date+round(app$estimation)
        ,'\n'
        ,'Prediction: '
        ,ifelse(
          app$prediction>app$threshold
          ,'Prelabor rupture of membranes (PROM)'
          ,'NOT Prelabor rupture of membranes (PROM)'
        )
      )
    ,hjust=0,vjust=1.1,angle=90,size=3,family='sans'
  ) +
  
  # Customize plot.
  theme_blank() +
  theme(
    axis.line.x=element_line()
    ,axis.ticks.x=element_line()
    ,axis.text.x=element_text(size=8,family='sans')
    ,legend.title=element_text(size=10,family='sans')
    ,legend.text=element_text(family='sans')
    ,legend.position='top'
  ) +
  scale_x_date(
    date_breaks='1 month'
    ,date_minor_breaks='1 week'
    ,date_labels='%b'
  ) +
  scale_color_manual('Output',values=c('#B24745FF','#79AF97FF')) +
  scale_fill_manual('Output',values=c('#B24745FF','#79AF97FF')) +
  theme(title=element_text(size=unit(8,'pt'),face='bold',family='sans')) +
  ggtitle('Timeline')
```

```{r Show the timeline, eval=FALSE, include=FALSE}
results$timeline
```

```{r Show the timetable, eval=FALSE, include=FALSE}
app$ontoarray$representation %>%
  app$ontoarray$ontotable() %>%
  filter(!is.na(feature)) %>%
  select(-max_fill,-z_lab) %>%
  rename(output=fill) %>%
  left_join(
    pw_hlin %>%
      fData() %>%
      rownames_to_column(var='feature') %>%
      rename(description=desc)
    ,by='feature'
  ) %>%
  inner_join(
    app$input %>%
      filter(!is.na(I1)) %>%
      rownames_to_column(var='feature') %>%
      mutate(admission_date=app$prediction_date-I1)
    ,by='feature'
  ) %>%
  select(admission_date,ontology,feature,description,output) %>%
  arrange(desc(admission_date),output) %>%
  kable() %>%
  kable_classic()
```

```{r Create individual ontology network and array, include=FALSE}
source('R/ontoproperties1-function.R')
```

```{r Ontology network and array for ONT169, eval=FALSE, include=FALSE}
ontoproperties1('169',pointer_stroke=2,pointer_size=2.5)
```

```{r Ontology network and array for root, eval=FALSE, include=FALSE}
ontoproperties1('root',pointer_stroke=2,pointer_size=2.5)
```

```{r Population-level performances of the input example, include=FALSE}
# Construct a dataframe containing population-level evaluation metrics.
results$pop_est_data=
  
  # Combine ROC and estimation table.
  rbind(
    readRDS('data/classification.rds') %>%
      filter(th==app$threshold)
    ,estimation %>%
      filter(
        calib_pred==ifelse(app$prediction>app$threshold,'event','nonevent')
        & timing_pred==round(app$estimation/7)
      ) %>%
      rename(th=calib_pred,metric=timing_pred) %>%
      mutate(th=app$threshold,metric='timing')
  ) %>%
  select(-th) %>%
  
  # Wrap up.
  mutate(
    avg=round(avg,ifelse(metric=='timing',0,3))
    ,lb=round(lb,ifelse(metric=='timing',0,3))
    ,ub=round(ub,ifelse(metric=='timing',0,3))
    ,metric=ifelse(metric=='timing',str_to_title(metric),str_to_upper(metric))
    ,metric=ifelse(metric=='TPR','Sensitivity',metric)
    ,metric=ifelse(metric=='TNR','Specificity',metric)
    ,metric=ifelse(metric=='PPV','Positive Predictive Value',metric)
    ,metric=ifelse(metric=='NPV','Negative Predictive Value',metric)
  ) %>%
  mutate(
    value=
      paste0(
        formatC(avg,decimal.mark='.')
        ,' (95% CI '
        ,formatC(lb,decimal.mark='.')
        ,' to '
        ,formatC(ub,decimal.mark='.')
        ,')'
      )
  ) %>%
  select(-lb,-avg,-ub) %>%
  mutate_all(as.character) %>%
  rename_all(str_to_title) %>%
  
  # Create a table for plotting the previous table.
  rbind(
    data.frame(colnames=colnames(.)) %>%
      mutate(value=colnames) %>%
      spread(colnames,value)
  ) %>%
  .[c(nrow(.),1:(nrow(.)-1)),] %>%
  mutate(row=seq(nrow(.))) %>%
  setNames(c(seq(ncol(.))[1:(ncol(.)-1)],'row')) %>%
  gather(column,value,-row) %>%
  mutate_at('column',as.integer) %>%
  mutate(column=ifelse(column>1,column+0.1,column))

# Transform the table as a plot.
results$pop_est=
  results$pop_est_data %>%
  ggplot(aes(x=column,y=row)) +
  geom_blank(data=data.frame(column=3.25,row=7)) +
  geom_hline(yintercept=c(0.8,1.8,7)) +
  geom_text(aes(label=value),hjust=0,vjust=1,size=3,family='sans') +
  scale_y_reverse() +
  theme_blank() +
  theme(
    title=element_text(size=unit(8,'pt'),face='bold',family='sans')
  ) +
  ggtitle('Population-level performances')
```

```{r Show the population-level performances, eval=FALSE, include=FALSE}
results$pop_est
```

```{r Save identity and results of example, include=FALSE}
results$identity_results=
  data.frame(
    row=1:6
    ,column=rep(1,6)
    ,text=
      c(paste0('Name: ','____________________________',' Age: _________')
        ,paste0(
          'Input: '
          ,app$raw_visit %>%
            filter(admission_date<=app$prediction_date) %>%
            pull(admission_date) %>%
            unique() %>%
            length()
          ,' visits consisting '
          ,app$raw_visit %>%
            filter(admission_date<=app$prediction_date) %>%
            nrow()
          ,' code entries'
        )
        ,paste0(
          'Time of prediction: '
          ,app$prediction_date
        )
        ,paste0(
          'Estimated time of delivery: '
          ,app$prediction_date+round(app$estimation)
        )
        ,paste0(
          'Predicted outcome: '
          ,ifelse(
            app$prediction>app$threshold
            ,'Prelabor rupture of membranes (PROM)'
            ,'NOT Prelabor rupture of membranes (PROM)'
          )
        )
        ,paste0(
          'Predicted probability: '
          ,formatC(round(app$prediction,3),decimal.mark='.')
          ,' (threshold='
          ,formatC(app$threshold,decimal.mark='.')
          ,')'
        )
      )
  )
```

```{r Show DI-VNN ROC curve, eval=FALSE, include=FALSE}
readRDS('data/classification.rds') %>%
  
  # Stack and spread the ROC table.
  gather(bound,value,-th,-metric) %>%
  spread(metric,value) %>%
  
  # Get the averages.
  filter(bound=='avg') %>%
  select(-bound) %>%
  
  # Draw the ROC curve.
  ggplot(aes(tnr,tpr),data=.) +
  geom_line(size=1.5,color='#4DBBD5FF') +
  geom_abline(intercept=1,slope=1,lty=2) +
  geom_blank(data=data.frame(tnr=-1,tpr=1,model=NA)) +
  
  # Add projecting lines.
  geom_segment(
    data=
      readRDS('data/classification.rds') %>%
      filter(th==app$threshold) %>%
      gather(bound,value,-th,-metric) %>%
      spread(metric,value) %>%
      filter(bound=='avg') %>%
      select(-bound)
    ,aes(x=tnr,xend=1,y=tpr,yend=tpr)
    ,lty=2,size=1,color='#4DBBD5FF'
  ) +
  geom_segment(
    data=
      readRDS('data/classification.rds') %>%
      filter(th==app$threshold) %>%
      gather(bound,value,-th,-metric) %>%
      spread(metric,value) %>%
      filter(bound=='avg') %>%
      select(-bound)
    ,aes(x=tnr,xend=tnr,y=0,yend=tpr)
    ,lty=2,size=1,color='#4DBBD5FF'
  ) +
  
  # Plot the predictive performances at the chosen threshold.
  geom_point(
    data=
      readRDS('data/classification.rds') %>%
      filter(th==app$threshold) %>%
      gather(bound,value,-th,-metric) %>%
      spread(metric,value) %>%
      filter(bound=='avg') %>%
      select(-bound)
    ,size=4,color='#4DBBD5FF'
  ) +
  
  # Customize plot.
  annotate(
    geom='label'
    ,x=0.4,y=0.1,label='DI-VNN'
    ,hjust=0,vjust=1,size=4,fill='#4DBBD5FF',alpha=0.75
  ) +
  coord_equal() +
  scale_x_reverse('Specificity',limits=1:0,breaks=rev(seq(0,1,0.1))) +
  scale_y_continuous('Sensitivity',limits=0:1,breaks=seq(0,1,0.1)) +
  guides(color=F,fill=F) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
```

```{r Show PC-RF estimation, eval=FALSE, include=FALSE}
estimation %>%
  # Identify the estimation that falls into the true interval.
  mutate(
    fall_into_interval=
      timing_pred>=lb & timing_pred<=ub & abs(ub-lb)<=timing_pred
      & lb<=42 & ub<=42
    ,fall_into_interval=
      ifelse(is.na(fall_into_interval),FALSE,fall_into_interval)
    ,fall_into_interval=
      ifelse(fall_into_interval,'Yes','No')
  ) %>%
  
  # Define boundaries of the optimum estimation period.
  mutate(
    timing_true=
      ifelse(
        calib_pred=='event'
        ,ifelse(timing_pred>=6 & timing_pred<=22,avg,NA)
        ,ifelse(timing_pred>=9 & timing_pred<=34,avg,NA)
      )
    ,pred_optmin=ifelse(calib_pred=='event',6,9)
    ,pred_optmax=ifelse(calib_pred=='event',22,34)
  ) %>%
  
  # Compute RMSE within the boundaries for each prediction.
  group_by(calib_pred) %>%
  mutate(RMSE=sqrt(mean((timing_pred-timing_true)^2,na.rm=T))) %>%
  ungroup() %>%
  
  # Draw the estimation plot.
  ggplot(aes(timing_pred,avg),data=.) +
  geom_point(aes(color=fall_into_interval)) +
  geom_linerange(
    aes(ymin=lb,ymax=ub,color=fall_into_interval)
    ,size=1
  ) +
  geom_abline(slope=1,intercept=0,lty=2) +
  
  # Draw the boundaries and the RMSEs.
  geom_vline(
    aes(
      xintercept=
        ifelse(
          calib_pred==ifelse(app$prediction>app$threshold,'event','nonevent')
          ,round(app$estimation/7)
          ,NA
        )
    )
    ,na.rm=T
  ) +
  geom_errorbarh(aes(xmin=pred_optmin,xmax=pred_optmax,y=2),size=1) +
  geom_label(
    y=2
    ,aes(
      x=pred_optmin+(pred_optmax-pred_optmin)/2
      ,label=paste0('RMSE Â± ',round(RMSE,1), ' weeks')
    )
    ,size=unit(3,'pt')
  ) +
  geom_label(
    y=42,hjust=0.5,vjust=1,alpha=0.75
    ,aes(
      x=ifelse(timing_pred==0,pred_optmin+(pred_optmax-pred_optmin)/2,NA)
      ,label=str_to_title(calib_pred),fill=calib_pred
    )
    ,show.legend=F
    ,na.rm=T
  ) +
  
  
  # Customize plot.
  facet_grid(~str_to_title(calib_pred)) +
  scale_x_continuous(limits=c(0,42),breaks=seq(0,42,2)) +
  scale_y_continuous(limits=c(0,42),breaks=seq(0,42,2)) +
  labs(
    x='Predicted time of delivery'
    ,y='True time of delivery (95% CI)'
    ,color='Fulfilling the criteria in main text?'
  ) +
  scale_color_manual(values=c('#7E6148FF','#00A087FF')) +
  scale_fill_manual(values=c('#E64B35FF','#3C5488FF')) +
  theme(
    axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)
    ,strip.text=element_blank()
    ,strip.background=element_blank()
    ,legend.position='top'
  )
```

# Results

In the R Markdown, we show data for Figure 1 in the main text. Up to the latest 
date for uncensored outcome and after splitting if >1 pregnancies, the total 
visit was the sum of totals from all subsets, while the total subject was the 
sum of totals from all subsets with attention for the overlaps (d to j in 
footnote of data for Figure 1 in the R Markdown). For association tests, visits 
and subjects of the censored outcome were those after external random splitting 
(k and j in footnote of data for Figure 1 in the R Markdown). We also 
explicitly show PROM prevalence for each subset.

```{r Prepare data for Figure 1 of the main text, include=FALSE}
# Create an empty list to save the results.
ovl=list()

# Instance summary by pregnancy status
ovl$pregnancy=
  readRDS('data/pregnancy_status.rds') %>%
  select(subject_id,gestation) %>%
  filter(!duplicated(.)) %>%
  group_by(gestation) %>%
  summarize(n=n(),.groups='drop') %>%
  mutate(p=n/sum(n)) %>%
  left_join(
    readRDS('data/pregnancy_status.rds') %>%
      separate(subject_id,c('subject_id','gestation_n'),sep='\\.') %>%
      select(subject_id,gestation) %>%
      filter(!duplicated(.)) %>%
      group_by(gestation) %>%
      summarize(n2=n(),.groups='drop') %>%
      mutate(p2=n2/sum(n2))
    ,by='gestation'
  )

# Subjects in re-calibration set
ovl$a=
  pw_hlin %>%
  .[,!pData(phenoData(.))$censoring] %>%
  .[,pData(protocolData(.))$int] %>%
  .[,colnames(.)%in%training_parameters$pre_calib_set] %>%
  protocolData() %>%
  pData() %>%
  pull(subject_id)

# Subjects in calibration set
ovl$b=
  pw_hlin %>%
  .[,!pData(phenoData(.))$censoring] %>%
  .[,pData(protocolData(.))$int] %>%
  .[,!colnames(.)%in%training_parameters$pre_calib_set] %>%
  protocolData() %>%
  pData() %>%
  pull(subject_id)

# Subjects in random set
ovl$c=
  pw_hlin %>%
  .[,!pData(phenoData(.))$censoring] %>%
  .[,pData(protocolData(.))$ran] %>%
  protocolData() %>%
  pData() %>%
  pull(subject_id)

# Create a function to overlap among three groups.
ovl$func=function(a,b,c,na=NULL,nb=NULL,nc=NULL){
  abc=a %>% intersect(b) %>% intersect(c)
  ab=a %>% intersect(b) %>% setdiff(abc)
  bc=b %>% intersect(c) %>% setdiff(abc)
  ac=a %>% intersect(c) %>% setdiff(abc)
  a=a %>% setdiff(c(ab,abc,ac))
  b=b %>% setdiff(c(ab,abc,bc))
  c=c %>% setdiff(c(ac,abc,bc))
  output=
    c(abc=length(abc)
      ,ab=length(ab)
      ,bc=length(bc)
      ,ac=length(ac)
      ,a=length(a)
      ,b=length(b)
      ,c=length(c)) %>%
    setNames(
      c(paste(na,'\u2229',nb,'\u2229',nc)
         ,paste(na,'\u2229',nb,'-',na,'\u2229',nb,'\u2229',nc)
         ,paste(nb,'\u2229',nc,'-',na,'\u2229',nb,'\u2229',nc)
         ,paste(na,'\u2229',nc,'-',na,'\u2229',nb,'\u2229',nc)
         ,paste(na,'-',na,'\u222A',nb,'\u222A',nc)
         ,paste(nb,'-',na,'\u222A',nb,'\u222A',nc)
         ,paste(nc,'-',na,'\u222A',nb,'\u222A',nc)
         )
    )
  paste(names(output),'=',formatC(output,big.mark=','))
}

# Visit table of outcome and censoring summary
ovl$d=
  cf_nw_int_bin %>%
  phenoData() %>%
  pData() %>%
  select(outcome,censoring) %>%
  table(visits=.)

# Subject table of outcome and censoring summary
ovl$e=
  cf_nw_int_bin %>%
  phenoData() %>%
  pData() %>%
  select(outcome,censoring) %>%
  cbind(
    cf_nw_int_bin %>%
    protocolData() %>%
    pData() %>%
    select(subject_id)
  ) %>%
  filter(!duplicated(.)) %>%
  select(outcome,censoring) %>%
  table(subjects=.)

# Compile into one table.
ovl$f=
  
  # Step-by-step summary for visit/subject selection
  readRDS('data/selection.rds') %>%
  setNames(c('set','exc_v_total','v_total','exc_s_total','s_total')) %>%
  mutate(v_nonevent=NA,v_event=NA,s_nonevent=NA,s_event=NA,note=NA,p=NA) %>%
  select(
    set,v_nonevent,v_event,v_total,s_nonevent,s_event,s_total
    ,note,p,everything()
  ) %>%
  mutate(
    note=case_when(
      set=='up to the latest date for uncensored and split if >1 pregnancies'
      ~'a,b,c'
      ,TRUE~'NA'
    )
  ) %>%
  
  # Summary of the visits/subjects by subset
  rbind(
    
    # Visit summary
    pw_hlin %>%
      
      # Summarize censoring.
      .[,pData(phenoData(.))$censoring] %>%
      ncol() %>%
      `names<-`('censoring') %>%
      data.frame(total=.) %>%
      rownames_to_column(var='set') %>%
      mutate(nonevent=NA,event=NA) %>%
      select(set,nonevent,event,everything()) %>%
      
      # Summarize all subsets.
      rbind(
        pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int] %>%
          .[,colnames(.)%in%training_parameters$pre_calib_set] %>%
          sapply(X=1,Y=.,function(X,Y)table(Y$outcome)) %>%
          `colnames<-`('precalib') %>%
          cbind(sapply(test_data,function(x)table(x$outcome))) %>%
          t() %>%
          as.data.frame() %>%
          rownames_to_column(var='set') %>%
          mutate(total=nonevent+event)
      ) %>%
      setNames(c('set',paste0('v_',colnames(.)[-1]))) %>%
      
      # Subject summary
      left_join(
        pw_hlin %>%
          
          # Summarize censoring.
          .[,pData(phenoData(.))$censoring] %>%
          protocolData() %>%
          pData() %>%
          select(subject_id) %>%
          filter(!duplicated(.)) %>%
          nrow() %>%
          `names<-`('censoring') %>%
          data.frame(total=.) %>%
          rownames_to_column(var='set') %>%
          mutate(nonevent=NA,event=NA) %>%
          select(set,nonevent,event,everything()) %>%
          
          # Summarize all subsets.
          rbind(
            lapply(X=c('precalib','calib','ran','geo','tem','bgt')
                   ,Y=pw_hlin %>% .[,!pData(phenoData(.))$censoring]
                   ,function(X,Y){
                
                if(X=='precalib'){
                  Z=Y %>%
                    .[,pData(protocolData(.))$int] %>%
                    .[,colnames(.)%in%training_parameters$pre_calib_set]
                }else if(X=='calib'){
                  Z=Y %>%
                    .[,pData(protocolData(.))$int] %>%
                    .[,!colnames(.)%in%training_parameters$pre_calib_set]
                }else{
                  Z=Y %>%
                    .[,pData(protocolData(.))[[X]]]
                }
                Z %>%
                  lapply(X=1,Y=.,function(X,Y){
                    Y %>%
                      phenoData() %>%
                      pData() %>%
                      select(outcome) %>%
                      cbind(
                        Y %>%
                          protocolData() %>%
                          pData() %>%
                          select(subject_id)
                      )
                  }) %>%
                  .[[1]] %>%
                  filter(!duplicated(.)) %>%
                  pull(outcome) %>%
                  table(precalib=.) %>%
                  as.data.frame() %>%
                  spread(precalib,Freq) %>%
                  mutate(set=X) %>%
                  select(set,everything())
                
              }) %>%
              do.call(rbind,.) %>%
              mutate(total=nonevent+event)
          ) %>%
          setNames(c('set',paste0('s_',colnames(.)[-1])))
        ,by='set'
      ) %>%
      
      # Annotate each subset for the footnote.
      mutate(
        note=case_when(
          set=='precalib'~'d,e,h'
          ,set=='calib'~'d,f,i'
          ,set=='ran'~'d,g,j'
          ,TRUE~'NA'
        )
      ) %>%
      mutate(p=s_event/s_total) %>%
      mutate(exc_v_total=NA,exc_s_total=NA)
  )
```

```{r Show data for Figure 1 of the main text, eval=FALSE, include=FALSE}
ovl$f %>%
  
  # Clean up naming.
  mutate(
    set=ifelse(set=='precalib','internal pre-calibration split',set)
    ,set=ifelse(set=='calib','internal calibration split',set)
    ,set=ifelse(set=='ran','external random split',set)
    ,set=ifelse(set=='geo','external geographical split',set)
    ,set=ifelse(set=='tem','external temporal split',set)
    ,set=ifelse(set=='bgt','external geotemporal split',set)
  ) %>%
  mutate(set=str_to_sentence(set)) %>%
  setNames(c('Subset'
             ,'Nonevent visits','Event visits','Total visits'
             ,'Nonevent subjects','Event subjects','Total subjects'
             ,'Note','Prevalence','Excluded visits','Excluded subjects')) %>%
  
  # Show the table.
  kable(format.args=list(big.mark=',')) %>%
  kable_classic() %>%
  
  # Add the footnote.
  add_footnote(
    c(paste('Gestation'
            ,ovl$pregnancy$gestation
            ,'is',formatC(ovl$pregnancy$n,big.mark=','))
      ,ovl$func(ovl$a,ovl$b,ovl$c,'P','C','R')
      ,paste('Censored visits in causal inference are'
             ,formatC(sum(ovl$d[3:4]),big.mark=','))
      ,paste('Censored subjects in causal inference are'
             ,formatC(sum(ovl$e[3:4]),big.mark=',')))
  )
```

```{r The subject characteristics 1, eval=FALSE, include=FALSE}
char_sub1=
  cf_nw_int_bin %>%
  phenoData() %>%
  pData() %>%
  rownames_to_column(var='id') %>%
  left_join(
    infercause %>%
      protocolData() %>%
      pData() %>%
      rownames_to_column(var='id') %>%
      select(id,subject_id)
    ,by='id'
  ) %>%
  mutate(outcome=as.character(outcome)) %>%
  mutate(outcome=ifelse(censoring,'censored',outcome)) %>%
  group_by(subject_id) %>%
  arrange(desc(age)) %>%
  slice(1) %>%
  ungroup() %>%
  select(
    outcome
    ,gestation
    ,subject_id
    ,age
    ,marital_status
    ,insurance_class
    ,occupation_segment
  ) %>%
  mutate(gestation=as.integer(gestation>0)) %>%
  lapply(X=1,Y=.,function(X,Y){
    Z=data.frame(variable='age',term='y') %>%
      cbind(
        Y %>%
          mutate(group=paste0(outcome,gestation)) %>%
          group_by(group) %>%
          summarize(
            n=round(mean(age))
            ,p=round(sd(age))
            ,total=n()
          )
      )
    
    K=Y %>%
      filter(outcome!='censored') %>%
      select(outcome,age) %>%
      mutate(outcome=factor(outcome,c('nonevent','event'))) %>%
      glm(outcome~age,data=.,family=binomial(link='logit')) %>%
      tidy() %>%
      filter(term!='(Intercept)') %>%
      mutate(
        variable=term
        ,term='y'
        ,OR=exp(estimate)
        ,LB=exp(estimate-qnorm(0.975)*std.error)
        ,UB=exp(estimate+qnorm(0.975)*std.error)
      ) %>%
      select(variable,term,OR,LB,UB, p.value)
    
    L=Y  %>%
      mutate(group=paste0(outcome,gestation)) %>%
      select(-subject_id,-age,-gestation)
    L=suppressWarnings(gather(L,variable,term,-group)) %>%
      group_by(group,variable,term) %>%
      summarize(n=n()) %>%
      group_by(group,variable) %>%
      mutate(
        p=round(n/sum(n)*100,2)
        ,total=sum(n)
      ) %>%
      mutate(term=ifelse(is.na(term),'unspecified',term))
    
    M=Y %>%
      filter(outcome!='censored') %>%
      select(-subject_id,-age,-gestation) %>%
      gather(variable,value,-outcome) %>%
      mutate(
        value=
          ifelse(
            value%in%c('first','married','central-government-paid householder')
            ,paste0('0_',value)
            ,ifelse(is.na(value),'unspecified',value)
          )
      ) %>%
      group_by(variable)
    
    M=suppressWarnings(suppressMessages(
        M %>%
          mutate(outcome=factor(outcome,c('nonevent','event'))) %>%
          do(tidy(glm(outcome~value,data=.,family=binomial(link='logit'))))
      )) %>%
      mutate(
        term=
          ifelse(
            term=='(Intercept)'
            ,case_when(
              variable=='insurance_class'
              ~'first'
              ,variable=='marital_status'
               ~'married'
              ,variable=='occupation_segment'
               ~'central-government-paid householder'
              ,TRUE~'(Intercept)'
            )
            ,term
          )
      ) %>%
      mutate(
        term=str_remove_all(term,'value')
        ,OR=exp(estimate)
        ,LB=exp(estimate-qnorm(0.975)*std.error)
        ,UB=exp(estimate+qnorm(0.975)*std.error)
      ) %>%
      select(variable,term,OR,LB,UB, p.value)
    
    Z %>%
      left_join(K,by=c('variable','term')) %>%
      rbind(
        L %>%
        left_join(M,by=c('variable','term'))
      )
  }) %>%
  .[[1]] %>%
  pivot_wider(names_from='group',values_from=c('n','p','total')) %>%
  group_by(variable) %>%
  fill(
    n_censored0,n_censored1,n_nonevent1,n_event1
    ,p_censored0,p_censored1,p_nonevent1,p_event1
    ,total_censored0,total_censored1,total_nonevent1,total_event1
    ,.direction='updown'
  ) %>%
  filter(!(variable=='outcome' & term%in%c('nonevent','event'))) %>%
  mutate(
    term=ifelse(variable=='outcome','',term)
    ,total=total_censored0+total_censored1+total_nonevent1+total_event1
  ) %>%
  arrange(variable,term) %>%
  mutate(
    OR=
      ifelse(
        term%in%c('first','married','central-government-paid householder')
        ,NA
        ,OR
      )
    ,LB=ifelse(is.na(OR),NA,LB)
    ,UB=ifelse(is.na(OR),NA,UB)
  ) %>%
  mutate(
    term=
      ifelse(
        variable%in%c('age','outcome')
        ,ifelse(
          variable=='age'
          ,paste0('mean (SD), ',term)
          ,'no. (%)'
        )
        ,paste0(term,', no. (%)')
      )
    ,censored0=
      ifelse(
        variable=='age'
        ,paste0(
          format(n_censored0,digits=0,scientific=F)
          ,' ('
          ,format(p_censored0,digits=0,scientific=F)
          ,')'
        )
        ,ifelse(
          variable=='outcome'
          ,paste0(
            format(total_censored0,digits=0,big.mark=',',scientific=F)
            ,'/'
            ,format(
              total_censored0+total_censored1+total_nonevent1+total_event1
              ,digits=0
              ,big.mark=','
              ,scientific=F
            )
            ,' ('
            ,round(
              total_censored0
              /(total_censored0+total_censored1+total_nonevent1+total_event1)
              *100
              ,2
            )
            ,')'
          )
          ,paste0(
            format(n_censored0,digits=0,big.mark=',',scientific=F)
            ,' ('
            ,format(p_censored0,digits=2,scientific=F)
            ,')'
          )
        )
      )
    ,censored1=
      ifelse(
        variable=='age'
        ,paste0(
          format(n_censored1,digits=0,scientific=F)
          ,' ('
          ,format(p_censored1,digits=0,scientific=F)
          ,')'
        )
        ,ifelse(
          variable=='outcome'
          ,paste0(
            format(total_censored1,digits=0,big.mark=',',scientific=F)
            ,'/'
            ,format(
              total_censored0+total_censored1+total_nonevent1+total_event1
              ,digits=0
              ,big.mark=','
              ,scientific=F
            )
            ,' ('
            ,round(
              total_censored1
              /(total_censored0+total_censored1+total_nonevent1+total_event1)
              *100
              ,2
            )
            ,')'
          )
          ,paste0(
            format(n_censored1,digits=0,big.mark=',',scientific=F)
            ,' ('
            ,format(p_censored1,digits=2,scientific=F)
            ,')'
          )
        )
      )
    ,nonevent1=
      ifelse(
        variable=='age'
        ,paste0(
          format(n_nonevent1,digits=0,scientific=F)
          ,' ('
          ,format(p_nonevent1,digits=0,scientific=F)
          ,')'
        )
        ,ifelse(
          variable=='outcome'
          ,paste0(
            format(total_nonevent1,digits=0,big.mark=',',scientific=F)
            ,'/'
            ,format(
              total_nonevent1+total_event1
              ,digits=0
              ,big.mark=','
              ,scientific=F
            )
            ,' ('
            ,round(total_nonevent1/(total_nonevent1+total_event1)*100,2)
            ,')'
          )
          ,paste0(
            format(n_nonevent1,digits=0,big.mark=',',scientific=F)
            ,' ('
            ,format(p_nonevent1,digits=2,scientific=F)
            ,')'
          )
        )
      )
    ,event1=
      ifelse(
        variable=='age'
        ,paste0(
          format(n_event1,digits=0,scientific=F)
          ,' ('
          ,format(p_event1,digits=0,scientific=F)
          ,')'
        )
        ,ifelse(
          variable=='outcome'
          ,paste0(
            format(total_event1,digits=0,big.mark=',',scientific=F)
            ,'/'
            ,format(
              total_nonevent1+total_event1
              ,digits=0
              ,big.mark=','
              ,scientific=F
            )
            ,' ('
            ,round(total_event1/(total_nonevent1+total_event1)*100,2)
            ,')'
          )
          ,paste0(
            format(n_event1,digits=0,big.mark=',',scientific=F)
            ,' ('
            ,format(p_event1,digits=2,scientific=F)
            ,')'
          )
        )
      )
    ,unadjusted_OR=
      ifelse(
        is.na(OR)
        ,ifelse(variable=='outcome','','(reference)')
        ,paste0(
          round(OR,3)
          ,' ('
          ,round(LB,3)
          ,' to '
          ,round(UB,3)
          ,'; '
          ,case_when(
            p.value<0.001~'P<.001***'
            ,p.value<0.01
             ~paste0(
               'P='
               ,p.value %>%
                 round(3) %>%
                 as.character() %>%
                 str_replace_all('0\\.','.')
               ,'**'
              )
            ,p.value<0.05
             ~paste0(
               'P='
               ,p.value %>%
                 round(2) %>%
                 as.character() %>%
                 str_replace_all('0\\.','.')
               ,'*'
              )
            ,p.value<=0.99
             ~paste0(
               'P='
               ,p.value %>%
                 round(2) %>%
                 as.character() %>%
                 str_replace_all('0\\.','.')
              )
            ,TRUE~'p>.99'
          )
          ,')'
        )
      )
  ) %>%
  arrange(
    factor(
      variable
      ,c('outcome'
         ,'age'
         ,'insurance_class'
         ,'marital_status'
         ,'occupation_segment')
    )
    ,desc(unadjusted_OR=='(reference)')
    ,desc(OR)
  ) %>%
  group_by(variable) %>%
  mutate(variable=ifelse(seq(n())==1,variable,'')) %>%
  ungroup() %>%
  select(variable,term,censored0,censored1,nonevent1,event1,unadjusted_OR)
```

```{r The subject characteristics 2, eval=FALSE, include=FALSE}
cat('Extract baseline data of medical histories\n')
cat('Started:',as.character(now()),'\n')

nw_bin=
  ExpressionSet(
    assayData=
      rbind(
        exprs(infercause) %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,exprs(inferdata)
      )
    ,phenoData=phenoData(inferdata)
    ,featureData=
      rbind(
        fData(infercause) %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,fData(inferdata)
      ) %>%
      AnnotatedDataFrame(varMetadata=fvarMetadata(inferdata))
    ,experimentData=
      MIAME(
        name='Herdiantri Sufriyana'
        ,lab='Emily Chia-Yu Su Lab'
        ,contact='herdiantrisufriyana@unusa.ac.id'
        ,title='Medical history dataset for causal inference'
        ,abstract=
          str_replace_all(
            'This dataset is a medical-history table from the BPJS Kesehatan.
            Selected causal factors, that is re-assigned by regex, are added.
            The target population is health insurance holders of 
            12-to-55-years-old females who visit healthcare providers between 
            2015 and 2016. The outcome of interest is prelabor rupture of 
            membrane (PROM). The medical history scenario is recorded
            individually within each healthcare provider nationwide.'
            ,'\n',' '
          ) %>%
          str_replace_all('\\s+',' ')
        ,url='https://github.com/herdiantrisufriyana/prom'
      )
    ,annotation=annotation(inferdata)
    ,protocolData=protocolData(inferdata)
  ) %>%
  trans_binary()

nw_subject_bin=
  cbind(
    nw_bin %>%
      protocolData() %>%
      pData() %>%
      select(subject_id)
    ,nw_bin %>%
      phenoData() %>%
      pData() %>%
      select(outcome)
    ,nw_bin %>%
      exprs() %>%
      t() %>%
      as.data.frame()
  ) %>%
  group_by(subject_id,outcome) %>%
  summarize_all(function(x)as.integer(sum(x)>0)) %>%
  ungroup()

cat('End:',as.character(now()))

# Extract baseline data of medical histories
# Started: 2021-08-19 17:07:19 
# Convert the day intervals into the binarized ones
#   |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=12s  
# End: 2021-08-19 17:16:39

char_sub2=
  cf_nw_int_bin %>%
  phenoData() %>%
  pData() %>%
  rownames_to_column(var='id') %>%
  left_join(
    infercause %>%
      protocolData() %>%
      pData() %>%
      rownames_to_column(var='id') %>%
      select(id,subject_id)
    ,by='id'
  ) %>%
  mutate(outcome=as.character(outcome)) %>%
  mutate(outcome=ifelse(censoring,'censored',outcome)) %>%
  group_by(subject_id) %>%
  arrange(desc(age)) %>%
  slice(1) %>%
  ungroup() %>%
  select(subject_id,outcome) %>%
  left_join(select(nw_subject_bin,-outcome),by='subject_id') %>%
  separate(subject_id,c('subject_id','pregnancy'),sep='\\.') %>%
  mutate(gestation=as.integer(pregnancy>0)) %>%
  .[,c('outcome','gestation',paste0('causal_',colnames(dag$sig2)))] %>%
  lapply(X=1,Y=.,function(X,Y){
    Z=Y %>%
      unite(outcome,outcome,gestation,sep='') %>%
      mutate(
        outcome=factor(outcome,c('censored0','censored1','nonevent1','event1'))
      )
    
    K=dag$ipw2 %>%
      lapply(function(x){
        data.frame(
            OR=exp(x$marginal_effect)
            ,LB=exp(x$CI95_interval[1])
            ,UB=exp(x$CI95_interval[2])
            ,p.value=x$p_value
          ) %>%
          `rownames<-`(NULL)
      }) %>%
      do.call(rbind,.) %>%
      rownames_to_column(var='variable') %>%
      mutate(term='positive') %>%
      select(variable,term,OR,LB,UB, p.value) %>%
      mutate(variable=paste0('causal_',variable))

    Z=Z %>%
      group_by(outcome) %>%
      summarize_all(function(x)sum(x==0)) %>%
      gather(variable,negative,-outcome) %>%
      left_join(
        Z %>%
          group_by(outcome) %>%
          summarize_all(function(x)sum(x==1)) %>%
          gather(variable,positive,-outcome)
        ,by=c('outcome','variable')
      ) %>%
      gather(term,n,-outcome,-variable) %>%
      group_by(outcome,variable) %>%
      mutate(
        p=n/sum(n)
        ,total=sum(n)
      ) %>%
      ungroup() %>%
      pivot_wider(names_from='outcome',values_from=c('n','p','total'))

    Z %>%
      left_join(K,by=c('variable','term')) %>%
      arrange(variable,term)
  }) %>%
  .[[1]] %>%
  mutate(
    term=
      ifelse(
        variable=='causal_A20'
        ,ifelse(
          term=='negative'
          ,'20 to 35 y'
          ,'<20 or >35 y'
        )
        ,term
      )
    ,term=paste0(term,', no. (%)')
    ,censored0=
      paste0(
        format(n_censored0,digits=0,big.mark=',',scientific=F)
        ,' ('
        ,format(p_censored0,digits=2,scientific=T)
        ,')'
      )
    ,censored1=
      paste0(
        format(n_censored1,digits=0,big.mark=',',scientific=F)
        ,' ('
        ,format(p_censored1,digits=2,scientific=T)
        ,')'
      )
    ,nonevent1=
      paste0(
        format(n_nonevent1,digits=0,big.mark=',',scientific=F)
        ,' ('
        ,format(p_nonevent1,digits=2,scientific=T)
        ,')'
      )
    ,event1=
      paste0(
        format(n_event1,digits=0,big.mark=',',scientific=F)
        ,' ('
        ,format(p_event1,digits=2,scientific=T)
        ,')'
      )
    ,unadjusted_OR=
      ifelse(
        is.na(OR)
        ,ifelse(variable=='outcome','','(reference)')
        ,paste0(
          round(OR,3)
          ,' ('
          ,round(LB,3)
          ,' to '
          ,round(UB,3)
          ,'; '
          ,case_when(
            p.value<0.001~'P<.001***'
            ,p.value<0.01
             ~paste0(
               'P='
               ,p.value %>%
                 round(3) %>%
                 as.character() %>%
                 str_replace_all('0\\.','.')
               ,'**'
              )
            ,p.value<0.05
             ~paste0(
               'P='
               ,p.value %>%
                 round(2) %>%
                 as.character() %>%
                 str_replace_all('0\\.','.')
               ,'*'
              )
            ,p.value<=0.99
             ~paste0(
               'P='
               ,p.value %>%
                 round(2) %>%
                 as.character() %>%
                 str_replace_all('0\\.','.')
              )
            ,TRUE~'p>.99'
          )
          ,')'
        )
      )
  ) %>%
  arrange(
    factor(
      variable
      ,c(paste0('causal_',colnames(dag$sig2)))
    )
    ,desc(unadjusted_OR=='(reference)')
    ,desc(OR)
  ) %>%
  left_join(
    dag$baseline_nodes %>%
      rename(variable=label,desc=name) %>%
      mutate(variable=paste0('causal_',variable)) %>%
      select(variable,desc)
    ,by='variable'
  ) %>%
  unite(variable,variable,desc,sep=': ') %>%
  group_by(variable) %>%
  mutate(variable=ifelse(seq(n())==1,variable,'')) %>%
  ungroup() %>%
  select(variable,term,censored0,censored1,nonevent1,event1,unadjusted_OR)
```

```{r The subject characteristics 3, eval=FALSE, include=FALSE}
char_sub3=
  cf_nw_int_bin %>%
  phenoData() %>%
  pData() %>%
  rownames_to_column(var='id') %>%
  left_join(
    infercause %>%
      protocolData() %>%
      pData() %>%
      rownames_to_column(var='id') %>%
      select(id,subject_id)
    ,by='id'
  ) %>%
  mutate(outcome=as.character(outcome)) %>%
  mutate(outcome=ifelse(censoring,'censored',outcome)) %>%
  group_by(subject_id) %>%
  arrange(desc(age)) %>%
  slice(1) %>%
  ungroup() %>%
  lapply(X=1,Y=.,function(X,Y){
    Z=Y %>%
      filter(gestation>=1) %>%
      mutate(gestation=factor(gestation,c(1,2))) %>%
      mutate(outcome=factor(outcome,c('nonevent','event'))) %>%
      glm(outcome~gestation,data=.,family=binomial(link='logit')) %>%
      tidy() %>%
      mutate(
        variable='pregnancy episode within the database period'
        ,estimate=ifelse(term=='(Intercept)',NA,estimate)
        ,term=ifelse(term=='(Intercept)','first pregnancy','second pregnancy')
        ,term=paste0(term,',c no. (%)')
        ,OR=exp(estimate)
        ,LB=exp(estimate-qnorm(0.975)*std.error)
        ,UB=exp(estimate+qnorm(0.975)*std.error)
      ) %>%
      mutate(
        unadjusted_OR=
          ifelse(
            is.na(OR)
            ,ifelse(variable=='outcome','','(reference)')
            ,paste0(
              round(OR,3)
              ,' ('
              ,round(LB,3)
              ,' to '
              ,round(UB,3)
              ,'; '
              ,case_when(
                p.value<0.001~'P<.001***'
                ,p.value<0.01
                 ~paste0(
                   'P='
                   ,p.value %>%
                     round(3) %>%
                     as.character() %>%
                     str_replace_all('0\\.','.')
                   ,'**'
                  )
                ,p.value<0.05
                 ~paste0(
                   'P='
                   ,p.value %>%
                     round(2) %>%
                     as.character() %>%
                     str_replace_all('0\\.','.')
                   ,'*'
                  )
                ,p.value<=0.99
                 ~paste0(
                   'P='
                   ,p.value %>%
                     round(2) %>%
                     as.character() %>%
                     str_replace_all('0\\.','.')
                  )
                ,TRUE~'P>.99'
              )
              ,')'
            )
          )
      ) %>%
      select(variable,term,unadjusted_OR)
    
    K=Y %>%
      mutate(
        gestation2=as.integer(gestation>0)
      ) %>%
      unite(outcome,outcome,gestation2,sep='') %>%
      select(gestation,outcome) %>%
      table() %>%
      as.data.frame() %>%
      group_by(outcome) %>%
      mutate(p=Freq/sum(Freq)) %>%
      mutate(
        Freq=format(Freq,digits=0,big.mark=',')
        ,p=paste0('(',round(p*100,2),')')
      ) %>%
      unite(summary,Freq,p,sep=' ') %>%
      spread(outcome,summary) %>%
      mutate(
        variable='pregnancy episode within the database period'
        ,term=
          case_when(
            gestation==0~'no pregnancy, no. (%)'
            ,gestation==1~'first pregnancy,c no. (%)'
            ,gestation==2~'second pregnancy,c no. (%)'
            ,TRUE~''
          )
      ) %>%
      select(-gestation) %>%
      select(variable,term,censored0,censored1,nonevent1,event1)
    
    K %>%
      left_join(Z,by=c('variable','term')) %>%
      mutate(unadjusted_OR=ifelse(is.na(unadjusted_OR),'N/A',unadjusted_OR))
  }) %>%
  .[[1]] %>%
  group_by(variable) %>%
  mutate(variable=ifelse(seq(n())==1,variable,'')) %>%
  ungroup()
```

```{r The subject_characteristics, eval=FALSE, include=FALSE}
char_sub3 %>%
  mutate(variable=ifelse(variable=='',NA,variable)) %>%
  fill(variable) %>%
  filter(term!='no pregnancy, no. (%)') %>%
  group_by(variable) %>%
  mutate(variable=ifelse(seq(n())==1,variable,'')) %>%
  ungroup() %>%
  rbind(char_sub1 %>% .[2:nrow(.),]) %>%
  rbind(char_sub2) %>%
  mutate(
    unadjusted_OR=
      ifelse(
        unadjusted_OR%in%c('-','(reference)')
        ,unadjusted_OR
        ,str_split_fixed(unadjusted_OR,'; ',2)[,2] %>%
          str_remove_all('\\)')
      )
  ) %>%
  mutate(
    variable=
      case_when(
        str_detect(variable,'causal_')
         ~str_split_fixed(variable,': ',2)[,2]
        ,str_detect(variable,'_')
         ~str_to_sentence(str_replace_all(variable,'_',' '))
        ,variable==''~''
        ,TRUE~str_to_sentence(variable)
      )
  ) %>%
  select(-censored0,-censored1) %>%
  setNames(
    c('Variable'
      ,''
      ,paste0(
        'Not PROMa (n='
        ,str_split_fixed(char_sub1$nonevent1,'/',2)[1,1]
        ,')'
      )
      ,paste0(
        'PROMa (n='
        ,str_split_fixed(char_sub1$event1,'/',2)[1,1]
        ,')'
      )
      ,'P value')
  ) %>%
  kable() %>%
  kable_classic() %>%
  
  # Add the footnote.
  add_footnote(
    c('Subject per pregnancy episode (not including censored delivery)'
      ,'Not PROM vs. PROM (not including those who were not pregnant)'
      ,paste0(
        'The first and second pregnancies of '
        ,'a subject within the database period'
      ))
    ,notation='alphabet'
  )
```

## Association diagram

For latent candidate predictors whose data were available, we created the 
association diagrams (eFigures 1 to 12 in Appendix). We excluded all common 
effects of PROM that we found during the systematic human learning. These are 
not needed for association tests. Instead, inclusion of these variables will 
cause collider-stratification bias. In the association diagrams, we apply 
different colors based on the types of nodes representing several factors: (1) 
type A is a first-level factor (with variable prefixed by A) that has a role as 
a confounder; (2) type I is a first-level factor (with variable also prefixed 
by A) that has a role as a candidate factor of interest; (3) type U is an 
unmeasured variable that can affect measure variable of type-A/I/Y variable; 
and (4) type Y is a target or dependent factor which is PROM. Node, of which 
variable denoted by asterisk, represents a type-A/I/Y variable that can be 
represented by several diagnosis or procedure codes. In each of the diagrams, 
we also show adjustment formulas which were used for association tests. 
Conceivably, all variables of type A/I/Y with asterisk may be included in each 
formula, but, some of these variables cannot be included because these were not 
available in our data, particularly in the training set (likely because of low 
prevalence). All measurement errors that may be affected by Type-U variables in 
this study were assumed as independent non-differential errors, because all 
data were measured from electronic medical records. As the main text, results 
of the association tests were also described (eTable 13 in Appendix), either by 
outcome regression or inverse probability weighting (IPW).

```{r Prepare causal inference table, include=FALSE}
cauinf_tab=
  # Get causal inference results.
  rbind(
    
    # Causal inference table by outcome regression
    dag$coef %>%
      lapply(X=seq(length(.)),Y=.,function(X,Y){
        Y[[X]] %>%
          filter(term!='(Intercept)') %>%
          slice(1) %>%
          mutate(
            code=term
            ,marginal_effect=OR
            ,lb=OR_lb
            ,ub=OR_ub
            ,p.value=p.value
          ) %>%
          select(code,marginal_effect,lb,ub,p.value)
      }) %>%
      do.call(rbind,.) %>%
      mutate(method='Outcome regression (95% CI; P value)')
    
    # Causal inference table by IPW
    ,dag$ipw %>%
      lapply(X=seq(length(.)),Y=.,function(X,Y){
        data.frame(
          code=names(Y)[X]
          ,marginal_effect=exp(Y[[X]]$marginal_effect)
          ,lb=exp(Y[[X]]$CI95_interval['LB'])
          ,ub=exp(Y[[X]]$CI95_interval['UB'])
          ,p.value=Y[[X]]$p_value
        ) %>%
          `rownames<-`(NULL)
      }) %>%
      do.call(rbind,.) %>%
      mutate(method='Inverse probability weighting (95% CI; P value)')
    
  ) %>%
  
  # Include the description of the causal factors.
  left_join(
    dag$baseline_nodes %>%
      rename(code=label) %>%
      mutate(factor=paste(code,name)) %>%
      select(factor,code)
    ,by='code'
  ) %>%
  select(-code) %>%
  
  # Wrap up.
  select(method,factor,everything()) %>%
  mutate_at(c('marginal_effect','lb','ub'),round,3) %>%
  mutate(
    effect=
      paste0(
        marginal_effect
        ,' ('
        ,lb
        ,' to '
        ,ub
        ,'; '
        ,case_when(
          p.value<0.001~'P<.001***'
          ,p.value<0.01
           ~paste0(
             'P='
             ,p.value %>%
               round(3) %>%
               as.character() %>%
               str_replace_all('0\\.','.')
             ,'**'
            )
          ,p.value<0.05
           ~paste0(
             'P='
             ,p.value %>%
               round(2) %>%
               as.character() %>%
               str_replace_all('0\\.','.')
             ,'*'
            )
          ,p.value<=0.99
           ~paste0(
             'P='
             ,p.value %>%
               round(2) %>%
               as.character() %>%
               str_replace_all('0\\.','.')
            )
          ,TRUE~'P>.99'
        )
        ,')'
      )
  ) %>%
  select(-marginal_effect,-lb,-ub,-p.value) %>%
  spread(method,effect) %>%
  select(factor,`Outcome regression (95% CI; P value)`,everything()) %>%
  mutate(factor=sapply(X=factor,function(X)str_sub(X,5,str_count(X)))) %>%
  rename(`Latent candidate predictors`=factor)
```

```{r Causal inference, eval=FALSE, include=FALSE}
cauinf_tab %>%
  kable() %>%
  kable_classic() %>%
  add_footnote(
    c('P<.05; ** P<.01; *** P<.001; CI, confidence interval')
    ,'symbol'
  )
```

```{r Save causal inference table, eval=FALSE, include=FALSE}
cauinf_tab %>%
  
  # Save for eTable 13.
  write_csv('data/supp_table_13.csv')
```

```{r Show causal inference table with unadjusted OR, eval=FALSE, include=FALSE}
char_sub2 %>%
  mutate(variable=ifelse(variable=='',NA,variable)) %>%
  fill(variable) %>%
  separate(variable,c('code','variable'),sep=': ') %>%
  filter(unadjusted_OR!='(reference)') %>%
  select(variable,unadjusted_OR) %>%
  left_join(
    cauinf_tab %>%
      select(-`Outcome regression (95% CI; P value)`) %>%
      setNames(c('variable','adjusted_OR'))
    ,by='variable'
  ) %>%
  mutate(
    adjustment=
      variable %>%
      sapply(function(x){
        y=dag$formula[[
            dag$baseline_nodes$label[dag$baseline_nodes$name==x]
          ]] %>%
          as.character() %>%
          .[3] %>%
          str_split(' \\+ ') %>%
          .[[1]]
        
        if(length(y)>1){
          y %>%
            .[2:length(.)] %>%
            data.frame(label=.) %>%
            left_join(dag$baseline_nodes,by='label') %>%
            pull(name) %>%
            paste0(collapse=' + ')
        }else{
          '(no adjustment)'
        }
      })
  ) %>%
  mutate(adjusted_OR=str_remove_all(adjusted_OR,'95% CI ')) %>%
  setNames(
    c('Variable of interest'
      ,'Unadjusted OR (95% CI; P value)'
      ,'Adjusted OR (95% CI; P value)'
      ,'Adjustment')
  ) %>%
  kable() %>%
  kable_classic() %>%
  
  # Add the footnote.
  add_footnote(
    c('P<.05; ** P<.01; *** P<.001; CI, confidence interval;')
    ,'symbol'
  )
```

We would describe each association diagram. Fifty-six studies were found from 
PubMed (eTable 11 in R Markdown) [...]. Since only some factors are possible to 
include in the association tests, only some of these studies were explained 
below. After verifying the assumption using our data, we constructed a final 
association diagram consisting all the confirmed latent candidate predictors.

### Multiple pregnancy

Association model of multiple pregnancy on PROM (ACOG, 2016a) included only 
maternal age as a confounder (Song J, et al 2019; Thilaganathan B, and 
Khalil A, 2014; Martin JA, and Osterman MJK, 2019). However, several 
confounders were not blocked yet: (1) Assisted reproduction (Lei LL, et al 
2019; Thilaganathan B, and Khalil A, 2014); and (2) Race (Fiscella K, 1996; 
Martin JA, and Osterman MJK 2019). These are shown in the association diagram 
(eFigure 1 in Appendix). Assisted reproduction can be represented by 
diagnosis/procedure codes but unavailable in our training set. For race, it is 
conceivably not able being represented by those codes.

### Chorioamnionitis

There were two assumptions regarding relationship between chorioamnionitis and 
PROM. Chorioamnionitis may affect or be affected by PROM. We applied 
chorioamnionitis as the former one (eFigure 2 in Appendix), as previously 
demonstrated (Fukami T, et al 2017). Similar assumptions were also regarded 
between chorioamnionitis and intra-amniotic infection (IAI), but, we only 
treated chorioamnionitis as being  affected by IAI (Tantengco OAG, et al, 
2019). Except cigarette smoking (ACOG, 2016a; Kim CJ, et al, 2015), data for 
all confounders were available in the training set: (1) influenza (Littauer EQ, 
et al, 2017; Kim CJ, et al, 2015); (2) asthma (Baghlaf H, et al, 2019); and (3) 
IAI (ACOG, 2016a; Tantengco OAG, et al, 2019).

### Intra-amniotic infection (IAI)

Similar to chorioamnionitis, IAI may affect or be affected by PROM. 
Consistently, we treated IAI as the former one (ACOG, 2016a) (eFigure 3 in 
Appendix). We did not have data for these confounders, especially in training 
set: (1) cervical shortening (ACOG, 2016a; Kiefer DG, et al, 2009); and 
(2) race (Fiscella K, 1996; Menon R, et al, 2011). Therefore, we used these 
factors as the confounders: (1) genital tract infection (GTI) (Pandey D, et al, 
2019; Yan JJ, et al, 2016; Romero R, et al, 2019; Tantengco OAG, et al, 2019); 
(2) periodontal disease (Figueiredo MGOP, et al, 2019; Stinson LF, et al, 
2019); (3) pneumonia (Getahun D, et al, 2007; Stinson LF, et al, 2019); and (4) 
multiple pregnancy (ACOG, 2016a; Lee SM, et al, 2020).

### Ante-partum hemorrhage (APH)

Most data for confounders of ante-partum hemorrhage (APH) and PROM (ACOG, 
2016a) were not available in the training set (eFigure 4 in Appendix). Only two 
confounders were adjusted: (1) low socioeconomic status (SES) (ACOG, 2016a; 
Bhandari S, et al, 2014); and (2) maternal age (Song J, et al, 2019; Fan D, et 
al, 2017). The other confounders were: (1) cigarette smoking (ACOG, 2016a; 
Bhandari S, et al, 2014); (2) illicit drug use (ACOG, 2016a; Bhandari S, et al, 
2014); (3) race (Fiscella K, 1996; Shen JJ, et al, 2005); (4) assisted 
reproduction (Lei LL, et al, 2019; Qin J, et al, 2016); and (5) placenta on 
anterior wall (Torricelli M, et al, 2015; Fan D, et al, 2017).

### Genital tract infection (GTI)

Association between GTI and PROM (Pandey D, et al, 2019; Yan JJ, et al, 2016) 
was only confounded by tuberculosis (FernÃ¡ndez AA, et al, 2017; Sharma JB, 
et al, 2018). We did not have data for tuberculosis in the training set. Thus, 
GTI is the only variable in the association model (eFigure 5 in Appendix).

### Periodontal disease

We also found association between periodontal disease and PROM 
(Figueiredo MGOP, et al, 2019). An association model was constructed by adding 
these confounders: (1) asthma (Baghlaf H, et al, 2019; Moraschini V, et al, 
2018); and (2) maternal age (Song J, et al, 2019; Genco RJ, and Borgnakke WS, 
2013). Because of data availability, we could not include these confounders 
into the model: (1) stress (Wang W, et al, 2020; Genco RJ, and Borgnakke WS, 2013); (2) low education (Wang W, et al, 2020; Genco RJ, and Borgnakke WS, 2013); and cigarette smoking (ACOG, 2016a; Genco RJ, and Borgnakke WS, 2013) 
(eFigure 6 in Appendix).

### Polyhydramnios

Polyhydramnios was also an associated factor of PROM (Odibo IN, et al, 2016). 
The confounders were: (1) assisted reproduction (Thilaganathan B, and Khalil A, 
2014; Lei LL, et al, 2019); and (2) multiple pregnancy (ACOG, 2016a; Moise KJ, 
1997). Only multiple pregnancy data were available in our training set; thus, 
a PROM association model was constructed using polyhydramnios and multiple pregnancy (eFigure 7 in Appendix).

### Pneumonia

Association model of pneumonia on PROM (Getahun D, et al, 2007) was confounded 
by two factors. The first confounder was influenza (Littauer EQ, et al, 2017; 
Goodnight WH, and Soper DE, 2005), while the second one was asthma 
(Baghlaf H, et al, 2019; Goodnight WH, and Soper DE, 2005). Both were included 
in the causal model (eFigure 8 in Appendix).

### Asthma

As shown in the association model of pneumonia and PROM, asthma was also an 
associated factor of PROM (Baghlaf H, et al, 2019). Influenza was the only 
common confounder (Littauer EQ, et al, 2017; Murphy VE, et al, 2017). 
Therefore, we included this confounder in the association model of asthma on 
PROM (eFigure 9 in Appendix).

### Low socioeconomic status (SES)

Low SES was also indicated as an associated factor of PROM (ACOG, 2016a). We 
could not find the confounder. The association model only included low SES. We 
represented several demographical factors as low SES (eFigure 10 in Appendix).

### Maternal age

We could find maternal age as a confounder of PROM and multiple pregnancy/APH/
periodontal disease. Obviously, there is no confounder of PROM in an associated 
model with maternal age as the variable of interest (Song J, et al 2019). We 
included this variable exclusively in the association model (eFigure 11 in 
Appendix).

### Influenza

Similar to maternal age, obviously there is no confounder of PROM in an 
association model with influenza as the variable of interest 
(Littauer EQ, et al, 2017). This disease has a specific agent. Therefore, the 
PROM association model only included this disease (eFigure 12 in Appendix).

### Final association diagram

Chorioamnionitis (odds ratio [OR] 1.351, 95% confidence interval [CI] 1.33 to 
1.372), intra-amniotic infection (OR 1.118, 95% CI 1.083 to 1.153), and genital 
tract infection (OR 1.116, 95% CI 1.101 to 1.132) were the top three highest 
effects estimated by IPW. Polyhydramnios was not verified as being associated 
with PROM by either IPW (OR 0.998, 95% CI 0.989 to 1.006) or the outcome 
regression (OR 1.238, 95% CI 0.851 to 1.801) using our data. Outcome regression 
showed the same ranks for chorioamnionitis and genital tract infection (GTI), 
while the effect estimate of intra-amniotic infection on PROM was not 
statistically significant by this method (OR 2.134, 95% CI 1 to 4.555). Three 
of 11 factors were assigned as associated factors by IPW but not by outcome 
regression. These included intra-amniotic infection and two variables: 
(1) pneumonia (OR 0.91, 95% CI 0.538 to 1.539); and (2) influenza (OR 0.957, 
95% CI 0.863 to 1.061). Effects by outcome regression were mostly larger than 
those by IPW (eTable 13 in Appendix).

In the final association diagram (eFigure 13 in Appendix), infection- and 
immune-related conditions were seen: (1) influenza; (2) asthma; and 
(3) pneumonia. Maternal factors were also observed: (1) maternal age; (2) low 
socio-economic status; (3) multiple pregnancy; and (4) ante-partum hemorrhage. 
Both groups are shown colliding on periodontal disease in the final association 
diagram (eFigure 13 in Appendix), then collided with genital tract infection on 
intra-amniotic infection continuing to either chorioamnionitis or PROM.

## Prognostic prediction of premature rupture of membranes

Predictive performances for classification task were already clearly described 
in the main text. In this Supplement, parameter estimates in each model is 
described. However, these were not always straightforward because of the 
complexity of several models.

```{r Compute ROC, include=FALSE}
roc=
  
  # Create ROC table per model.
  calib_model %>%
  lapply(X=names(.),Y=.,function(X,Y){
    Y[[X]]$pred %>%
      
      # Get the prediction table.
      select(Resample,rowIndex,event,obs) %>%
      setNames(c('subset','index','score','outcome')) %>%
      
      # Preprocess the table.
      mutate(outcome=as.double(outcome=='event')) %>%
      left_join(
        select(.,index) %>%
          filter(!duplicated(.)) %>%
          arrange(index) %>%
          cbind(
            data.frame(
                key=paste0('th_',str_pad(0:100,3,'left','0'))
                ,value=seq(0,1,len=101)
              ) %>%
              spread(key,value)
          )
        ,by='index'
      ) %>%
      gather(key,th,-subset,-index,-score,-outcome) %>%
      select(-key) %>%
      
      # Compute confusion matrices.
      mutate(pred=as.integer(score>th)) %>%
      mutate(
        tp=as.integer(outcome==1 & pred==1)
        ,fn=as.integer(outcome==1 & pred==0)
        ,fp=as.integer(outcome==0 & pred==1)
        ,tn=as.integer(outcome==0 & pred==0)
      ) %>%
      select(-index,-score,-outcome,-pred) %>%
      group_by(subset,th) %>%
      summarize_all(sum) %>%
      ungroup() %>%
      
      # Compute evaluation metrics.
      mutate(
        tpr=tp/(tp+fn+1e-17)
        ,tnr=tn/(tn+fp+1e-17)
        ,ppv=tp/(tp+fp+1e-17)
        ,npv=tn/(tn+fn+1e-17)
      ) %>%
      select(-subset,-tp,-fn,-fp,-tn) %>%
      group_by(th) %>%
      
      # Compute the interval estimates.
      summarize_all(function(x){
        y=mean(x)+(-1:1)*qnorm(0.975)*sd(x)/sqrt(length(x))
        paste0(y,collapse='|')
      }) %>%
      gather(metric,interval,-th) %>%
      separate(interval,c('lb','avg','ub'),sep='\\|') %>%
      mutate_at(c('lb','avg','ub'),as.numeric) %>%
      mutate_at(c('lb','avg','ub'),function(x){
        round(ifelse(x<0,0,ifelse(x>1,1,x)),4)
      }) %>%
      
      # Wrap up.
      mutate(model=X) %>%
      select(model,everything()) %>%
      gather(bound,value,-th,-metric,-model) %>%
      spread(metric,value)
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'RR'
    ,model=='pc_elnet'~'PC-ENR'
    ,model=='pc_rf'~'PC-RF'
    ,model=='pc_gbm'~'PC-GBM'
    ,model=='divnn'~'DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(
    model=factor(model,c('RR','PC-ENR','PC-RF','PC-GBM','DI-VNN'))
  )

# Get optimum threshold per model.
opt_th=
  roc %>%
  filter(bound=='avg') %>%
  select(model,th,tpr,tnr) %>%
  mutate(avg_acc=(tpr+tnr)/2) %>%
  arrange(model,desc(avg_acc),th,desc(tpr),desc(tnr)) %>%
  group_by(model) %>%
  slice(1) %>%
  ungroup()

# Get sensitive threshold per model.
sens_th=
  roc %>%
  filter(bound=='avg') %>%
  select(model,th,tpr,tnr) %>%
  mutate(tpr=round(tpr,2)) %>%
  right_join(
    expand.grid(model=.$model %>% .[!duplicated(.)],tpr=seq(0,1,len=101))
    ,by=c('model','tpr')
  ) %>%
  arrange(model) %>%
  group_by(model) %>%
  arrange(model,tpr,tnr,th) %>%
  mutate_at(c('tnr','th'),na_interpolation) %>%
  filter(tpr>=0.95) %>%
  slice(1) %>%
  ungroup()

# Get specific threshold per model.
spec_th=
  roc %>%
  filter(bound=='avg') %>%
  select(model,th,tpr,tnr) %>%
  mutate(tnr=round(tnr,2)) %>%
  right_join(
    expand.grid(model=.$model %>% .[!duplicated(.)],tnr=seq(0,1,len=101))
    ,by=c('model','tnr')
  ) %>%
  arrange(model) %>%
  group_by(model) %>%
  arrange(tnr,tpr,th) %>%
  mutate_at(c('tpr','th'),na_interpolation) %>%
  filter(tnr>=0.95) %>%
  slice(1) %>%
  ungroup()
```

```{r Create plot components, include=FALSE}
# Create table for calibration plot of the calibrated models.
calibration_plot_data=
  
  # Compute the interval estimates of true probabilities.
  calib_model %>%
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,obs=mean(obs)
        ,lb=mean(obs)-qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
        ,ub=mean(obs)+qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'RR'
    ,model=='pc_elnet'~'PC-ENR'
    ,model=='pc_rf'~'PC-RF'
    ,model=='pc_gbm'~'PC-GBM'
    ,model=='divnn'~'DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(
    model=factor(model,c('RR','PC-ENR','PC-RF','PC-GBM','DI-VNN'))
  ) %>%
  
  # Compute the calibration measures.
  left_join(
    group_by(.,model) %>%
      do(tidy(lm(obs~event,data=.))) %>%
      mutate(term=str_remove_all(str_to_lower(term),'[:punct:]')) %>%
      mutate(
        avg=estimate
        ,ci=qnorm(0.975)*std.error
      ) %>%
      select(model,term,avg,ci) %>%
      pivot_wider(names_from='term',values_from=c('avg','ci'))
    ,by='model'
  )

# Plot the calibration table.
calibration_plot=
  
  # Plot the table.
  calibration_plot_data %>%
  qplot(event,obs,data=.) +
  geom_linerange(aes(ymin=lb,ymax=ub)) +
  geom_abline(lty=2) +
  
  # Annotate the calibration measures.
  geom_text(
    aes(
      x=0,y=1.3
      ,label=
        ifelse(
          event==0.3
          ,paste0(
            'Intercept ',formatC(round(avg_intercept,2),decimal.mark='.')
            ,' Â± '
            ,formatC(round(ci_intercept,2),decimal.mark='.')
            ,'\n'
            ,'Slope ',formatC(round(avg_event,2),decimal.mark='.')
            ,' Â± '
            ,formatC(round(ci_event,2),decimal.mark='.')
          )
          ,NA
        )
    )
    ,family='sans'
    ,size=3
    ,hjust=0
    ,vjust=1
    ,alpha=1
    ,na.rm=T
  ) +
  
  # Customized plot.
  facet_wrap(~model,ncol=5) +
  coord_equal() +
  scale_x_continuous('Predicted probability',limits=c(0:1)) +
  scale_y_continuous(
    'True probability\n(95% CI)',limits=c(0,1.3),breaks=seq(0,1,0.2)
    ,labels=scales::number_format(accuracy=0.1,decimal.mark='.')
  ) +
  theme(
    # axis.text.x=element_text(angle=90,vjust=0.5,hjust=1)
    axis.ticks.x=element_blank()
    ,axis.text.x=element_blank()
    ,axis.text.y=element_text(family='sans')
    ,axis.title.x=element_blank()
    ,axis.title.y=element_text(hjust=0.3,family='sans')
    ,strip.text=element_text(family='sans')
  )

# Create probability distribution table of events for the calibrated models.
event_dist_data=
  
  # Compute the numbers of instances per predicted probability for each model.
  calib_model %>%
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      filter(obs==1) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,n=n()/10e+4
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'RR'
    ,model=='pc_elnet'~'PC-ENR'
    ,model=='pc_rf'~'PC-RF'
    ,model=='pc_gbm'~'PC-GBM'
    ,model=='divnn'~'DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(
    model=factor(model,c('RR','PC-ENR','PC-RF','PC-GBM','DI-VNN'))
  )

# Plot the probability distribution of events for each model, as histogram.
event_dist=
  event_dist_data %>%
  qplot(event,n,data=.,geom='col',na.rm=T) +
  geom_vline(data=opt_th,aes(xintercept=th),lty=2) +
  facet_wrap(~model,ncol=5) +
  scale_x_continuous('Predicted probability') +
  scale_y_continuous(
    'Events per\n10,000 visits',breaks=seq(0,0.3,0.1)
    ,labels=scales::number_format(accuracy=0.1,decimal.mark='.')
  ) +
  theme(
    # axis.text.x=element_text(angle=90,vjust=0.5,hjust=1)
    axis.ticks.x=element_blank()
    ,axis.text.x=element_blank()
    ,axis.text.y=element_text(family='sans')
    ,axis.title.x=element_blank()
    ,axis.title.y=element_text(hjust=0.3,family='sans')
    ,strip.text.x=element_blank()
  )

# Create probability distribution table of nonevents for the calibrated models.
nonevent_dist_data=
  
  # Compute the numbers of instances per predicted probability for each model.
  calib_model %>%
  lapply(X=names(.),Y=.,function(X,Y){
    mutate(
        Y[[X]]$pred
        ,event=round(event,1)
        ,obs=as.integer(obs=='event')
      ) %>%
      filter(obs==0) %>%
      group_by(event) %>%
      summarize(
        model=factor(X,names(Y))
        ,n=n()/10e+5
      )
  }) %>%
  do.call(rbind,.) %>%
  
  # Clean up naming.
  mutate(model=case_when(
    model=='causal_ridge'~'RR'
    ,model=='pc_elnet'~'PC-ENR'
    ,model=='pc_rf'~'PC-RF'
    ,model=='pc_gbm'~'PC-GBM'
    ,model=='divnn'~'DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(
    model=factor(model,c('RR','PC-ENR','PC-RF','PC-GBM','DI-VNN'))
  )

# Plot the probability distribution of nonevents for each model, as histogram.
nonevent_dist=
  nonevent_dist_data %>%
  qplot(event,n,data=.,geom='col',na.rm=T) +
  geom_text(
    data=opt_th,aes(x=th,y=0.2,label=round(th,2))
    ,size=3,hjust=-0.3,vjust=0,family='serif'
  ) +
  geom_vline(data=opt_th,aes(xintercept=th),lty=2) +
  facet_wrap(~model,ncol=5) +
  scale_x_continuous(
    'Predicted probability'
    ,labels=scales::number_format(accuracy=0.01,decimal.mark='.')
  ) +
  scale_y_reverse(
    'Nonevents per\n100,000 visits',breaks=seq(0,0.2,0.1)
    ,labels=scales::number_format(accuracy=0.1,decimal.mark='.')
  ) +
  theme(
    axis.title=element_text(family='sans')
    ,axis.text.x=element_text(angle=90,vjust=0.5,hjust=1,family='sans')
    ,axis.text.y=element_text(family='sans')
    ,strip.text.x=element_blank()
    ,strip.text.y=element_text(family='sans')
  )

# Filter only the averages in the ROC table.
roc_curve_data=
  roc %>%
  filter(bound=='avg') %>%
  select(-bound)

# Plot the ROC curve.
roc_curve=
  
  # Plot the curve.
  roc_curve_data %>%
  ggplot(aes(tnr,tpr,color=model,fill=model),data=.) +
  geom_line(size=1) +
  geom_abline(intercept=1,slope=1,lty=2) +
  geom_blank(data=data.frame(tnr=-1,tpr=1,model=NA)) +
  geom_point(data=opt_th,size=3) +
  
  # Plot references from the systematic review results.
  geom_point(
    data=
      read_xlsx('data/srma_prom.xlsx') %>%
      # To modify the citation
      # mutate(
      #   study=str_replace(study,'\\[69\\]','[6]')
      #   ,study=str_replace(study,'\\[23\\]','[7]')
      # ) %>%
      rename(tnr=spec,tpr=sens)
    ,color='black',fill='black'
  ) +
  geom_label(
    data=
      read_xlsx('data/srma_prom.xlsx') %>%
      # To modify the citation
      # mutate(
      #   study=str_replace(study,'\\[69\\]','[6]')
      #   ,study=str_replace(study,'\\[23\\]','[7]')
      # ) %>%
      rename(tnr=spec,tpr=sens)
    ,aes(label=study)
    ,color='black',fill='white',family='sans'
    ,hjust=0,vjust=1
    ,size=3,alpha=0.75,label.padding=unit(0.15,'lines')
  ) +
  geom_label(
    data=
      opt_th %>%
      mutate(pos=rev(seq(0.1,0.5,len=5)))
    ,aes(x=0.3,y=pos,label=model)
    ,hjust=0,vjust=1,size=3,color='black',alpha=0.75,family='sans'
    ,show.legend=F
  ) +
  
  # Customize plot.
  coord_equal() +
  scale_x_reverse(
    'Specificity',limits=1:0,breaks=rev(seq(0,1,0.1))
    ,labels=scales::number_format(accuracy=0.1,decimal.mark='.')
  ) +
  scale_y_continuous(
    'Sensitivity',limits=0:1,breaks=seq(0,1,0.1)
    ,labels=scales::number_format(accuracy=0.1,decimal.mark='.')
  ) +
  scale_color_jama() +
  scale_fill_jama() +
  guides(color=F,fill=F) +
  theme(
    axis.title=element_text(family='sans')
    ,axis.text.x=element_text(angle=90,hjust=1,vjust=0.5,family='sans')
    ,axis.text.y=element_text(family='sans')
  )

# Create table for AUROC plot of the calibrated models.
auroc_table=
  
  # Get the evaluation results.
  eval_model %>%
  lapply(X=names(.),Y=.,function(X,Y){
    Z=Y[[X]] %>%
      lapply(function(x)x$optres$Group1['AUC-ROC',]) %>%
      do.call(rbind,.) %>%
      rownames_to_column(var='set') %>%
      mutate(model=X)
    suppressWarnings(separate(Z,CI,c('lb','ub'),sep='-'))
  }) %>%
  do.call(rbind,.) %>%
  filter(!str_detect(set,'nocalib')) %>%
  mutate(set=str_remove_all(set,'calib_')) %>%
  
  # Clean up model naming.
  mutate(model=case_when(
    model=='causal_ridge'~'RR'
    ,model=='pc_elnet'~'PC-ENR'
    ,model=='pc_rf'~'PC-RF'
    ,model=='pc_gbm'~'PC-GBM'
    ,model=='divnn'~'DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(
    model=factor(model,c('RR','PC-ENR','PC-RF','PC-GBM','DI-VNN'))
  ) %>%
  
  # Clean up subset naming.
  mutate(set=case_when(
    set=='ran'~'External\nrandom\nsplit'
    ,set=='geo'~'External\ngeographical\nsplit'
    ,set=='tem'~'External\ntemporal\nsplit'
    ,set=='bgt'~'External\ngeotemporal\nsplit'
    ,TRUE~''
  )) %>%
  
  # Compute the AUROC average of all the models in each subset.
  group_by(set) %>%
  mutate(set_score=mean(Score)) %>%
  ungroup() %>%
  
  # Add the interval validation results.
  rbind(
    calib_model %>%
      
      # Get the evaluation results. 
      lapply(X=names(.),Y=.,function(X,Y){
        mutate(
            Y[[X]]$results
            ,ROC_lb=ROC-qnorm(0.975)*ROCSD
            ,ROC_ub=ROC+qnorm(0.975)*ROCSD
          ) %>%
          mutate(model=X) %>%
          select(model,ROC,ROC_lb,ROC_ub) %>%
          setNames(c('model','Score','lb','ub'))
      }) %>%
      do.call(rbind,.) %>%
      
      # Clean up model naming.
      mutate(model=case_when(
        model=='causal_ridge'~'RR'
        ,model=='pc_elnet'~'PC-ENR'
        ,model=='pc_rf'~'PC-RF'
        ,model=='pc_gbm'~'PC-GBM'
        ,model=='divnn'~'DI-VNN'
        ,TRUE~''
      )) %>%
      mutate(
        model=factor(model,c('RR','PC-ENR','PC-RF','PC-GBM','DI-VNN'))
      ) %>%
      
      # Clean up subset naming.
      mutate(
        set='Internal\ncalibration\nsplit'
        ,set_score=mean(Score)
      ) %>%
      select(set,Score,lb,ub,model,everything())
  ) %>%
  
  # Convert lower and upper bounds to numeric.
  mutate_at(c('lb','ub'),as.numeric) %>%
  
  # Add references from the systematic review results.
  rbind(
    read_xlsx('data/srma_prom.xlsx') %>%
      mutate(set='Training set') %>%
      # To modify the citation
      # mutate(
      #   study=str_replace(study,'\\[37\\]','[6]')
      #   ,study=str_replace(study,'\\[23\\]','[7]')
      # ) %>%
      rename(model=study,Score=auroc,lb=auroc_lb,ub=auroc_ub) %>%
      select(set,Score,lb,ub,model) %>%
      mutate(set_score=mean(Score))
  ) %>%
  
  # Clean up subset naming.
  mutate(
    set=
      factor(
        set
        ,c('Internal\ncalibration\nsplit'
           ,'Training set'
           ,'External\nrandom\nsplit'
           ,'External\ngeographical\nsplit'
           ,'External\ntemporal\nsplit'
           ,'External\ngeotemporal\nsplit')
      )
  )


# Plot the AUROCs.
auroc=
  auroc_table %>%
  ggplot(aes(model,Score),data=.) +
  geom_point(size=1) +
  geom_errorbar(aes(ymin=lb,ymax=ub),width=0.5,size=0.75) +
  geom_hline(aes(yintercept=set_score),lty=2) +
  geom_hline(yintercept=0.5,lty=2) +
  facet_grid(set~.,scales='free_y',space='free') +
  coord_flip() +
  scale_x_discrete('Data Partition') +
  scale_y_continuous(
    'AUROC (95% CI)',breaks=seq(0,1,0.1)
    ,labels=scales::number_format(accuracy=0.1,decimal.mark='.')
  ) +
  theme(
    axis.title.y=element_blank()
    ,axis.title.x=element_text(family='sans')
    ,axis.text.y=element_text(size=unit(8,'pt'),family='sans')
    ,axis.text.x=element_text(family='sans')
    ,strip.text.y=element_text(angle=0,family='sans')
  )
```

```{r Save data for Figure 2 of the main text, eval=FALSE, include=FALSE}
# Figure 2a
calibration_plot_data %>%
  
  # Calibration plot data
  select(
    model
    ,event
    ,obs
    ,lb
    ,ub
    ,avg_intercept
    ,ci_intercept
    ,avg_event
    ,ci_event
  ) %>%
  rename(prob=event) %>%
  
  # Probability distribution data
  left_join(
    event_dist_data %>%
      rename(prob=event) %>%
      rename(event=n) %>%
      left_join(
        nonevent_dist_data %>%
          rename(prob=event) %>%
          rename(nonevent=n)
        ,by=c('prob','model')
      ) %>%
      left_join(opt_th,by='model') %>%
      select(model,prob,event,nonevent,th)
    ,by=c('model','prob')
  ) %>%
  
  # Clean up naming.
  setNames(
    c('Model'
      ,'Predicted probability'
      ,'True probability'
      ,'Lower bound'
      ,'Upper bound'
      ,'Intercept'
      ,'Â± 95% CI'
      ,'Slope'
      ,'Â± 95% CI'
      ,'Events per 10,000 visits'
      ,'Nonevents per 100,000 visits'
      ,'Optimum threshold')
  ) %>%
  
  # Save data for Figure 2a.
  write_csv('data/figure2a.csv')

# Figure 2b
read_xlsx('data/srma_prom.xlsx') %>%
  
  # Reference from the systematic review results
  mutate(th=NA,tnr=NA,tpr=NA) %>%
  select(study,th,tnr,tpr,spec,sens) %>%
  rename(model=study) %>%
  
  # ROC curve data
  rbind(
    roc_curve_data %>%
      select(-npv,-ppv) %>%
      left_join(
        opt_th %>%
          select(model,tnr,tpr) %>%
          rename(sens=tpr,spec=tnr)
        ,by='model'
      )
  ) %>%
  
  # Clean up naming.
  setNames(
    c('Model'
      ,'Threshold'
      ,'Specificity'
      ,'Sensitivity'
      ,'Specificity at targeted threshold *'
      ,'Sensitivity at targeted threshold *'
    )
  ) %>%
  
  # Save data for Figure 2b.
  write_csv('data/figure2b.csv')

# Figure 2c
auroc_table %>%
  
  # Clean up naming.
  mutate(set=str_replace_all(set,'\n',' ')) %>%
  arrange(
    factor(
      set
      ,c('Internal calibration split'
         ,'Training set'
         ,'External random split'
         ,'External geographical split'
         ,'External temporal split'
         ,'External geotemporal split')
    )
    ,factor(
      model
      ,c('DI-VNN'
         ,'PC-GBM'
         ,'PC-RF'
         ,'PC-ENR'
         ,'RR'
         ,'Basbug, et al. [40]'
         ,'El-Achi, et al. [23]')
    )
  ) %>%
  slice(c(1:5,7,6,8:nrow(.))) %>%
  select(set,model,everything()) %>%
  setNames(
    c('Set'
      ,'Model'
      ,'AUROC (95% CI)'
      ,'Lower bound'
      ,'Upper bound'
      ,'Average AUROC per set'
    )
  ) %>%
  
  # Save data for Figure 2c.
  write_csv('data/figure2c.csv')
```

```{r Save image of Figure 2 of the main text, fig.height=8.85827, fig.width=7.08661, eval=FALSE, include=FALSE}
# fig.height=8.85827, fig.width=7.08661 on top of this chunk if shown here
ggarrange(
    ggarrange(
      calibration_plot
      ,event_dist
      ,nonevent_dist
      ,heights=c(2,1.5,1.5)
      ,labels=c('a','','')
      ,ncol=1,nrow=3
    )
    ,ggarrange(
      roc_curve
      ,auroc
      ,heights=c(1,1)
      ,labels=c('b','c')
      ,ncol=2,nrow=1
    )
    ,heights=c(5,3.85827)
    ,labels=c('','')
    ,ncol=1,nrow=2
  ) +
  ggsave(
    filename="figure2.eps"
    # To modify the citation
    # filename="figure2b.eps"
    ,device=cairo_ps
    ,width=170
    ,height=170/190*237.5
    ,unit='mm'
  )
```

```{r PC-RF at 95% specificity, eval=FALSE, include=FALSE}
roc %>%
  filter(
    model=='PC-RF'
    & between(
      th
      ,floor(spec_th$th[spec_th$model=='PC-RF']*100)/100
      ,ceiling(spec_th$th[spec_th$model=='PC-RF']*100)/100
    )
  ) %>%
  group_by(th) %>%
  mutate(dif=abs(0.95-mean(tnr))) %>%
  ungroup() %>%
  filter(dif==min(dif)) %>%
  select(model,th,bound,tpr) %>%
  spread(bound,tpr) %>%
  kable(caption='PC-RF at 95% specificity') %>%
  kable_classic()
```

```{r DI-VNN at 95% specificity, eval=FALSE, include=FALSE}
roc %>%
  filter(
    model=='DI-VNN'
    & between(
      th
      ,floor(spec_th$th[spec_th$model=='DI-VNN']*100)/100
      ,ceiling(spec_th$th[spec_th$model=='DI-VNN']*100)/100
    )
  ) %>%
  group_by(th) %>%
  mutate(dif=abs(0.95-mean(tnr))) %>%
  ungroup() %>%
  filter(dif==min(dif)) %>%
  select(model,th,bound,tpr) %>%
  spread(bound,tpr) %>%
  kable(caption='DI-VNN at 95% specificity') %>%
  kable_classic()
```

```{r Optimum PC-RF performance, eval=FALSE, include=FALSE}
rbind(
    roc %>%
      filter(
        model=='PC-RF'
        & th==opt_th$th[opt_th$model=='PC-RF']
      ) %>%
      select(model,th,bound,tpr) %>%
      spread(bound,tpr) %>%
      mutate(metric='Sensitivity')
    ,roc %>%
      filter(
        model=='PC-RF'
        & th==opt_th$th[opt_th$model=='PC-RF']
      ) %>%
      select(model,th,bound,tnr) %>%
      spread(bound,tnr) %>%
      mutate(metric='Specificity')
  ) %>%
  kable(caption='Optimum PC-RF performance') %>%
  kable_classic()
```

```{r Optimum DI-VNN performance, eval=FALSE, include=FALSE}
rbind(
    roc %>%
      filter(
        model=='DI-VNN'
        & th==opt_th$th[opt_th$model=='DI-VNN']
      ) %>%
      select(model,th,bound,tpr) %>%
      spread(bound,tpr) %>%
      mutate(metric='Sensitivity')
    ,roc %>%
      filter(
        model=='DI-VNN'
        & th==opt_th$th[opt_th$model=='DI-VNN']
      ) %>%
      select(model,th,bound,tnr) %>%
      spread(bound,tnr) %>%
      mutate(metric='Specificity')
  ) %>%
  kable(caption='Optimum DI-VNN performance') %>%
  kable_classic()
```

```{r AUROCs of DI-VNN, eval=FALSE, include=FALSE}
auroc_table %>%
  filter(model=='DI-VNN') %>%
  kable() %>%
  kable_classic()
```

For ridge regression (RR), the estimates were weights or beta values, as 
commonly described in a regression model (eTable 14 in R Markdown). For 
classification task, top three highest weights were assigned for 
chorioamnionitis, IAI, and GTI. This is similar to the effect ranks in 
association tests by IPW.

```{r Create empty list for tables of model weight, include=FALSE}
model_weight=list()
```

```{r Prepare weight table of RR, include=FALSE}
model_weight$causal_ridge=
  rbind(
    coef(model$causal_ridge$finalModel,model$causal_ridge$bestTune$lambda) %>%
      as.matrix() %>%
      as.data.frame() %>%
      rownames_to_column(var='term') %>%
      mutate(term=str_remove_all(term,'\\`|\\(|\\)')) %>%
      setNames(c('term','estimate')) %>%
      mutate(task='classification weight')
    ,coef(
        timing_model$causal_ridge$finalModel
        ,timing_model$causal_ridge$bestTune$lambda
      ) %>%
      as.matrix() %>%
      as.data.frame() %>%
      rownames_to_column(var='term') %>%
      mutate(term=str_remove_all(term,'\\`|\\(|\\)')) %>%
      setNames(c('term','estimate')) %>%
      mutate(task='estimation weight')
  ) %>%
  left_join(
    dag$baseline_nodes %>%
      mutate(label=paste0('causal_',label)) %>%
      rename(term=label,description=name) %>%
      add_case(data.frame(term='Intercept',description='Intercept'))
    ,by='term'
  ) %>%
  spread(task,estimate) %>%
  arrange(desc(`classification weight`),desc(`estimation weight`)) %>%
  slice(c(which(.$term=='Intercept'),which(.$term!='Intercept'))) %>%
  setNames(str_to_sentence(str_replace_all(colnames(.),'_',' ')))
```

```{r Save and show weights of RR, eval=FALSE, include=FALSE}
model_weight$causal_ridge %>%
  
  # Save for eTable 14.
  write_csv('data/supp_table_14.csv')

model_weight$causal_ridge %>%
  kable() %>%
  kable_classic()
```

Three models used principal components (PCs), including PC elastic net 
regression (PC-ENR). To transform predictors into a PC, each predictor has a 
weight to multiply with. These weights were also shown (eTable 16 in R 
Markdown). For the PCs themselves, the parameter estimates in PC-ENR were 
similar to those in RR. The weights in PC-ENR were also shown (eTable 16 in R 
Markdown).

```{r Prepare weight table of PC model, include=FALSE}
model_weight$pc=
  composition(pw_int_rsdr) %>%
  rownames_to_column(var='Predictor') %>%
  left_join(
    rbind(
      annotation %>%
        rename(Predictor=code,Description=desc)
      ,dag$baseline_nodes %>%
        mutate(label=paste0('causal_',label)) %>%
        rename(Predictor=label,Description=name)
    )
    ,by='Predictor'
  ) %>%
  arrange(factor(Predictor,int_nps_eol_p$key)) %>%
  select(Predictor,Description,everything())
```

```{r Save and show weights of PC, eval=FALSE, include=FALSE}
model_weight$pc %>%
  
  # Save for eTable 15.
  write_csv('data/supp_table_15.csv')

model_weight$pc %>%
  kable() %>%
  kable_classic()
```

```{r Prepare weight table of PC-ENR, include=FALSE}
model_weight$pc_elnet=
  rbind(
    coef(model$pc_elnet$finalModel,model$pc_elnet$bestTune$lambda) %>%
      as.matrix() %>%
      as.data.frame() %>%
      rownames_to_column(var='term') %>%
      mutate(term=str_remove_all(term,'\\`|\\(|\\)')) %>%
      setNames(c('term','estimate')) %>%
      mutate(task='classification weight')
    ,coef(
        timing_model$pc_elnet$finalModel
        ,timing_model$pc_elnet$bestTune$lambda
      ) %>%
      as.matrix() %>%
      as.data.frame() %>%
      rownames_to_column(var='term') %>%
      mutate(term=str_remove_all(term,'\\`|\\(|\\)')) %>%
      setNames(c('term','estimate')) %>%
      mutate(task='estimation weight')
  ) %>%
  spread(task,estimate) %>%
  arrange(desc(`classification weight`),desc(`estimation weight`)) %>%
  setNames(str_to_sentence(str_replace_all(colnames(.),'_',' ')))
```

```{r Save and show weights of PC-ENR, eval=FALSE, include=FALSE}
model_weight$pc_elnet %>%
  
  # Save for eTable 16.
  write_csv('data/supp_table_16.csv')

model_weight$pc_elnet %>%
  kable() %>%
  kable_classic()
```

For PC random forest (PC-RF) and PC gradient boosting machine (PC-GBM), the 
parameter estimates may be represented by variable importance. It is a 
proportion of learners (trees) that include a predictor. These numbers were 
also shown (eTables 17 and 18 in R Markdown).

```{r Prepare weight table of PC-RF, include=FALSE}
model_weight$pc_rf=
  rbind(
    varImp(model$pc_rf)[[1]] %>%
      setNames('variable_importance') %>%
      rownames_to_column(var='term') %>%
      mutate(task='classification variable importance')
    ,varImp(timing_model$pc_rf)[[1]] %>%
      setNames('variable_importance') %>%
      rownames_to_column(var='term') %>%
      mutate(task='estimation variable importance')
  ) %>%
  spread(task,variable_importance) %>%
  arrange(
    desc(`classification variable importance`)
    ,desc(`estimation variable importance`)
  ) %>%
  setNames(str_to_sentence(str_replace_all(colnames(.),'_',' ')))
```

```{r Save and show weights of PC-RF, eval=FALSE, include=FALSE}
model_weight$pc_rf %>%
  
  # Save for eTable 17.
  write_csv('data/supp_table_17.csv')

model_weight$pc_rf %>%
  kable() %>%
  kable_classic()
```

```{r Prepare weight table of PC-GBM, include=FALSE}
model_weight$pc_gbm=
  rbind(
    varImp(model$pc_gbm)[[1]] %>%
      setNames('variable_importance') %>%
      rownames_to_column(var='term') %>%
      mutate(task='classification variable importance')
    ,varImp(timing_model$pc_gbm)[[1]] %>%
      setNames('variable_importance') %>%
      rownames_to_column(var='term') %>%
      mutate(task='estimation variable importance')
  ) %>%
  spread(task,variable_importance) %>%
  arrange(
    desc(`classification variable importance`)
    ,desc(`estimation variable importance`)
  ) %>%
  setNames(str_to_sentence(str_replace_all(colnames(.),'_',' ')))
```

```{r Save and show weights of PC-GBM, eval=FALSE, include=FALSE}
model_weight$pc_gbm %>%
  
  # Save for eTable 18.
  write_csv('data/supp_table_18.csv')

model_weight$pc_gbm %>%
  kable() %>%
  kable_classic()
```

For DI-VNN, we filtered predictors by differential analysis which consisted of 
multiple univariable linear regressions. The parameter estimates were expressed 
as log of fold changes. These were equivalent to log of odds ratios. This 
number and others, including false discovery rate (FDR), were also shown for 
the selected predictors by FDR <0.05 (eTable 19 in R Markdown).

```{r Prepare weight table of differential analysis, include=FALSE}
model_weight$fit=
  
  # Get differential analysis results.
  input$fit %>%
  rownames_to_column(var='Predictor') %>%
  arrange(adj.P.Val,P.Value,desc(abs(AveExpr))) %>%
  
  # Clean up data.
  mutate(
    P.Value=
      ifelse(
        P.Value<0.0000001
        ,'p<1e-08'
        ,format(P.Value,scientific=T,digits=1)
      )
    ,adj.P.Val=
      ifelse(
        adj.P.Val<0.0000001
        ,'p<1e-08'
        ,format(adj.P.Val,scientific=T,digits=1)
      )
  ) %>%
  
  # Clean up naming.
  left_join(
    rbind(
      annotation %>%
        rename(Predictor=code,Description=desc)
      ,dag$baseline_nodes %>%
        mutate(label=paste0('causal_',label)) %>%
        rename(Predictor=label,Description=name)
    )
    ,by='Predictor'
  ) %>%
  select(Predictor,Description,everything()) %>%
  setNames(
    ifelse(names(.)=='logFC','log(Fold change)',
    ifelse(names(.)=='AveExpr','Normalized average of global value',
    ifelse(names(.)=='t','t statistic',
    ifelse(names(.)=='P.Value','p-value',
    ifelse(names(.)=='adj.P.Val','FDR or adjusted p-value',
    ifelse(names(.)=='B','B statistic',names(.)))))))
  )
```

```{r Save and show differential analysis, eval=FALSE, include=FALSE}
model_weight$fit %>%
  
  # Save for eTable 19.
  write_csv('data/supp_table_19.csv')

model_weight$fit %>%
  kable() %>%
  kable_classic()
```

Parameter estimates of the DI-VNN were extremely enormous. To get similar sense 
with those of the other models, we used an intermediate output at a layer after 
being fed to Inception v4 for each ontology. This showed a learning 
representation by DI-VNN on a predictor. How these were computed had been 
described in a protocol we followed for these procedures [...]. The 
intermediate outputs were shown (eTable 20 in R Markdown). We would point out 
several meanings of these outputs after the next section. In addition, 
connections between ontologies under the root are also shown (eTable 21 in 
R Markdown).

For comparison of our models with those of previous studies, we applied methods 
in the preferred reporting items for systematic reviews and meta-analyses 
(PRISMA) 2020 expanded checklist (eTable 4 in Appendix). From three literature 
databases and several steps, we found two prediction models as described in the 
main text (eTable 5 in Appendix). The steps are shown (eFigure 14 in Appendix).

```{r A function to visualize ontoarray of estimation DI-VNN, include=FALSE}
source('R/viz.ontoarray.reg-function.R')
```

```{r Get ontoarray visualisation of timing regression table., include=FALSE}
if(run_heavy_computation){
  cat('Get ontoarray visualisation of timing regression table.\n')
  visualization$ontoarray_reg=
    output_reg %>%
    viz.ontoarray.reg(modeling_divnn_reg$ontonet,batch_size=512,verbose=T)
  
  saveRDS(visualization$ontoarray_reg,'data/visualization_ontoarray_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['visualization_ontoarray_reg']])
  visualization$ontoarray_reg=readRDS('data/visualization_ontoarray_reg.rds')
}
```

```{r Prepare weight table of DI-VNN, include=FALSE}
model_weight$divnn=
  
  # Get the classification DI-VNN weights of the representation layers.
  visualization$ontoarray %>%
  lapply(function(x){
    x$ontotype %>%
      lapply(X=seq(nrow(.)),Y=.,Z=x$output,function(X,Y,Z){
        K=Z[Y$x[X],Y$y[X],Y$z[X]]
        if(K==0){
          NULL
        }else{
          K
        }
      }) %>%
      setNames(x$ontotype$feature)
  }) %>%
  setNames(names(visualization$ontoarray)) %>%
  unlist() %>%
  data.frame(output=.) %>%
  rownames_to_column(var='ontology') %>%
  rename(classification_output=output) %>%
  
  # Get the estimation DI-VNN weights of the representation layers.
  left_join(
    visualization$ontoarray_reg %>%
      lapply(function(x){
        x$ontotype %>%
          lapply(X=seq(nrow(.)),Y=.,Z=x$output,function(X,Y,Z){
            K=Z[Y$x[X],Y$y[X],Y$z[X]]
            if(K==0){
              NULL
            }else{
              K
            }
          }) %>%
          setNames(x$ontotype$feature)
      }) %>%
      setNames(names(visualization$ontoarray)) %>%
      unlist() %>%
      data.frame(output=.) %>%
      rownames_to_column(var='ontology') %>%
      rename(estimation_output=output)
    ,by='ontology'
  ) %>%
  separate(ontology,c('ontology','predictor'),sep='\\.') %>%
  
  # Get the predictor annotation.
  left_join(
    rbind(
        annotation %>%
          rename(predictor=code,description=desc)
        ,dag$baseline_nodes %>%
          mutate(label=paste0('causal_',label)) %>%
          rename(predictor=label,description=name)
      )
    ,by='predictor'
  ) %>%
  
  # Get the predictor position.
  left_join(
    output %>%
      fData() %>%
      rownames_to_column(var='xyz') %>%
      rename(predictor=feature) %>%
      select(xyz,predictor) %>%
      filter(!is.na(predictor)) %>%
      mutate(xyz=str_remove_all(xyz,'x')) %>%
      separate(xyz,c('dimension_1','yz'),sep='y') %>%
      separate(yz,c('dimension_2','dimension_3'),sep='z')
    ,by='predictor'
  ) %>%
  
  # Wrap up.
  arrange(ontology,dimension_3,dimension_2,dimension_1,predictor) %>%
  select(
    ontology
    ,dimension_1
    ,dimension_2
    ,dimension_3
    ,predictor
    ,description
    ,everything()
  ) %>%
  setNames(str_to_sentence(str_replace_all(colnames(.),'_',' ')))
```

```{r Save and show weights of DI-VNN, eval=FALSE, include=FALSE}
model_weight$divnn %>%
  
  # Save for eTable 20.
  write_csv('data/supp_table_20.csv')

model_weight$divnn %>%
  kable() %>%
  kable_classic()
```

```{r Save ontology table, eval=FALSE, include=FALSE}
input$ontology %>%
  filter(relation!='feature') %>%
  select(-relation) %>%
  setNames(str_to_sentence(colnames(.))) %>%
  
  # Save for eTable 21.
  write_csv('data/supp_table_21.csv')
```

## Estimation of the time of delivery

Estimation performances were already clearly described in the main text. 
Parameter estimates of the estimation models were also shown in the same 
eTables with those of the classification models (eTable 14 in R Markdown). For  
estimation task, which implicitly related to preterm delivery, the lowest 
(negative) weights in the RR were also assigned to chorioamnionitis, IAI, and  
GTI, but, the second lowest rank was shifted up by multiple pregnancy. Lower-
rank weight means earlier time of delivery, which is, likely having more chance 
to be a preterm delivery. It turns out our decision to choose IAI and 
chorioamnionitis as the associated factors, instead of those being affected of  
PROM, is consistent with these findings.

Although the RR mostly had achieved highest proportion of criteria fulfilled 
among the models by external validation sets, we noticed this only applies for  
the predicted non-events based on visual assessment. The associated factors may 
be well-generalized to estimate the time of delivery under non-events predicted 
(modeled) by DI-VNN. Nevertheless, we selected PC-RF as the best model for 
estimation task of the time of delivery.

Challenge on estimation task is reasonable considering different distributions 
among internal and external validation sets. In internal validation set, of 
which we only utilized the calibration set for model evaluation, predicted 
events by DI-VNN happened 15 weeks (95% CI 11 to 18; *n*=760) on average from 
the time of prediction. This was similar to those in external random (18, 
95% CI 14 to 21; *n*=973) and temporal splits (15, 95% CI 12 to 19; *n*=687), 
but later than those in external geographical (9, 95% CI 6 to 13; *n*=500) and 
geotemporal splits (3, 95% CI 1 to 5; *n*=157). Meanwhile, the pregnancies 
predicted as nonevents by DI-VNN ended at 40 weeks (95% CI 39 to 41; 
*n*=20,746) on average from time of prediction. This was earlier than those in 
external random (42, 95% CI 40 to 43; *n*=25,959), temporal (52, 
95% CI 50 to 53; *n*=17,533), and geotemporal splits (60, 95% CI 54 to 65; 
*n*=2,067), but later than that in external geographical split (37, 
95% CI 35 to 38; *n*=15,318).

```{r Prepare estimation plots, include=FALSE}
estimation_plot_all=
  
  # Clean up model naming.
  eval_timing_model_plt %>%
  mutate(model=case_when(
    model=='causal_ridge'~'RR'
    ,model=='pc_elnet'~'PC-ENR'
    ,model=='pc_rf'~'PC-RF'
    ,model=='pc_gbm'~'PC-GBM'
    ,model=='divnn'~'DI-VNN'
    ,TRUE~''
  )) %>%
  mutate(
    model=factor(model,c('RR','PC-ENR','PC-RF','PC-GBM','DI-VNN'))
  ) %>%
  
  # Clean up subset naming.
  mutate(set=case_when(
    set=='timing_calib'~'Internal\ncalibration\nsplit'
    ,set=='timing_ran'~'External\nrandom\nsplit'
    ,set=='timing_geo'~'External\ngeographical\nsplit'
    ,set=='timing_tem'~'External\ntemporal\nsplit'
    ,set=='timing_bgt'~'External\ngeotemporal\nsplit'
    ,TRUE~''
  )) %>%
  mutate(
    set=
      factor(
        set
        ,c('Internal\ncalibration\nsplit'
           ,'External\nrandom\nsplit'
           ,'External\ngeographical\nsplit'
           ,'External\ntemporal\nsplit'
           ,'External\ngeotemporal\nsplit')
      )
  )

# Filter the estimation of interest.
estimation_plot_comp_data=
  estimation_plot_all %>%
  filter(metric_name=='calib_pred' & timing_pred<=42) %>%
  mutate(
    fall_into_interval=
      timing_pred>=lb & timing_pred<=ub & abs(ub-lb)<=timing_pred
      & lb<=42 & ub<=42
    ,fall_into_interval=
      ifelse(is.na(fall_into_interval),FALSE,fall_into_interval)
  ) %>%
  group_by(model,set) %>%
  mutate(p=sum(fall_into_interval)/42)

# Plot the comparison of the estimated time of delivery.
estimation_plot_comp=
  
  # Plot the estimation.
  estimation_plot_comp_data %>%
  ggplot(aes(timing_pred,avg,color=str_to_title(metric)),data=.) +
  geom_point(alpha=0.5) +
  # geom_linerange(aes(ymin=lb,ymax=ub),size=1,alpha=0.5,show.legend=F) +
  geom_smooth(method='loess',formula=y~x,size=1,se=F) +
  geom_abline(slope=1,intercept=0,lty=2) +
  
  # Show the criteria fulfillment proportion.
  geom_label(
    x=40,y=2
    ,aes(
      label=paste0(formatC(round(p*100,2),decimal.mark='.'),'%')
    )
    ,hjust=1,vjust=0,color='black'
    ,size=unit(3,'pt'),label.padding=unit(0.1,'lines'),family='sans'
  ) +
  
  # Customize plot.
  facet_grid(set~model) +
  scale_x_continuous(limits=c(0,42)) +
  scale_y_continuous(limits=c(0,42)) +
  labs(
    x='Predicted time of delivery (week)'
    ,y='True time of delivery (average week)'
    ,color='Prediction of PROM by DI-VNN'
  ) +
  scale_color_manual(values=c('#B24745FF','#374E55FF')) +
  theme(
    axis.title=element_text(family='sans')
    ,axis.text=element_text(family='sans')
    ,strip.text.x=element_text(family='sans')
    ,strip.text.y=element_text(angle=0,family='sans')
    ,legend.title=element_text(family='sans')
    ,legend.text=element_text(family='sans')
    ,legend.position='top'
  )

# Filter the best estimation .
estimation_plot_best_data=
  
  # Plot the estimation.
  eval_timing_model_plt  %>%
  filter(model=='pc_rf' & set=='timing_calib' & metric_name=='calib_pred') %>%
  rename(calib_pred=metric) %>%
  
  # Determine the estimated times that fulfill the criteria.
  mutate(
    fall_into_interval=
      timing_pred>=lb & timing_pred<=ub & abs(ub-lb)<=timing_pred
      & lb<=42 & ub<=42
    ,fall_into_interval=
      ifelse(is.na(fall_into_interval),FALSE,fall_into_interval)
    ,fall_into_interval=
      ifelse(fall_into_interval,'Yes','No')
  ) %>%
  
  # Determine the optimum period of estimation.
  mutate(
    timing_true=
      ifelse(
        calib_pred=='event'
        ,ifelse(timing_pred>=6 & timing_pred<=22,avg,NA)
        ,ifelse(timing_pred>=9 & timing_pred<=34,avg,NA)
      )
    ,pred_optmin=ifelse(calib_pred=='event',6,9)
    ,pred_optmax=ifelse(calib_pred=='event',22,34)
  ) %>%
  group_by(calib_pred) %>%
  mutate(RMSE=sqrt(mean((timing_pred-timing_true)^2,na.rm=T))) %>%
  ungroup()

# Plot the best estimated time of delivery.
estimation_plot_best=
  
  # Plot the estimation.
  estimation_plot_best_data %>%
  ggplot(aes(timing_pred,avg),data=.) +
  geom_point(aes(color=fall_into_interval)) +
  geom_linerange(
    aes(ymin=lb,ymax=ub,color=fall_into_interval)
    ,size=1
  ) +
  geom_abline(slope=1,intercept=0,lty=2) +
  
  # Show the optimum period.
  geom_vline(aes(xintercept=pred_optmin),lty=2) +
  geom_vline(aes(xintercept=pred_optmax),lty=2) +
  geom_errorbarh(aes(xmin=pred_optmin,xmax=pred_optmax,y=2),size=1) +
  
  # Show the RMSEs.
  geom_label(
    y=2
    ,aes(
      x=pred_optmin+(pred_optmax-pred_optmin)/2
      ,label=paste0('RMSE Â± ',formatC(round(RMSE,1),decimal.mark='.'), ' weeks')
    )
    ,size=unit(3,'pt'),family='sans'
  ) +
  
  # Show the prediction.
  geom_label(
    y=42,hjust=0.5,vjust=1,alpha=0.75
    ,aes(
      x=ifelse(timing_pred==0,pred_optmin+(pred_optmax-pred_optmin)/2,NA)
      ,label=str_to_title(calib_pred),fill=calib_pred
    )
    ,family='sans'
    ,show.legend=F
    ,na.rm=T
  ) +
  
  # Customize plot.
  facet_grid(~str_to_title(calib_pred)) +
  scale_x_continuous(limits=c(0,42),breaks=seq(0,42,2)) +
  scale_y_continuous(limits=c(0,42),breaks=seq(0,42,2)) +
  labs(
    x='Predicted time of delivery (week)'
    ,y='True time of delivery (95% CI)'
    ,color='Fulfilling the criteria in main text?'
  ) +
  scale_color_manual(values=c('#80796B99','#79AF97FF')) +
  scale_fill_manual(values=c('#B24745FF','#374E55FF')) +
  theme(
    axis.title=element_text(family='sans')
    ,axis.text.x=element_text(angle=90,hjust=1,vjust=0.5,family='sans')
    ,axis.text.y=element_text(family='sans')
    ,strip.text=element_blank()
    ,strip.background=element_blank()
    ,legend.title=element_text(family='sans')
    ,legend.text=element_text(family='sans')
    ,legend.position='top'
  )
```

```{r Time of delivery distribution, eval=FALSE, include=FALSE}
eval_timing_model %>%
  
  # Get the estimation.
  .[c('pc_rf','divnn')] %>%
  lapply(X=names(.),Y=.,function(X,Y){
    Y[[X]] %>%
      lapply(X=names(.),Y=.,Z=X,K=Y,function(X,Y,Z,K){
        cbind(
            Y[[X]] %>%
              select(timing,outcome,timing_pred)
            ,K$divnn[[X]] %>%
              select(pred,calib_pred)
          ) %>%
          select_at(colnames(Y[[X]])) %>%
          filter(timing_pred>0) %>%
          mutate(timing_pred=round(timing_pred)) %>%
          mutate(
            outcome=
              ifelse(outcome==1|outcome=='event','event','nonevent') %>%
                  factor(c('nonevent','event'))
            ,pred=
              ifelse(pred==1|pred=='event','event','nonevent') %>%
                  factor(c('nonevent','event'))
          ) %>%
          mutate(set=X)
      }) %>%
      do.call(rbind,.) %>%
      mutate(model=X)
  }) %>%
  do.call(rbind,.) %>%
  
  # Choose the best estimation.
  filter(model=='pc_rf') %>%
  select(-model) %>%
  
  # Compute the interval estimates.
  group_by(set,calib_pred) %>%
  summarize(
    avg=round(mean(timing))
    ,lb=round(mean(timing)-qnorm(0.975)*sd(timing)/sqrt(length(timing)))
    ,ub=round(mean(timing)+qnorm(0.975)*sd(timing)/sqrt(length(timing)))
    ,n=length(timing)
  ) %>%
  arrange(calib_pred,set) %>%
  select(set,everything()) %>%
  
  # Clean up naming.
  mutate(set=case_when(
    set=='timing_calib'~'Internal\ncalibration\nsplit'
    ,set=='timing_ran'~'External\nrandom\nsplit'
    ,set=='timing_geo'~'External\ngeographical\nsplit'
    ,set=='timing_tem'~'External\ntemporal\nsplit'
    ,set=='timing_bgt'~'External\ngeotemporal\nsplit'
    ,TRUE~''
  )) %>%
  mutate(calib_pred=str_to_sentence(calib_pred)) %>%
  setNames(c('Set','DI-VNN prediction','Average','LB','UB','Sample size')) %>%
  
  # Show as table.
  kable(format.args=list(big.mark=',')) %>%
  kable_classic()
```

```{r RMSE per data partition, eval=FALSE, include=FALSE}
eval_timing_model_plt %>%
  
  # Get the best estimation.
  filter(model=='pc_rf' & metric_name=='calib_pred') %>%
  rename(calib_pred=metric) %>%
  group_by(set) %>%
  
  # Determine the estimated times that fulfill the criteria.
  mutate(
    fall_into_interval=
      timing_pred>=lb & timing_pred<=ub & abs(ub-lb)<=timing_pred
      & lb<=42 & ub<=42
    ,fall_into_interval=
      ifelse(is.na(fall_into_interval),FALSE,fall_into_interval)
    ,fall_into_interval=
      ifelse(fall_into_interval,'Yes','No')
  ) %>%
  
  # Determine the optimum period of estimation.
  mutate(
    timing_true=
      ifelse(
        calib_pred=='event'
        ,ifelse(timing_pred>=6 & timing_pred<=22,avg,NA)
        ,ifelse(timing_pred>=9 & timing_pred<=34,avg,NA)
      )
    ,pred_optmin=ifelse(calib_pred=='event',6,9)
    ,pred_optmax=ifelse(calib_pred=='event',22,34)
  ) %>%
  
  # Compute the RMSEs.
  group_by(set,calib_pred) %>%
  mutate(RMSE=sqrt(mean((timing_pred-timing_true)^2,na.rm=T))) %>%
  ungroup() %>%
  select(set,calib_pred,RMSE) %>%
  filter(!duplicated(.)) %>%
  
  # Clean up naming and data.
  mutate(set=case_when(
    set=='timing_calib'~'Internal\ncalibration\nsplit'
    ,set=='timing_ran'~'External\nrandom\nsplit'
    ,set=='timing_geo'~'External\ngeographical\nsplit'
    ,set=='timing_tem'~'External\ntemporal\nsplit'
    ,set=='timing_bgt'~'External\ngeotemporal\nsplit'
    ,TRUE~''
  )) %>%
  mutate(
    calib_pred=str_to_sentence(calib_pred)
    ,RMSE=round(RMSE,1)
  ) %>%
  setNames(c('Set','DI-VNN prediction','RMSE')) %>%
  
  # Show as table.
  kable(format.args=list(big.mark=',')) %>%
  kable_classic()
```

```{r Save data for Figure 3 of the main text, eval=FALSE, include=FALSE}
# Figure 3a
estimation_plot_comp_data %>%
  select(-metric_name,-lb,-ub,-fall_into_interval) %>%
  select(model,set,p,everything()) %>%
  mutate(set=str_replace_all(set,'\n',' ')) %>%
  mutate(avg=round(avg)) %>%
  mutate(p=p*100) %>%
  setNames(
    c('Model'
      ,'Set'
      ,'Criteria fulfilled (%)'
      ,'Predicted time of deliver (week)'
      ,'Prediction of PROM by DI-VNN'
      ,'True time of delivery (average week)')
  ) %>%
  
  # Save data for Figure 3a.
  write_csv('data/figure3a.csv')

# Figure 3b
estimation_plot_best_data %>%
  select(-metric_name,-set,-model,-timing_true) %>%
  mutate_at(c('avg','lb','ub'),round) %>%
  setNames(
    c('Predicted time of deliver (week)'
      ,'Prediction of PROM by DI-VNN'
      ,'True time of delivery (week 95% CI)'
      ,'Lower bound'
      ,'Upper bound'
      ,'Fulfilling the criteria in main text?'
      ,'Estimation window'
      ,'Maximum'
      ,'RMSE')
  ) %>%
  
  # Save data for Figure 3b.
  write_csv('data/figure3b.csv')
```

```{r Save image of Figure 3 of the main text, fig.height=8.85827, fig.width=7.08661, eval=FALSE, include=FALSE}
# fig.height=8.85827, fig.width=7.08661 on top of this chunk if shown here
suppressWarnings(
  ggarrange(
    estimation_plot_comp
    ,estimation_plot_best
    ,heights=c(5,3.85827)
    ,labels=c('a','b')
    ,ncol=1,nrow=2
  ) +
  ggsave(
    filename="figure3.eps"
    ,device=cairo_ps
    ,width=170
    ,height=170/190*237.5
    ,unit='mm'
  )
)
```

## Exploring deep-insight visible neural network

Population-level data exploration is described in this Supplement. For 
interactive figure and table of DI-VNN, we provide these in our web application 
(https://predme.app/promtime). Technical details for exploring DI-VNN were 
already described in a protocol we followed for these procedures [...].

The network architecture of the DI-VNN is data-driven (eFigure 15b in 
Appendix). Using the clique-extracted ontology (CliXO) (Kramer M, et al, 2014) 
algorithm, we constructed an architecture for the convolutional neural network 
(CNN) using only a training set (see Methods). We can consider this algorithm 
as an agglomerative hierarchical clustering algorithm, but there could be more 
than one parent-child connection. A child ontology array has a similar value 
distribution with the parent one is because a neural network applies a 
backpropagation algorithm that updates the model parameters consecutively from 
the surface to the deeper layer following the path, which is the ontology 
network of DI-VNN. But, unlike other neural network models, DI-VNN isolates the 
backpropagation effect; therefore, we can trace the array values to interpret 
the possible meaning.

We explored each node of the DI-VNN at the population level. The 
diagnosis/procedure codes constructing the feature members were ICD-10 codes. 
We began from the most visually distinguished array (eFigure 15b in Appendix) 
which was ONT:171. These were N760 (acute vaginitis) and B379 (unspecified 
candidiasis). A positive output does not necessarily refer to an event. 
Nevertheless, positive and negative (color-coded) outputs tend to contribute to 
opposite outcomes, which are interpreted based on external contextual 
knowledge. Another distinguished array, ONT:144, was connected to the same node 
as that of the previous array. The feature member (9059, other microscopic 
examination of blood) tended to contribute to the same outcome as that of 
âunspecified candidiasisâ. Unlike âacute vaginitisâ, which is a local 
infection, both B379 and 9059 may be related to systemic infections. To this 
point, we gained insights into describing coincidences between systemic vs. 
local infection and PROM. Another array, ONT:154, was also visually 
distinguished. Although the value of the feature member causal_A03 
(chorioamnionitis) was zero, it was next to higher values that supported the 
same outcome as that of âacute vaginitisâ. We traced the node on the upper 
layer to ONT:171. This array had a similar value distribution on the same 
channel (*z*=2) to that of ONT:154.

The feature position within any ontology array was determined using *t*-SNE 
with the Barnes-Hut approximation (see Methods) (Maaten LVD, 2014). It mapped 
features on high dimensional to lower dimensional space, as multiple 
localities. This algorithm spreads small clusters instead of making a large 
bubble of clusters; thus, we expected our CNN algorithm can be better 
extracting predictive features from these localities. If a feature nearer to 
one than another, this means there is a closer relationship between both 
features. The localities clustered by *t*-SNE are grouped together at the root 
node on the most superficial layer (eFigure 15b in Appendix). Deeper layers 
have different subsets of features separated by the ontology grouping. By 
understanding how this algorithm works, we assumed causal_A03 (ONT:154), i.e., 
âchorioamnionitisâ, was closer to âacute vaginitisâ than was âunspecified 
candidiasisâ since both were in the same channel, the positions of the first 
and second dimensions were adjacent, and *t*-SNE preserved the neighborhood 
identity. Acute vaginitis is semantically a genital tract infection (GTI). By 
external contextual knowledge inferred from systematic human learning and 
confirmed by IPW using our data, GTI and chorioamnionitis were determined to be 
associated factors of PROM.

A node on more superficial layer, which is ONT:155, consisted a feature that 
prefers the same outcome that N760 (acute vaginitis) and causal_A03 
(chorioamnionitis) prefer, which is 598 (urethral catheterization). This 
feature is semantically related to acute vaginitis because the anatomical sites 
are adjacent. But, since ONT:155 is on a more superficial layer, this node will 
connect to the same node with many features from other ontology terms. This 
means more factors may need to interact with urethral catheterization (598) to 
be predictive for PROM. In addition, within the same ontology term, there is 
also 8602 (injection or tattooing of skin lesion or defect). Apparently, the 
CliXO algorithm have clustered these features together semantically, which are 
similarly invasive procedures. In addition, local infection from acute 
vaginitis may be related to chorioamnionitis, such that prefers the same 
outcome, as opposed to the possible systemic infection by the unspecified 
candidiasis.

Exploring this DI-VNN should be done with caution, since we developed the model 
using medical histories, which are diagnosis or procedure codes provided by 
medical doctors. We may or may not be modeling a human pathophysiology, but, we 
are definitely modeling the doctorsâ behaviors of coding a diagnosis or 
procedure (Beaulieu-Jones BK, et al, 2021). By providing an interface to the internal properties of this 
model, a human user can assess each prediction case-by-case. From ONT:167 and 
ONT:149 on the deeper layer, we can find unusual features in the context of 
PROM, which are H527 (unspecified disorder of refraction) preferring nonevents 
while 734 (flat foot), H521 (myopia), and H522 (astigmatism) preferring events. 
These codes might be responses to the subject symptoms of edema in the feet and 
blurry vision. Both symptoms in a pregnant woman may be typically associated 
with severe preeclampsia. But, a doctor may avoid this association if the 
context does not support the symptoms, e.g. symptoms by a non-pregnant subject. 
This may lead a doctor to assign these codes responding to those symptoms. For 
each prediction, a human user may need to explore the model to avoid 
misclassification by ignoring the prediction if it counters the clinical 
reasoning. More pragmatically at individual level, the predictive value may not 
be sufficient or the estimation may not be quite precise based on the 
corresponding subpopulation data with respectively the same predicted outcome 
or estimated time of delivery. In addition, albeit all of the population-level 
patterns from this exploration, every subject may reveal a different pattern 
using the same DI-VNN model, as described in the next section.

## Web application

Briefly, we uploaded a record of 20 visits by a 19-year-old female subject from 
December 2, 2015 to July 30, 2016, consisting of 28 code entries. After 
determining the prediction date, which was set to July 30, 2016, we ran the 
application in 5.14 minutes (95% CI 5.11 to 5.18 minutes when we repeated it 
10 times). We downloaded the report after the application was completed 
(eFigure 16 in Appendix). The predicted outcome was PROM, and the estimated 
time of delivery was 11 weeks after the time of prediction. The predicted 
probability was 0.867.

Furthermore, after the application was done (eFigure 16 in Appendix), we tried 
to adjust the threshold to the maximum value, such that the data for 
population-level performances are still available and the predictive value is 
also maximized depending on the prediction result. The population-level data 
was the same with internal validation (calibration split; *n*=21,506) which was 
used to plot the ROC curve (Figure 2b in the main text). The threshold was 0.67 
with positive predictive value of 0.809 (95% CI 0.798 to 0.82). The sensitivity 
was reasonably low (0.107, 95% CI 0.104 to 0.11) if using this threshold. But, 
from a standpoint of prediction at individual level, a precise estimation is 
important to determine whether a decision corresponding to this prediction can 
be made with a good confidence. By default, the threshold is set at an optimum 
value of 0.14 (sensitivity 0.494, 95% CI 0.489 to 0.5). Based on the predicted 
probability case-by-case, a user can decide the threshold to adjust at.

From the reported timeline (eFigure 6 in Appendix), this subject is shown 
predicted to deliver on October 18^th 2016, approximately. If the predicted 
outcome is not PROM, the shaded area would be red; otherwise, turquoise color 
is applied to the area as shown. It depicted population-level estimation of 
true time of delivery for subjects that were also estimated to deliver within 
11 weeks and predicted as PROM by the same threshold. The population-level data 
was the same with internal validation (*n*=107,536) which was used to plot the 
PC-RF estimation window (Figure 3b in the main text). By population-level 
estimation, the time of delivery might be at the beginning up to the end of 
October. Using threshold at 0.67, this population-level estimation was shifted 
earlier for a week compared to that at 0.14. In addition, for illustration 
purpose, just like a real-world setting, say we know the gestational age was 
22-23 weeksâ gestation based on last menstrual period and ultrasound 
examination. If this estimation model is precise for this case, the subject 
would deliver at 33-34 weeksâ gestation, which is a preterm PROM.

We also saw the medical history of this subject from the reported timeline 
(eFigure 16 in Appendix). Up to the date of prediction, the prediction model 
used these features: (1) A09 (diarrhea and gastroenteritis of presumed 
infectious origin); (2) J069 (unspecified acute upper respiratory infection); 
(3) K30 (dyspepsia); and (4) 8878 (diagnostic ultrasound of gravid uterus). On 
the timeline, these were ordered from the most positive (top) to the most 
negative (bottom) based on each output in the ontology array.

In the prediction model, any of these features were classified in the ontology 
arrays of ONT:158, ONT:169, ONT:176, and root, as depicted by the ontology 
network (eFigure 16 in Appendix). We also identified the deepest ontology that 
includes all features in the timeline, which is ONT:169, but the predicted 
outcome based on this ontology is not PROM using the same threshold with that 
of the root. A user also can see a predictive performance of any ontologies, 
just like AUROC of the root (0.714, 95% CI 0.712 to 0.716). It is computed for 
the pre-calibrated DI-VNN only, since the calibrated one only used the 
predicted probability that was the output convoluted from the root ontology 
array.

Negative values at population level tend to event, as described in the previous 
section. From the ontology array (eFigure 16 in Appendix), J069 (unspecified 
acute upper respiratory infection) tends to event in that array, as shown as 
mostly negative outputs in the timeline. This feature was also surrounded by 
more negative outputs. A09 (diarrhea and gastroenteritis of presumed infectious 
origin) also had a negative value, but, this feature and K30 (dyspepsia) were 
surrounded by more positive outputs in the ontology array. If we apply the same 
illustrative gestational age, these infectious diseases (J069 and A09) were 
diagnosed respectively ten and four weeks the start of pregnancy, as shown on 
the timeline. Root is the only ontology that predicted PROM and included all 
features in this subject. This is implied all of these features should be taken 
together for the prediction. In addition, one may question why J069 
(unspecified acute upper respiratory infection) tends to event while influenza 
have the opposite effect (OR 0.995,95% CI 0.993 to 0.997; eTable 13 in 
Appendix). We found that influenza, which is causal_A28, did not include J069 
(eTable 12 in R Markdown). This implied specific acute upper respiratory 
infection, such influenza, may not have the same effect with that by the 
unspecified one on PROM.

Beyond the root, the array of ontology ONT:169 is also shown (eFigure 17 in 
Appendix). A09 and J069 had negative values. As described in the main text, 
these features tend to an event. Respectively, the surrounding positive and 
negative values were subtler in this ontology array. Since this filtered array 
is fed forward to convolutional layers to be reduced each time passing a layer 
(see video in the protocol [a protocol citation]), the value of A09 and J069 
were averaged along with the adjacent values toward zero, opposite to event. In 
turn, this coincides with lower AUROC in ONT:169 compared to that in root.

Eventually, a user may want to know if the PROM prediction and estimated time 
of delivery are similar to true values. One can save this model online and 
return later to the web application to enter the true outcome and time of 
delivery. In this way, a user can collect data for external validation 
purposes, specifically describing the model performances based on local data 
distributions. In our case, the true outcome was also PROM and the time of 
delivery was 12 weeks after the time of prediction, a week later than the 
predicted time of delivery.

# Discussion

From association tests and feature extraction to model selection and 
exploration, we only used an internal validation set. But, the model is 
evaluated using four external validation sets with a large sample size. This 
has found the DI-VNN prediction was robust and the PC-RF estimation was precise 
within a reasonable time window. All of these models used only a medical 
history of a patient, which is easily extracted from the electronic medical 
record systems of most healthcare providers worldwide. Neither a biomarker 
testing nor even a laboratory test is needed. Eventually, the best models are 
ready to use for any healthcare providers using an open-access web application 
without changing their electronic health record systems and revealing private 
data.

To gain insights into PROM antecedents, we showed how this was achieved by 
exploring the model at both the population (eFigure 15b in Appendix) and 
individual levels (eFigure 16 in Appendix). If we assume that PROM is a 
subclass of preeclampsia, this may explain why population-level exploration 
implied an insight of systemic vs. local infection by competing risks. A 
hematogenous infection may be associated with reproductive-tract microbial 
dysbiosis and can affect several pregnancy outcomes, including PROM, 
preeclampsia, and fetal growth restriction (Amir M, et al, 2020). Both 
hematogenous and ascending infections from reproductive tract are found in PROM 
(Romero R, et al, 2019; Solt I, 2015). Hematogenous infections included those 
from digestive and respiratory organs. Similar to the population-level 
exploration (eFigure 15 in Appendix), we also found a possibility of 
hematogenous infection at the individual level (eFigure 16 in Appendix), and 
these were from infectious gastroenteritis and unspecified acute upper 
respiratory infection. Both population- and individual-level explorations also 
implied a period surrounding the beginning of pregnancy as the onset of the 
optimal time to predict PROM. Several features in the DI-VNN were 
counterintuitive, e.g., H527 (unspecified disorder of refraction), 734 (flat 
foot), H521 (myopia), and H522 (astigmatism). Yet, these describe blurry vision 
and swelling in the feet, which are also symptoms of preeclampsia. Similar 
eye-related codes, i.e., myopia and astigmatism, were also found to be 
important in predicting preeclampsia in an RF model (Sufriyana H, et al, 2020).

# References

\newpage
# Appendix

\listoffigures
\listoftables

\newpage
```{r figure-1, echo=FALSE, fig.cap='Multiple pregnancy', fig.height=5.11811, fig.width=7.08661}
caudag_img[[1]]
```

\newpage
```{r figure-2, echo=FALSE, fig.cap='Chorioamnionitis', fig.height=5.11811, fig.width=7.08661}
caudag_img[[2]]
```

\newpage
```{r figure-3, echo=FALSE, fig.cap='Intra-amniotic infection (IAI)', fig.height=5.11811, fig.width=7.08661}
caudag_img[[3]]
```

\newpage
```{r figure-4, echo=FALSE, fig.cap='Ante-partum hemorrhage (APH)', fig.height=5.11811, fig.width=7.08661}
caudag_img[[5]]
```

\newpage
```{r figure-5, echo=FALSE, fig.cap='Genital tract infection (GTI)', fig.height=3.46457, fig.width=3.46457}
caudag_img[[9]]
```

\newpage
```{r figure-6, echo=FALSE, fig.cap='Periodontal disease', fig.height=5.11811, fig.width=7.08661}
caudag_img[[10]]
```

\newpage
```{r figure-7, echo=FALSE, fig.cap='Polyhydramnios', fig.height=5.11811, fig.width=7.08661}
caudag_img[[11]]
```

\newpage
```{r figure-8, echo=FALSE, fig.cap='Pneumonia', fig.height=5.11811, fig.width=7.08661}
caudag_img[[12]]
```

\newpage
```{r figure-9, echo=FALSE, fig.cap='Asthma', fig.height=5.11811, fig.width=7.08661}
caudag_img[[14]]
```

\newpage
```{r figure-10, echo=FALSE, fig.cap='Low socio-economic status (SES)', fig.height=3.46457, fig.width=3.46457}
caudag_img[[18]]
```

\newpage
```{r figure-11, echo=FALSE, fig.cap='Maternal age', fig.height=3.46457, fig.width=3.46457}
caudag_img[[19]]
```

\newpage
```{r figure-12, echo=FALSE, fig.cap='Influenza', fig.height=3.46457, fig.width=3.46457}
caudag_img[[27]]
```

\newpage
```{r Prepare data for Figure 13 of the Supplement, include=FALSE}
suppressWarnings(set.seed(66,sample.kind=sample.kind))
figure13=
  
  # Filter the edges involving only the causal factors and outcome.
  dag$baseline_edges[,-3] %>%
  filter(
    (from %in% colnames(dag$sig)[dag$sig[2,]==1] &
     to %in% c('Y01',colnames(dag$sig)[dag$sig[2,]==1]))
  ) %>%
  
  # Join with the IPW results.
  left_join(
    dag$ipw %>%
      lapply(X=seq(length(.)),Y=.,function(X,Y){
        data.frame(
            from=names(Y)[X]
            ,to='Y01'
            ,marginal_effect=Y[[X]]$marginal_effect
            ,LB=Y[[X]]$CI95_interval['LB']
            ,UB=Y[[X]]$CI95_interval['UB']
            ,p_value=Y[[X]]$p_value
          ) %>%
          `rownames<-`(NULL)
      }) %>%
      do.call(rbind,.)
    ,by=c('from','to')
  ) %>%
  
  # Assign the labels.
  left_join(
    dag$baseline_nodes %>%
      rename(from=label) %>%
      mutate(label=paste(from,name)) %>%
      select(from,label)
    ,by='from'
  ) %>%
  select(-from) %>%
  rename(from=label) %>%
  left_join(
    dag$baseline_nodes %>%
      rename(to=label) %>%
      mutate(label=paste(to,name)) %>%
      select(to,label)
    ,by='to'
  ) %>%
  select(-to) %>%
  rename(to=label) %>%
  
  # Convert to igraph object then ggnetwork dataframe.
  select(from,to,everything()) %>%
  graph_from_data_frame() %>%
  ggnetwork(
    layout=
      layout_as_tree(
        .
        ,root=c()
        ,mode='in'
        ,circular=T
      )[,2:1]
  ) %>%
  mutate(code=str_sub(name,1,3))  %>%
  mutate(name=sapply(X=name,function(X)str_sub(X,5,str_count(X))))
```

```{r figure-13, echo=FALSE, fig.cap='Final association diagram', fig.height=3.46457, fig.width=3.46457}
figure13 %>%
  ggplot(aes(x=x,y=y,xend=xend,yend=yend,fill=code)) +
  geom_nodes(
    aes(color=code)
    ,show.legend=F
    ,size=8
  ) +
  geom_edges(
    aes(color=code)
    ,arrow=arrow(length=unit(4,'pt'),type='closed')
    ,show.legend=F
  ) +
  geom_nodelabel_repel(
    aes(label=name,fill=code)
    ,family='sans'
    ,size=unit(3,'pt')
    ,alpha=0.75
    ,show.legend=F
  ) +
  # scale_color_manual(
  #   values=
  #     unlist(mapply(
  #       RColorBrewer::brewer.pal
  #       ,RColorBrewer::brewer.pal.info %>%
  #         .[.$category=='qual',] %>%
  #         .$maxcolors
  #       ,RColorBrewer::brewer.pal.info %>%
  #         .[.$category=='qual',] %>%
  #         rownames()
  #     ))[-seq(14)]
  # ) +
  # scale_fill_manual(
  #   values=
  #     unlist(mapply(
  #       RColorBrewer::brewer.pal
  #       ,RColorBrewer::brewer.pal.info %>%
  #         .[.$category=='qual',] %>%
  #         .$maxcolors
  #       ,RColorBrewer::brewer.pal.info %>%
  #         .[.$category=='qual',] %>%
  #         rownames()
  #     ))[-seq(14)]
  # ) +
  scale_color_manual(
    values=
      c('#374E55FF','#DF8F44FF','#00A1D5FF','#B24745FF','#79AF97FF','#6A6599FF'
        ,'#80796BFF','#374E5599','#DF8F4499','#00A1D599','#B2474599','#79AF9799'
        )
  ) +
  scale_fill_manual(
    values=
      c('#374E55FF','#DF8F44FF','#00A1D5FF','#B24745FF','#79AF97FF','#6A6599FF'
        ,'#80796BFF','#374E5599','#DF8F4499','#00A1D599','#B2474599','#79AF9799'
        )
  ) +
  theme_blank()
# +
#   ggsave(
#     filename="efigure13.eps"
#     ,device=cairo_ps
#     ,width=85
#     ,height=85
#     ,unit='mm'
#   )
```

Caption:

All associated factors were verified by inverse probability weighting (IPW) 
using our data. Association tests were only conducted between PROM and each of 
the associated factors. Inter-associated factor relationships were not 
verified, but this demonstrates how an associated factor is included in the 
associated model of another associated factor in this figure. APH, ante-partum 
hemorrhage; GTI, genital tract infection; IAI, intra-amniotic infection; PROM, 
prelabor rupture of membranes; SES, socio-economic status.

\newpage
```{r Compute DI-VNN AUROC per period, include=FALSE}
# Create a table of AUROC per period.
auroc_per_period_data=
  
  # Combine the classification and estimation data.
  cbind(
    test_data_reg$calib %>%
      phenoData() %>%
      pData() %>%
      select(outcome) %>%
      rename(timing=outcome)
    ,test_data$calib %>%
      phenoData() %>%
      pData() %>%
      select(outcome) %>%
      rename(obs=outcome) %>%
      mutate(
        obs=
          ifelse(obs==1,'event','nonevent') %>%
          factor(c('nonevent','event'))
      )
  ) %>%
  rownames_to_column(var='id') %>%
  mutate(rowIndex=seq(nrow(.))) %>%
  right_join(calib_model$divnn$pred,by=c('obs','rowIndex')) %>%
  select(Resample,rowIndex,event,obs,timing) %>%
  setNames(c('subset','index','score','outcome','timing')) %>%
  mutate(outcome=as.double(outcome=='event')) %>%
  
  # Set multiple thresholds.
  left_join(
    select(.,index) %>%
      filter(!duplicated(.)) %>%
      arrange(index) %>%
      cbind(
        data.frame(
            key=paste0('th_',str_pad(0:100,3,'left','0'))
            ,value=seq(0,1,len=101)
          ) %>%
          spread(key,value)
      )
    ,by='index'
  ) %>%
  gather(key,th,-subset,-index,-score,-outcome,-timing) %>%
  select(-key) %>%
  mutate(pred=as.integer(score>th)) %>%
  
  # Compute confusion matrix.
  mutate(
    tp=as.integer(outcome==1 & pred==1)
    ,fn=as.integer(outcome==1 & pred==0)
    ,fp=as.integer(outcome==0 & pred==1)
    ,tn=as.integer(outcome==0 & pred==0)
  ) %>%
  select(-index,-score,-outcome,-pred) %>%
  mutate(timing=round(timing/28)) %>%
  group_by(timing,subset,th) %>%
  summarize_all(sum) %>%
  ungroup() %>%
  
  # Compute evaluation metrics.
  mutate(
    tpr=tp/(tp+fn+1e-17)
    ,tnr=tn/(tn+fp+1e-17)
    ,ppv=tp/(tp+fp+1e-17)
    ,npv=tn/(tn+fn+1e-17)
  ) %>%
  select(-tp,-fn,-fp,-tn) %>%
  
  # Summarize AUROC per period.
  group_by(subset,timing) %>%
  left_join(mutate(.,th=th+0.01),by=c('subset','timing','th')) %>%
  filter_all(function(x)!is.na(x)) %>%
  mutate(
    auroc=abs(tnr.x-tnr.y)*tpr.x+abs(tnr.x-tnr.y)*0.5*(abs(tpr.x-tpr.y))
  ) %>%
  summarize(auroc=sum(auroc)) %>%
  ungroup() %>%
  select(-subset) %>%
  
  # Compute the interval estimates.
  group_by(timing) %>%
  summarize_all(function(x){
    y=mean(x)+(-1:1)*qnorm(0.975)*sd(x)/sqrt(length(x))
    paste0(y,collapse='|')
  }) %>%
  gather(metric,interval,-timing) %>%
  separate(interval,c('lb','avg','ub'),sep='\\|') %>%
  mutate_at(c('lb','avg','ub'),as.numeric) %>%
  mutate_at(c('lb','avg','ub'),function(x){
    round(ifelse(x<0,0,ifelse(x>1,1,x)),4)
  }) %>%
  
  # Create the smooth version of the average AUROCs.
  cbind(
    loess(formula=avg~timing,data=.) %>%
      predict() %>%
      data.frame(loess=.)
  ) %>%
  
  # Determine if an AUROC is more than 0.5.
  mutate(
    higher_than_guessing=0.5<lb & 0.5<ub
    ,higher_than_guessing=ifelse(higher_than_guessing,'Yes','No')
  )

# Plot the AUROC per period.
auroc_per_period=
  auroc_per_period_data %>%
  ggplot(aes((0-timing)*4,avg,color=higher_than_guessing),data=.) +
  geom_point(size=3) +
  geom_linerange(aes(ymin=lb,ymax=ub),size=2) +
  geom_smooth(method='loess',formula=y~x,color='#DC0000FF',se=F,size=1) +
  geom_hline(yintercept=0.5,lty=2) +
  geom_vline(xintercept=-46,lty=2) +
  scale_x_continuous('Weeks before the end of pregnancy',breaks=seq(-104,0,4)) +
  scale_y_continuous(
    'AUROC (95% CI)',breaks=seq(0,1,0.1)
    ,labels=scales::number_format(accuracy=0.1,decimal.mark='.')
  ) +
  labs(color='Higher\nthan\nAUROC\nof 0.5?') +
  scale_color_manual(values=c('#80796B99','#79AF97FF')) +
  theme(
    axis.title=element_text(family='sans')
    ,axis.text.x=element_text(angle=90,hjust=1,vjust=0.5,family='sans')
    ,axis.text.y=element_text(family='sans')
    ,legend.title=element_text(family='sans')
    ,legend.text=element_text(family='sans')
    ,legend.position='right'
  )
```

```{r Prepare report of DI-VNN exploration, include=FALSE}
# Create an empty list to save DI-VNN exploration.
divnn_plot=list()

# Construc ggnetwork table the ontology network.
divnn_plot$base_data=
  visualization$ontonet$edge %>%
  graph_from_data_frame(directed=T) %>%
  ggnetwork(layout=layout_as_tree(.,mode='in')) %>%
  left_join(rename(visualization$ontonet$node,name=node),by='name')

# Plot the ontology network.
divnn_plot$base=
  divnn_plot$base_data %>%
  ggplot(aes(x=x,y=y,xend=xend,yend=yend,fill=avg)) +
  geom_edges(
    arrow=arrow(length=unit(5,'pt'),type='closed')
    ,show.legend=F
  ) +
  geom_nodes(
    aes(
      x=ifelse(avg>=0.58,NA,x)
      ,color=avg
    )
    ,show.legend=F
    ,size=8
    ,na.rm=T
  ) +
  geom_nodelabel(
    aes(label=ifelse(lb>0.55,paste0(formatC(round(avg,3),decimal.mark='.')),NA))
    ,family='sans'
    ,size=unit(4,'pt')
    ,alpha=0.95
    ,show.legend=F
    ,na.rm=T
  ) +
  geom_label(
    aes(
      label=
        ifelse(
          name%in%paste0('ONT:',c(154,171,144,155,149,167))
          ,str_remove_all(name,'ONT:')
          ,NA
        )
    )
    ,fill='white'
    ,family='sans'
    ,size=unit(3,'pt')
    ,segment.size=1
    ,alpha=0.2
    ,hjust=0.5
    ,vjust=0.5
    ,nudge_x=0
    ,nudge_y=0
    ,show.legend=F
    ,na.rm=T
  ) +
  scale_color_gradient(low='#B24745FF',high='#79AF97FF') +
  scale_fill_gradient(low='#B24745FF',high='#79AF97FF') +
  theme_blank()

# Convert all ontology arrays into a dataframe.
ontoarrays=
  visualization$ontoarray %>%
  lapply(X=names(.),Y=.,function(X,Y){
    ontoarray = Y[[X]]$output
    ontoarray %>%
      lapply(X=seq(dim(.)[3]),Y=.,Z=X,FUN=function(X,Y,Z){
        Y[, , X] %>% matrix()
      }) %>%
      do.call(rbind, .) %>%
      as.data.frame() %>% 
      setNames("fill") %>%
      mutate(
        x=rep(1:dim(ontoarray)[1],dim(ontoarray)[2]* dim(ontoarray)[3])
        ,y=
          rep(1:dim(ontoarray)[2],dim(ontoarray)[1]) %>%
          sort() %>%
          rep(dim(ontoarray)[3])
        ,z=
          rep(1:dim(ontoarray)[3], dim(ontoarray)[1]*dim(ontoarray)[2]) %>%
          sort()
      ) %>%
      left_join(
        Y[[X]]$ontotype
        ,by=c("x","y","z")
      ) %>% 
      mutate(ontology=X)
  }) %>%
  do.call(rbind,.) %>%
  mutate(z=paste0("z=",z)) %>%
  mutate(feature=str_replace_all(feature,'causal_','assoc_'))

source('R/ontoarray-function.R')
source('R/ontoarray_tab-function.R')

# Get data for the ontoarrays of interest.
ontogroup=
  visualized_codes %>%
  filter(code%in%c(
    'N760'
    ,'B379'
    ,'causal_A03'
    ,'598'
    ,'8602'
    ,'H521'
    ,'H522'
    ,'H527'
    ,'9059'
    ,'734'
  )) %>%
  mutate(
    desc=sapply(X=seq(nrow(.)),Y=code,Z=desc,function(X,Y,Z){
      ifelse(
        str_detect(Y[X],'causal_')
        ,dag$baseline_nodes$name[
          dag$baseline_nodes$label==str_remove_all(Y[X],'causal_')
        ]
        ,Z[X]
      )
    })
  ) %>%
  filter(
    ontotype%in%paste0('ONT:',c(171,154,144,149,155,167))
    | code%in%c('8602','causal_A03')
  ) %>%
  group_by(code) %>%
  filter(seq(n())==1) %>%
  ungroup() %>%
  mutate(
    ontotype=ifelse(code%in%c(8602,'causal_A03'),'',ontotype)
    ,outcome=ifelse(code%in%c(8602,'causal_A03'),'',outcome)
  ) %>%
  mutate(text=paste(code,desc)) %>%
  mutate(text=str_wrap(text,width=40)) %>%
  arrange(
    outcome
    ,factor(
      code
      ,c(
        'N760'
        ,'B379'
        ,'causal_A03'
        ,'598'
        ,'8602'
        ,'H521'
        ,'H522'
        ,'H527'
        ,'9059'
        ,'734'
      )
    )
  ) %>%
  pull(text) %>%
  str_replace_all('causal_','assoc_')

# Plot the ontology arrays on the top of the network.
divnn_plot$top=
  divnn_plot$base +
  annotation_custom(grob=ontoarray(171,c(2),1),0.85,1.05,0.475,0.875) +
  annotation_custom(grob=ontoarray(171,c(9),1),0.9,1,0.35,0.55) +
  annotation_custom(grob=ontoarray(154,c(2),1),0.800,0.935,-0.100,0.135) +
  annotation_custom(grob=ontoarray(144,c(3),1),0.59,0.69,0.05,0.25) +
  annotation_custom(grob=ontoarray(149,c(3),1),0.45,0.55,0.2,0.4) +
  annotation_custom(grob=ontoarray(155,c(2),1,legend=T),0.07,0.17,0.39,0.69) +
  annotation_custom(grob=ontoarray(167,c(3,6,7),2),0.59,0.79,0.5,0.9) +
  annotate(
    geom='text'
    ,label=
      ontogroup[8:10] %>%
      paste0(collapse='\n')
    ,x=0,y=0.3,hjust=0,vjust=0,size=unit(3,'pt'),color='#B24745FF'
    ,family='sans'
  ) +
  annotate(
    geom='text'
    ,label=
      ontogroup[1:2] %>%
      paste0(collapse='\n')
    ,x=0,y=0.2,hjust=0,vjust=0,size=unit(3,'pt'),alpha=0.75
    ,family='sans'
  ) +
  annotate(
    geom='text'
    ,label=
      ontogroup[3:7] %>%
      paste0(collapse='\n')
    ,x=0,y=0,hjust=0,vjust=0,size=unit(3,'pt'),color='#79AF97FF'
    ,family='sans'
  )
```

```{r figure-15, echo=FALSE, fig.cap='Exploratory data analysis', fig.height=8.26772, fig.width=7.08661}
# fig.height=8.26772, fig.width=7.08661 on top of this chunk if shown here
ggarrange(
    auroc_per_period
    ,divnn_plot$top
    ,heights=c(2.26772,6)
    ,labels=c('a','b')
    ,ncol=1,nrow=2
  )
# +
#   ggsave(
#     filename="efigure15.eps"
#     ,device=cairo_ps
#     ,width=170
#     ,height=170/190*221.67
#     ,unit='mm'
#   )
```

```{r Show complete non-zero ontotype, eval=FALSE, include=FALSE}
visualized_codes %>%
  kable() %>%
  kable_classic()
```

Caption:

(a) area under the receiver operating characteristic curve (AUROC) of DI-VNN 
every 4 weeks; (b) ontology (ONT) network and arrays of the DI-VNN. Showing the 
best time window for the prediction by the DI-VNN (a) and AUROCs of >0.55 for 
prediction using parts of the network architecture up to each layer on which a 
node resides (b). Each node is a CliXO term. We only show distinguished output 
arrays for a particular channel denoted by âzâ. A feature in the array may tend 
to positive or negative output, color-coded based on the gradient as shown, 
including the feature description. Yellow squares in an array refer to a 
feature if only its output is non-zero. A feature may not have this square, 
e.g. assoc_A03 and 8602. ONT:154 is an example of a backpropagation effect 
from ONT:171. CliXO, clique-extracted ontology; DI-VNN, deep-insight visible 
neural network.

\newpage
```{r figure-16, echo=FALSE, fig.cap='A case example', fig.height=8.26772, fig.width=7.08661}
# fig.height=8.26772, fig.width=7.08661 on top of this chunk if shown here
ggarrange(
    ggarrange(
      results$identity_results %>%
        ggplot(aes(column,row)) +
        geom_text(aes(label=text),hjust=0,vjust=1,size=3,family='sans') +
        geom_blank(data=data.frame(column=3,row=7)) +
        scale_y_reverse() +
        theme_blank() +
        theme(
          title=element_text(size=unit(8,'pt'),face='bold',family='sans')
        ) +
        ggtitle('Identity and results')
      ,results$pop_est
      ,widths=c(1,1)
      ,ncol=2,nrow=1
    )
    ,results$timeline
    ,ontoproperties1('root',widths=c(2,1),pointer_stroke=1.5,pointer_size=1.4)
    ,heights=c(1.7,3.3,3.26772)
    ,ncol=1,nrow=3
  )
# +
#   ggsave(
#     filename="efigure16.eps"
#     ,device=cairo_ps
#     ,width=190
#     ,height=190/190*221.67
#     ,unit='mm'
#   )
```

Caption:

This is an example for predicting prelabor rupture of membranes (PROM) by 
DI-VNN and estimating of the time of delivery by PC-RF. An ontology term in the 
timeline is prefixed by ONT, followed by the number and one of the feature 
members. 8878, diagnostic ultrasound of gravid uterus; A09, diarrhea and 
gastroenteritis of presumed infectious origin; AUROC, area under receiver 
operating characteristics curve; DI-VNN, deep-insight visible neural network; 
J069, unspecified acute upper respiratory infection; K30, dyspepsia; PC, 
principal component; RF, random forest.

\newpage
```{r figure-17, echo=FALSE, fig.cap='Ontology ONT:169', fig.height=3.26772, fig.width=7.08661}
ontoproperties1('169',widths=c(2,1),pointer_stroke=1.5,pointer_size=1.4)
# +
#   ggsave(
#     filename="efigure17.eps"
#     ,device=cairo_ps
#     ,width=190
#     ,height=190/7.08661*3.26772
#     ,unit='mm'
#   )
```

\newpage
```{r table-13, echo=FALSE}
cauinf_tab %>%
  kable(caption='Association tests') %>%
  kable_classic() %>%
  add_footnote(
    c('P<.05; ** P<.01; *** P<.001; CI, confidence interval')
    ,'symbol'
  )
```
